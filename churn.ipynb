{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hc-9LC17wSAA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "import random\n",
        "random.seed(21)\n",
        "\n",
        "np.random.seed(21)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3mKGAfAtB1b"
      },
      "source": [
        "# Data information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        },
        "id": "rbNV8UaatT34",
        "outputId": "749ad99d-2731-40bd-d09a-d911e9574214"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>CR_PROD_CNT_IL</th>\n",
              "      <th>AMOUNT_RUB_CLO_PRC</th>\n",
              "      <th>PRC_ACCEPTS_A_EMAIL_LINK</th>\n",
              "      <th>APP_REGISTR_RGN_CODE</th>\n",
              "      <th>PRC_ACCEPTS_A_POS</th>\n",
              "      <th>PRC_ACCEPTS_A_TK</th>\n",
              "      <th>TURNOVER_DYNAMIC_IL_1M</th>\n",
              "      <th>CNT_TRAN_AUT_TENDENCY1M</th>\n",
              "      <th>SUM_TRAN_AUT_TENDENCY1M</th>\n",
              "      <th>AMOUNT_RUB_SUP_PRC</th>\n",
              "      <th>PRC_ACCEPTS_A_AMOBILE</th>\n",
              "      <th>SUM_TRAN_AUT_TENDENCY3M</th>\n",
              "      <th>CLNT_TRUST_RELATION</th>\n",
              "      <th>PRC_ACCEPTS_TK</th>\n",
              "      <th>PRC_ACCEPTS_A_MTP</th>\n",
              "      <th>REST_DYNAMIC_FDEP_1M</th>\n",
              "      <th>CNT_TRAN_AUT_TENDENCY3M</th>\n",
              "      <th>CNT_ACCEPTS_TK</th>\n",
              "      <th>APP_MARITAL_STATUS</th>\n",
              "      <th>REST_DYNAMIC_SAVE_3M</th>\n",
              "      <th>CR_PROD_CNT_VCU</th>\n",
              "      <th>REST_AVG_CUR</th>\n",
              "      <th>CNT_TRAN_MED_TENDENCY1M</th>\n",
              "      <th>APP_KIND_OF_PROP_HABITATION</th>\n",
              "      <th>CLNT_JOB_POSITION_TYPE</th>\n",
              "      <th>AMOUNT_RUB_NAS_PRC</th>\n",
              "      <th>CLNT_JOB_POSITION</th>\n",
              "      <th>APP_DRIVING_LICENSE</th>\n",
              "      <th>TRANS_COUNT_SUP_PRC</th>\n",
              "      <th>APP_EDUCATION</th>\n",
              "      <th>CNT_TRAN_CLO_TENDENCY1M</th>\n",
              "      <th>SUM_TRAN_MED_TENDENCY1M</th>\n",
              "      <th>PRC_ACCEPTS_A_ATM</th>\n",
              "      <th>PRC_ACCEPTS_MTP</th>\n",
              "      <th>TRANS_COUNT_NAS_PRC</th>\n",
              "      <th>APP_TRAVEL_PASS</th>\n",
              "      <th>CNT_ACCEPTS_MTP</th>\n",
              "      <th>CR_PROD_CNT_TOVR</th>\n",
              "      <th>APP_CAR</th>\n",
              "      <th>CR_PROD_CNT_PIL</th>\n",
              "      <th>SUM_TRAN_CLO_TENDENCY1M</th>\n",
              "      <th>APP_POSITION_TYPE</th>\n",
              "      <th>TURNOVER_CC</th>\n",
              "      <th>TRANS_COUNT_ATM_PRC</th>\n",
              "      <th>AMOUNT_RUB_ATM_PRC</th>\n",
              "      <th>TURNOVER_PAYM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>CNT_TRAN_MED_TENDENCY3M</th>\n",
              "      <th>CR_PROD_CNT_CC</th>\n",
              "      <th>SUM_TRAN_MED_TENDENCY3M</th>\n",
              "      <th>REST_DYNAMIC_FDEP_3M</th>\n",
              "      <th>REST_DYNAMIC_IL_1M</th>\n",
              "      <th>APP_EMP_TYPE</th>\n",
              "      <th>SUM_TRAN_CLO_TENDENCY3M</th>\n",
              "      <th>LDEAL_TENOR_MAX</th>\n",
              "      <th>LDEAL_YQZ_CHRG</th>\n",
              "      <th>CR_PROD_CNT_CCFP</th>\n",
              "      <th>DEAL_YQZ_IR_MAX</th>\n",
              "      <th>LDEAL_YQZ_COM</th>\n",
              "      <th>DEAL_YQZ_IR_MIN</th>\n",
              "      <th>CNT_TRAN_CLO_TENDENCY3M</th>\n",
              "      <th>REST_DYNAMIC_CUR_1M</th>\n",
              "      <th>REST_AVG_PAYM</th>\n",
              "      <th>LDEAL_TENOR_MIN</th>\n",
              "      <th>LDEAL_AMT_MONTH</th>\n",
              "      <th>APP_COMP_TYPE</th>\n",
              "      <th>LDEAL_GRACE_DAYS_PCT_MED</th>\n",
              "      <th>REST_DYNAMIC_CUR_3M</th>\n",
              "      <th>CNT_TRAN_SUP_TENDENCY3M</th>\n",
              "      <th>TURNOVER_DYNAMIC_CUR_1M</th>\n",
              "      <th>REST_DYNAMIC_PAYM_3M</th>\n",
              "      <th>SUM_TRAN_SUP_TENDENCY3M</th>\n",
              "      <th>REST_DYNAMIC_IL_3M</th>\n",
              "      <th>CNT_TRAN_ATM_TENDENCY3M</th>\n",
              "      <th>CNT_TRAN_ATM_TENDENCY1M</th>\n",
              "      <th>TURNOVER_DYNAMIC_IL_3M</th>\n",
              "      <th>SUM_TRAN_ATM_TENDENCY3M</th>\n",
              "      <th>DEAL_GRACE_DAYS_ACC_S1X1</th>\n",
              "      <th>AVG_PCT_MONTH_TO_PCLOSE</th>\n",
              "      <th>DEAL_YWZ_IR_MIN</th>\n",
              "      <th>SUM_TRAN_SUP_TENDENCY1M</th>\n",
              "      <th>DEAL_YWZ_IR_MAX</th>\n",
              "      <th>SUM_TRAN_ATM_TENDENCY1M</th>\n",
              "      <th>REST_DYNAMIC_PAYM_1M</th>\n",
              "      <th>CNT_TRAN_SUP_TENDENCY1M</th>\n",
              "      <th>DEAL_GRACE_DAYS_ACC_AVG</th>\n",
              "      <th>TURNOVER_DYNAMIC_CUR_3M</th>\n",
              "      <th>PACK</th>\n",
              "      <th>MAX_PCLOSE_DATE</th>\n",
              "      <th>LDEAL_YQZ_PC</th>\n",
              "      <th>CLNT_SETUP_TENOR</th>\n",
              "      <th>DEAL_GRACE_DAYS_ACC_MAX</th>\n",
              "      <th>TURNOVER_DYNAMIC_PAYM_3M</th>\n",
              "      <th>LDEAL_DELINQ_PER_MAXYQZ</th>\n",
              "      <th>TURNOVER_DYNAMIC_PAYM_1M</th>\n",
              "      <th>CLNT_SALARY_VALUE</th>\n",
              "      <th>TRANS_AMOUNT_TENDENCY3M</th>\n",
              "      <th>MED_DEBT_PRC_YQZ</th>\n",
              "      <th>TRANS_CNT_TENDENCY3M</th>\n",
              "      <th>LDEAL_USED_AMT_AVG_YQZ</th>\n",
              "      <th>REST_DYNAMIC_CC_1M</th>\n",
              "      <th>LDEAL_USED_AMT_AVG_YWZ</th>\n",
              "      <th>TURNOVER_DYNAMIC_CC_1M</th>\n",
              "      <th>AVG_PCT_DEBT_TO_DEAL_AMT</th>\n",
              "      <th>LDEAL_ACT_DAYS_ACC_PCT_AVG</th>\n",
              "      <th>REST_DYNAMIC_CC_3M</th>\n",
              "      <th>MED_DEBT_PRC_YWZ</th>\n",
              "      <th>LDEAL_ACT_DAYS_PCT_TR3</th>\n",
              "      <th>LDEAL_ACT_DAYS_PCT_AAVG</th>\n",
              "      <th>LDEAL_DELINQ_PER_MAXYWZ</th>\n",
              "      <th>TURNOVER_DYNAMIC_CC_3M</th>\n",
              "      <th>LDEAL_ACT_DAYS_PCT_TR</th>\n",
              "      <th>LDEAL_ACT_DAYS_PCT_TR4</th>\n",
              "      <th>LDEAL_ACT_DAYS_PCT_CURR</th>\n",
              "      <th>TARGET</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>355190.0</td>\n",
              "      <td>355190.0</td>\n",
              "      <td>316867.0</td>\n",
              "      <td>155163.0</td>\n",
              "      <td>60550.0</td>\n",
              "      <td>155163.0</td>\n",
              "      <td>155163.0</td>\n",
              "      <td>355190.0</td>\n",
              "      <td>77112.0</td>\n",
              "      <td>77112.0</td>\n",
              "      <td>316867.0</td>\n",
              "      <td>155163.0</td>\n",
              "      <td>111052.0</td>\n",
              "      <td>69421</td>\n",
              "      <td>155163.0</td>\n",
              "      <td>155163.0</td>\n",
              "      <td>355190.0</td>\n",
              "      <td>111052.0</td>\n",
              "      <td>155163.0</td>\n",
              "      <td>68234</td>\n",
              "      <td>355190.0</td>\n",
              "      <td>355190.0</td>\n",
              "      <td>355190.0</td>\n",
              "      <td>68967.0</td>\n",
              "      <td>59361</td>\n",
              "      <td>44781</td>\n",
              "      <td>316867.0</td>\n",
              "      <td>210811</td>\n",
              "      <td>57257</td>\n",
              "      <td>316867.0</td>\n",
              "      <td>68104</td>\n",
              "      <td>66296.0</td>\n",
              "      <td>68967.0</td>\n",
              "      <td>155163.0</td>\n",
              "      <td>155163.0</td>\n",
              "      <td>316867.0</td>\n",
              "      <td>57257</td>\n",
              "      <td>155163.0</td>\n",
              "      <td>355190.0</td>\n",
              "      <td>57256</td>\n",
              "      <td>355190.0</td>\n",
              "      <td>66296.0</td>\n",
              "      <td>60545</td>\n",
              "      <td>355190.0</td>\n",
              "      <td>316867.0</td>\n",
              "      <td>316867.0</td>\n",
              "      <td>355190.0</td>\n",
              "      <td>355190.0</td>\n",
              "      <td>115877.0</td>\n",
              "      <td>355190.0</td>\n",
              "      <td>115877.0</td>\n",
              "      <td>355190.0</td>\n",
              "      <td>355190.0</td>\n",
              "      <td>67362</td>\n",
              "      <td>114898.0</td>\n",
              "      <td>8001.0</td>\n",
              "      <td>1241.0</td>\n",
              "      <td>355190.0</td>\n",
              "      <td>8001.0</td>\n",
              "      <td>1240.0</td>\n",
              "      <td>8001.0</td>\n",
              "      <td>114898.0</td>\n",
              "      <td>355190.0</td>\n",
              "      <td>355190.0</td>\n",
              "      <td>8001.0</td>\n",
              "      <td>1888.0</td>\n",
              "      <td>67362</td>\n",
              "      <td>355190.0</td>\n",
              "      <td>355190.0</td>\n",
              "      <td>198718.0</td>\n",
              "      <td>355190.0</td>\n",
              "      <td>355190.0</td>\n",
              "      <td>198718.0</td>\n",
              "      <td>355190.0</td>\n",
              "      <td>255595.0</td>\n",
              "      <td>205874.0</td>\n",
              "      <td>355190.0</td>\n",
              "      <td>255595.0</td>\n",
              "      <td>70449.0</td>\n",
              "      <td>1628.0</td>\n",
              "      <td>95713.0</td>\n",
              "      <td>155995.0</td>\n",
              "      <td>95713.0</td>\n",
              "      <td>205874.0</td>\n",
              "      <td>355190.0</td>\n",
              "      <td>155995.0</td>\n",
              "      <td>69433.0</td>\n",
              "      <td>355190.0</td>\n",
              "      <td>355190</td>\n",
              "      <td>1881.0</td>\n",
              "      <td>2808.0</td>\n",
              "      <td>355190.0</td>\n",
              "      <td>69433.0</td>\n",
              "      <td>355190.0</td>\n",
              "      <td>8001.0</td>\n",
              "      <td>355190.0</td>\n",
              "      <td>712.0</td>\n",
              "      <td>303194.0</td>\n",
              "      <td>8001.0</td>\n",
              "      <td>303194.0</td>\n",
              "      <td>8001.0</td>\n",
              "      <td>355190.0</td>\n",
              "      <td>95713.0</td>\n",
              "      <td>355190.0</td>\n",
              "      <td>1888.0</td>\n",
              "      <td>93448.0</td>\n",
              "      <td>355190.0</td>\n",
              "      <td>95713.0</td>\n",
              "      <td>93448.0</td>\n",
              "      <td>98175.0</td>\n",
              "      <td>95713.0</td>\n",
              "      <td>355190.0</td>\n",
              "      <td>93448.0</td>\n",
              "      <td>93448.0</td>\n",
              "      <td>93448.0</td>\n",
              "      <td>355190.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>21</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>19588</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>17</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>FRIEND</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>M</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>SO</td>\n",
              "      <td>SPECIALIST</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ДИРЕКТОР</td>\n",
              "      <td>N</td>\n",
              "      <td>NaN</td>\n",
              "      <td>H</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>N</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>N</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>SPECIALIST</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>PRIVATE</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>PRIVATE</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>102</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>24896</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>30724</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>28056</td>\n",
              "      <td>25123</td>\n",
              "      <td>NaN</td>\n",
              "      <td>11200</td>\n",
              "      <td>36332</td>\n",
              "      <td>NaN</td>\n",
              "      <td>42459</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>52750</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>32843</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>36622</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>59087</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>59087</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>116986</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>368794.674875</td>\n",
              "      <td>0.105225</td>\n",
              "      <td>0.044045</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.947498</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001305</td>\n",
              "      <td>0.416896</td>\n",
              "      <td>0.414572</td>\n",
              "      <td>0.085249</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.68908</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000723</td>\n",
              "      <td>0.691702</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0634</td>\n",
              "      <td>0.031214</td>\n",
              "      <td>66358.544527</td>\n",
              "      <td>0.443912</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.023971</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.191057</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.479829</td>\n",
              "      <td>0.437471</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.050886</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.30993</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.056863</td>\n",
              "      <td>0.472949</td>\n",
              "      <td>NaN</td>\n",
              "      <td>496.377249</td>\n",
              "      <td>0.370588</td>\n",
              "      <td>0.598252</td>\n",
              "      <td>14467.664032</td>\n",
              "      <td>457.148073</td>\n",
              "      <td>0.700676</td>\n",
              "      <td>0.071531</td>\n",
              "      <td>0.696133</td>\n",
              "      <td>0.003942</td>\n",
              "      <td>0.001648</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.697152</td>\n",
              "      <td>22.366329</td>\n",
              "      <td>0.005201</td>\n",
              "      <td>0.004944</td>\n",
              "      <td>26.303878</td>\n",
              "      <td>0.072366</td>\n",
              "      <td>25.594477</td>\n",
              "      <td>0.699966</td>\n",
              "      <td>0.212339</td>\n",
              "      <td>7108.81579</td>\n",
              "      <td>15.799525</td>\n",
              "      <td>194395.51737</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.001809</td>\n",
              "      <td>0.498813</td>\n",
              "      <td>0.642714</td>\n",
              "      <td>0.204254</td>\n",
              "      <td>0.07657</td>\n",
              "      <td>0.627334</td>\n",
              "      <td>0.00565</td>\n",
              "      <td>0.628063</td>\n",
              "      <td>0.327222</td>\n",
              "      <td>0.003976</td>\n",
              "      <td>0.617506</td>\n",
              "      <td>0.028795</td>\n",
              "      <td>-2.673593</td>\n",
              "      <td>37.204468</td>\n",
              "      <td>0.327835</td>\n",
              "      <td>39.498242</td>\n",
              "      <td>0.322222</td>\n",
              "      <td>0.027683</td>\n",
              "      <td>0.340662</td>\n",
              "      <td>0.024472</td>\n",
              "      <td>0.484825</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-21.320917</td>\n",
              "      <td>0.023978</td>\n",
              "      <td>4.377001</td>\n",
              "      <td>0.029233</td>\n",
              "      <td>0.071906</td>\n",
              "      <td>0.195601</td>\n",
              "      <td>0.025108</td>\n",
              "      <td>37060.533806</td>\n",
              "      <td>0.582237</td>\n",
              "      <td>0.92076</td>\n",
              "      <td>0.597247</td>\n",
              "      <td>0.432061</td>\n",
              "      <td>0.002191</td>\n",
              "      <td>0.901</td>\n",
              "      <td>0.000883</td>\n",
              "      <td>0.322192</td>\n",
              "      <td>0.051419</td>\n",
              "      <td>0.007309</td>\n",
              "      <td>0.055074</td>\n",
              "      <td>0.025707</td>\n",
              "      <td>0.049943</td>\n",
              "      <td>0.009252</td>\n",
              "      <td>0.004309</td>\n",
              "      <td>0.013938</td>\n",
              "      <td>0.013938</td>\n",
              "      <td>0.013938</td>\n",
              "      <td>0.081435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>128148.804566</td>\n",
              "      <td>0.431372</td>\n",
              "      <td>0.108449</td>\n",
              "      <td>0.0</td>\n",
              "      <td>21.777855</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.029118</td>\n",
              "      <td>0.316493</td>\n",
              "      <td>0.338612</td>\n",
              "      <td>0.14231</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.301725</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.014081</td>\n",
              "      <td>0.276582</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.202963</td>\n",
              "      <td>0.184059</td>\n",
              "      <td>187859.770747</td>\n",
              "      <td>0.323898</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.090774</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.19582</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.330155</td>\n",
              "      <td>0.361986</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.116497</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.588759</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.296068</td>\n",
              "      <td>0.361476</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12340.83026</td>\n",
              "      <td>0.34341</td>\n",
              "      <td>0.363546</td>\n",
              "      <td>145937.81578</td>\n",
              "      <td>136.435457</td>\n",
              "      <td>0.28179</td>\n",
              "      <td>0.294748</td>\n",
              "      <td>0.322099</td>\n",
              "      <td>0.045568</td>\n",
              "      <td>0.024114</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.322353</td>\n",
              "      <td>17.480072</td>\n",
              "      <td>0.008383</td>\n",
              "      <td>0.076439</td>\n",
              "      <td>14.74645</td>\n",
              "      <td>0.106017</td>\n",
              "      <td>14.261929</td>\n",
              "      <td>0.289665</td>\n",
              "      <td>0.236098</td>\n",
              "      <td>47167.847223</td>\n",
              "      <td>13.148424</td>\n",
              "      <td>475613.043792</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.039279</td>\n",
              "      <td>0.299354</td>\n",
              "      <td>0.267432</td>\n",
              "      <td>0.254973</td>\n",
              "      <td>0.204059</td>\n",
              "      <td>0.294924</td>\n",
              "      <td>0.056637</td>\n",
              "      <td>0.263095</td>\n",
              "      <td>0.274738</td>\n",
              "      <td>0.052736</td>\n",
              "      <td>0.302799</td>\n",
              "      <td>0.145742</td>\n",
              "      <td>4.516854</td>\n",
              "      <td>12.446143</td>\n",
              "      <td>0.30134</td>\n",
              "      <td>10.5039</td>\n",
              "      <td>0.306137</td>\n",
              "      <td>0.097298</td>\n",
              "      <td>0.28128</td>\n",
              "      <td>0.140826</td>\n",
              "      <td>0.331606</td>\n",
              "      <td>NaN</td>\n",
              "      <td>28.965629</td>\n",
              "      <td>0.04731</td>\n",
              "      <td>2.93653</td>\n",
              "      <td>0.175279</td>\n",
              "      <td>0.206863</td>\n",
              "      <td>0.386715</td>\n",
              "      <td>0.104482</td>\n",
              "      <td>55084.111834</td>\n",
              "      <td>0.282157</td>\n",
              "      <td>0.264637</td>\n",
              "      <td>0.247131</td>\n",
              "      <td>0.322284</td>\n",
              "      <td>0.02636</td>\n",
              "      <td>0.296082</td>\n",
              "      <td>0.027321</td>\n",
              "      <td>0.363298</td>\n",
              "      <td>0.13496</td>\n",
              "      <td>0.066681</td>\n",
              "      <td>0.215909</td>\n",
              "      <td>0.115732</td>\n",
              "      <td>0.18583</td>\n",
              "      <td>0.092789</td>\n",
              "      <td>0.059852</td>\n",
              "      <td>0.097099</td>\n",
              "      <td>0.097099</td>\n",
              "      <td>0.097099</td>\n",
              "      <td>0.273503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>146841.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.006944</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.010309</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.010417</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>0.032258</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.019608</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.008696</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.005525</td>\n",
              "      <td>0.003171</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-33.623656</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.004255</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-103.354839</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.345592</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.005747</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>257846.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.139645</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.446269</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4070.440479</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.118692</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.147808</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.083916</td>\n",
              "      <td>0.268672</td>\n",
              "      <td>0.0</td>\n",
              "      <td>348.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.426034</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.423332</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.000214</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17.49</td>\n",
              "      <td>0.007634</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.063104</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.317004</td>\n",
              "      <td>0.448276</td>\n",
              "      <td>0.014579</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.402697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.433333</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.39119</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-3.227529</td>\n",
              "      <td>25.99</td>\n",
              "      <td>0.11174</td>\n",
              "      <td>45.0</td>\n",
              "      <td>0.100436</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.218582</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-33.451613</td>\n",
              "      <td>0.006047</td>\n",
              "      <td>1.781187</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10178.715</td>\n",
              "      <td>0.383673</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.133742</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>368778.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.027117</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.722985</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16289.253092</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.147059</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.363636</td>\n",
              "      <td>0.31017</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.363395</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.689935</td>\n",
              "      <td>0.0</td>\n",
              "      <td>432.0</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.794423</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.795943</td>\n",
              "      <td>18.0</td>\n",
              "      <td>0.001633</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>0.01726</td>\n",
              "      <td>22.99</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.148145</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>8652.93</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.492649</td>\n",
              "      <td>0.606742</td>\n",
              "      <td>0.127528</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.604691</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.588235</td>\n",
              "      <td>0.227273</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.599272</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.941628</td>\n",
              "      <td>45.0</td>\n",
              "      <td>0.212212</td>\n",
              "      <td>45.0</td>\n",
              "      <td>0.203741</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.23913</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.494372</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-10.16129</td>\n",
              "      <td>0.016745</td>\n",
              "      <td>3.894098</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19396.33</td>\n",
              "      <td>0.552169</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.558824</td>\n",
              "      <td>0.365802</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.089863</td>\n",
              "      <td>0.008822</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>479737.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.036608</td>\n",
              "      <td>0.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.661195</td>\n",
              "      <td>0.110005</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>56595.163421</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.010536</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.310345</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.803627</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.048276</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.897347</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.954526</td>\n",
              "      <td>0.0</td>\n",
              "      <td>552.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>0.006617</td>\n",
              "      <td>0.0</td>\n",
              "      <td>29.98</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>29.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.251909</td>\n",
              "      <td>0.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>178311.74</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.690929</td>\n",
              "      <td>0.956522</td>\n",
              "      <td>0.255788</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.98595</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.96701</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.214083</td>\n",
              "      <td>45.0</td>\n",
              "      <td>0.440708</td>\n",
              "      <td>45.0</td>\n",
              "      <td>0.440194</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.726874</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-3.612903</td>\n",
              "      <td>0.033154</td>\n",
              "      <td>6.555388</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>39234.5</td>\n",
              "      <td>0.820226</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.766225</td>\n",
              "      <td>0.720972</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.697071</td>\n",
              "      <td>0.033563</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>590828.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>89.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>22021406.769183</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3753042.345</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>46280439.311667</td>\n",
              "      <td>1128.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>0.081181</td>\n",
              "      <td>4.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5798600.31145</td>\n",
              "      <td>182.0</td>\n",
              "      <td>5579639.262</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.379252</td>\n",
              "      <td>0.99552</td>\n",
              "      <td>59.9</td>\n",
              "      <td>1.0</td>\n",
              "      <td>59.9</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.535714</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>112.193548</td>\n",
              "      <td>1.287601</td>\n",
              "      <td>13.748937</td>\n",
              "      <td>9.071429</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>487300.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.226928</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dtype</th>\n",
              "      <td>int64</td>\n",
              "      <td>int64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>object</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>object</td>\n",
              "      <td>float64</td>\n",
              "      <td>int64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>object</td>\n",
              "      <td>object</td>\n",
              "      <td>float64</td>\n",
              "      <td>object</td>\n",
              "      <td>object</td>\n",
              "      <td>float64</td>\n",
              "      <td>object</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>object</td>\n",
              "      <td>float64</td>\n",
              "      <td>int64</td>\n",
              "      <td>object</td>\n",
              "      <td>int64</td>\n",
              "      <td>float64</td>\n",
              "      <td>object</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>int64</td>\n",
              "      <td>float64</td>\n",
              "      <td>int64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>object</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>int64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>object</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>object</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>size</th>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "      <td>355190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>%missing</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.107894</td>\n",
              "      <td>0.563155</td>\n",
              "      <td>0.829528</td>\n",
              "      <td>0.563155</td>\n",
              "      <td>0.563155</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.782899</td>\n",
              "      <td>0.782899</td>\n",
              "      <td>0.107894</td>\n",
              "      <td>0.563155</td>\n",
              "      <td>0.687345</td>\n",
              "      <td>0.804552</td>\n",
              "      <td>0.563155</td>\n",
              "      <td>0.563155</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.687345</td>\n",
              "      <td>0.563155</td>\n",
              "      <td>0.807894</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.805831</td>\n",
              "      <td>0.832875</td>\n",
              "      <td>0.873924</td>\n",
              "      <td>0.107894</td>\n",
              "      <td>0.406484</td>\n",
              "      <td>0.838799</td>\n",
              "      <td>0.107894</td>\n",
              "      <td>0.80826</td>\n",
              "      <td>0.813351</td>\n",
              "      <td>0.805831</td>\n",
              "      <td>0.563155</td>\n",
              "      <td>0.563155</td>\n",
              "      <td>0.107894</td>\n",
              "      <td>0.838799</td>\n",
              "      <td>0.563155</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.838802</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.813351</td>\n",
              "      <td>0.829542</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.107894</td>\n",
              "      <td>0.107894</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.673761</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.673761</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.810349</td>\n",
              "      <td>0.676517</td>\n",
              "      <td>0.977474</td>\n",
              "      <td>0.996506</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.977474</td>\n",
              "      <td>0.996509</td>\n",
              "      <td>0.977474</td>\n",
              "      <td>0.676517</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.977474</td>\n",
              "      <td>0.994685</td>\n",
              "      <td>0.810349</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.44053</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.44053</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.280399</td>\n",
              "      <td>0.420383</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.280399</td>\n",
              "      <td>0.801658</td>\n",
              "      <td>0.995417</td>\n",
              "      <td>0.73053</td>\n",
              "      <td>0.560813</td>\n",
              "      <td>0.73053</td>\n",
              "      <td>0.420383</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.560813</td>\n",
              "      <td>0.804519</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.994704</td>\n",
              "      <td>0.992094</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.804519</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.977474</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.997995</td>\n",
              "      <td>0.146389</td>\n",
              "      <td>0.977474</td>\n",
              "      <td>0.146389</td>\n",
              "      <td>0.977474</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.73053</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.994685</td>\n",
              "      <td>0.736907</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.73053</td>\n",
              "      <td>0.736907</td>\n",
              "      <td>0.723599</td>\n",
              "      <td>0.73053</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.736907</td>\n",
              "      <td>0.736907</td>\n",
              "      <td>0.736907</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     ID CR_PROD_CNT_IL AMOUNT_RUB_CLO_PRC  \\\n",
              "count          355190.0       355190.0           316867.0   \n",
              "unique              NaN            NaN                NaN   \n",
              "top                 NaN            NaN                NaN   \n",
              "freq                NaN            NaN                NaN   \n",
              "mean      368794.674875       0.105225           0.044045   \n",
              "std       128148.804566       0.431372           0.108449   \n",
              "min            146841.0            0.0                0.0   \n",
              "25%           257846.25            0.0                0.0   \n",
              "50%            368778.5            0.0                0.0   \n",
              "75%           479737.75            0.0           0.036608   \n",
              "max            590828.0           11.0                1.0   \n",
              "dtype             int64          int64            float64   \n",
              "size             355190         355190             355190   \n",
              "%missing            0.0            0.0           0.107894   \n",
              "\n",
              "         PRC_ACCEPTS_A_EMAIL_LINK APP_REGISTR_RGN_CODE PRC_ACCEPTS_A_POS  \\\n",
              "count                    155163.0              60550.0          155163.0   \n",
              "unique                        NaN                  NaN               NaN   \n",
              "top                           NaN                  NaN               NaN   \n",
              "freq                          NaN                  NaN               NaN   \n",
              "mean                          0.0            50.947498               0.0   \n",
              "std                           0.0            21.777855               0.0   \n",
              "min                           0.0                  0.0               0.0   \n",
              "25%                           0.0                 33.0               0.0   \n",
              "50%                           0.0                 54.0               0.0   \n",
              "75%                           0.0                 72.0               0.0   \n",
              "max                           0.0                 89.0               0.0   \n",
              "dtype                     float64              float64           float64   \n",
              "size                       355190               355190            355190   \n",
              "%missing                 0.563155             0.829528          0.563155   \n",
              "\n",
              "         PRC_ACCEPTS_A_TK TURNOVER_DYNAMIC_IL_1M CNT_TRAN_AUT_TENDENCY1M  \\\n",
              "count            155163.0               355190.0                 77112.0   \n",
              "unique                NaN                    NaN                     NaN   \n",
              "top                   NaN                    NaN                     NaN   \n",
              "freq                  NaN                    NaN                     NaN   \n",
              "mean                  0.0               0.001305                0.416896   \n",
              "std                   0.0               0.029118                0.316493   \n",
              "min                   0.0                    0.0                0.006944   \n",
              "25%                   0.0                    0.0                0.166667   \n",
              "50%                   0.0                    0.0                     0.3   \n",
              "75%                   0.0                    0.0                0.571429   \n",
              "max                   0.0                    1.0                     1.0   \n",
              "dtype             float64                float64                 float64   \n",
              "size               355190                 355190                  355190   \n",
              "%missing         0.563155                    0.0                0.782899   \n",
              "\n",
              "         SUM_TRAN_AUT_TENDENCY1M AMOUNT_RUB_SUP_PRC PRC_ACCEPTS_A_AMOBILE  \\\n",
              "count                    77112.0           316867.0              155163.0   \n",
              "unique                       NaN                NaN                   NaN   \n",
              "top                          NaN                NaN                   NaN   \n",
              "freq                         NaN                NaN                   NaN   \n",
              "mean                    0.414572           0.085249                   0.0   \n",
              "std                     0.338612            0.14231                   0.0   \n",
              "min                          0.0                0.0                   0.0   \n",
              "25%                     0.139645                0.0                   0.0   \n",
              "50%                     0.285714           0.027117                   0.0   \n",
              "75%                     0.661195           0.110005                   0.0   \n",
              "max                          1.0                1.0                   0.0   \n",
              "dtype                    float64            float64               float64   \n",
              "size                      355190             355190                355190   \n",
              "%missing                0.782899           0.107894              0.563155   \n",
              "\n",
              "         SUM_TRAN_AUT_TENDENCY3M CLNT_TRUST_RELATION PRC_ACCEPTS_TK  \\\n",
              "count                   111052.0               69421       155163.0   \n",
              "unique                       NaN                  21            NaN   \n",
              "top                          NaN              FRIEND            NaN   \n",
              "freq                         NaN               24896            NaN   \n",
              "mean                     0.68908                 NaN            0.0   \n",
              "std                     0.301725                 NaN            0.0   \n",
              "min                     0.000002                 NaN            0.0   \n",
              "25%                     0.446269                 NaN            0.0   \n",
              "50%                     0.722985                 NaN            0.0   \n",
              "75%                          1.0                 NaN            0.0   \n",
              "max                          1.0                 NaN            0.0   \n",
              "dtype                    float64              object        float64   \n",
              "size                      355190              355190         355190   \n",
              "%missing                0.687345            0.804552       0.563155   \n",
              "\n",
              "         PRC_ACCEPTS_A_MTP REST_DYNAMIC_FDEP_1M CNT_TRAN_AUT_TENDENCY3M  \\\n",
              "count             155163.0             355190.0                111052.0   \n",
              "unique                 NaN                  NaN                     NaN   \n",
              "top                    NaN                  NaN                     NaN   \n",
              "freq                   NaN                  NaN                     NaN   \n",
              "mean                   0.0             0.000723                0.691702   \n",
              "std                    0.0             0.014081                0.276582   \n",
              "min                    0.0                  0.0                   0.008   \n",
              "25%                    0.0                  0.0                     0.5   \n",
              "50%                    0.0                  0.0                0.666667   \n",
              "75%                    0.0                  0.0                     1.0   \n",
              "max                    0.0                  1.0                     1.0   \n",
              "dtype              float64              float64                 float64   \n",
              "size                355190               355190                  355190   \n",
              "%missing          0.563155                  0.0                0.687345   \n",
              "\n",
              "         CNT_ACCEPTS_TK APP_MARITAL_STATUS REST_DYNAMIC_SAVE_3M  \\\n",
              "count          155163.0              68234             355190.0   \n",
              "unique              NaN                 13                  NaN   \n",
              "top                 NaN                  M                  NaN   \n",
              "freq                NaN              30724                  NaN   \n",
              "mean                0.0                NaN               0.0634   \n",
              "std                 0.0                NaN             0.202963   \n",
              "min                 0.0                NaN                  0.0   \n",
              "25%                 0.0                NaN                  0.0   \n",
              "50%                 0.0                NaN                  0.0   \n",
              "75%                 0.0                NaN                  0.0   \n",
              "max                 0.0                NaN                  1.0   \n",
              "dtype           float64             object              float64   \n",
              "size             355190             355190               355190   \n",
              "%missing       0.563155           0.807894                  0.0   \n",
              "\n",
              "         CR_PROD_CNT_VCU     REST_AVG_CUR CNT_TRAN_MED_TENDENCY1M  \\\n",
              "count           355190.0         355190.0                 68967.0   \n",
              "unique               NaN              NaN                     NaN   \n",
              "top                  NaN              NaN                     NaN   \n",
              "freq                 NaN              NaN                     NaN   \n",
              "mean            0.031214     66358.544527                0.443912   \n",
              "std             0.184059    187859.770747                0.323898   \n",
              "min                  0.0              0.0                0.010309   \n",
              "25%                  0.0      4070.440479                0.181818   \n",
              "50%                  0.0     16289.253092                0.333333   \n",
              "75%                  0.0     56595.163421                0.666667   \n",
              "max                  4.0  22021406.769183                     1.0   \n",
              "dtype              int64          float64                 float64   \n",
              "size              355190           355190                  355190   \n",
              "%missing             0.0              0.0                0.805831   \n",
              "\n",
              "         APP_KIND_OF_PROP_HABITATION CLNT_JOB_POSITION_TYPE  \\\n",
              "count                          59361                  44781   \n",
              "unique                             5                      4   \n",
              "top                               SO             SPECIALIST   \n",
              "freq                           28056                  25123   \n",
              "mean                             NaN                    NaN   \n",
              "std                              NaN                    NaN   \n",
              "min                              NaN                    NaN   \n",
              "25%                              NaN                    NaN   \n",
              "50%                              NaN                    NaN   \n",
              "75%                              NaN                    NaN   \n",
              "max                              NaN                    NaN   \n",
              "dtype                         object                 object   \n",
              "size                          355190                 355190   \n",
              "%missing                    0.832875               0.873924   \n",
              "\n",
              "         AMOUNT_RUB_NAS_PRC CLNT_JOB_POSITION APP_DRIVING_LICENSE  \\\n",
              "count              316867.0            210811               57257   \n",
              "unique                  NaN             19588                   2   \n",
              "top                     NaN          ДИРЕКТОР                   N   \n",
              "freq                    NaN             11200               36332   \n",
              "mean               0.023971               NaN                 NaN   \n",
              "std                0.090774               NaN                 NaN   \n",
              "min                     0.0               NaN                 NaN   \n",
              "25%                     0.0               NaN                 NaN   \n",
              "50%                     0.0               NaN                 NaN   \n",
              "75%                0.010536               NaN                 NaN   \n",
              "max                     1.0               NaN                 NaN   \n",
              "dtype               float64            object              object   \n",
              "size                 355190            355190              355190   \n",
              "%missing           0.107894          0.406484            0.838799   \n",
              "\n",
              "         TRANS_COUNT_SUP_PRC APP_EDUCATION CNT_TRAN_CLO_TENDENCY1M  \\\n",
              "count               316867.0         68104                 66296.0   \n",
              "unique                   NaN            17                     NaN   \n",
              "top                      NaN             H                     NaN   \n",
              "freq                     NaN         42459                     NaN   \n",
              "mean                0.191057           NaN                0.479829   \n",
              "std                  0.19582           NaN                0.330155   \n",
              "min                      0.0           NaN                0.010417   \n",
              "25%                      0.0           NaN                     0.2   \n",
              "50%                 0.147059           NaN                0.363636   \n",
              "75%                 0.310345           NaN                    0.75   \n",
              "max                      1.0           NaN                     1.0   \n",
              "dtype                float64        object                 float64   \n",
              "size                  355190        355190                  355190   \n",
              "%missing            0.107894       0.80826                0.813351   \n",
              "\n",
              "         SUM_TRAN_MED_TENDENCY1M PRC_ACCEPTS_A_ATM PRC_ACCEPTS_MTP  \\\n",
              "count                    68967.0          155163.0        155163.0   \n",
              "unique                       NaN               NaN             NaN   \n",
              "top                          NaN               NaN             NaN   \n",
              "freq                         NaN               NaN             NaN   \n",
              "mean                    0.437471               0.0             0.0   \n",
              "std                     0.361986               0.0             0.0   \n",
              "min                          0.0               0.0             0.0   \n",
              "25%                     0.118692               0.0             0.0   \n",
              "50%                      0.31017               0.0             0.0   \n",
              "75%                     0.803627               0.0             0.0   \n",
              "max                          1.0               0.0             0.0   \n",
              "dtype                    float64           float64         float64   \n",
              "size                      355190            355190          355190   \n",
              "%missing                0.805831          0.563155        0.563155   \n",
              "\n",
              "         TRANS_COUNT_NAS_PRC APP_TRAVEL_PASS CNT_ACCEPTS_MTP CR_PROD_CNT_TOVR  \\\n",
              "count               316867.0           57257        155163.0         355190.0   \n",
              "unique                   NaN               2             NaN              NaN   \n",
              "top                      NaN               N             NaN              NaN   \n",
              "freq                     NaN           52750             NaN              NaN   \n",
              "mean                0.050886             NaN             0.0          0.30993   \n",
              "std                 0.116497             NaN             0.0         0.588759   \n",
              "min                      0.0             NaN             0.0              0.0   \n",
              "25%                      0.0             NaN             0.0              0.0   \n",
              "50%                      0.0             NaN             0.0              0.0   \n",
              "75%                 0.048276             NaN             0.0              1.0   \n",
              "max                      1.0             NaN             0.0             13.0   \n",
              "dtype                float64          object         float64            int64   \n",
              "size                  355190          355190          355190           355190   \n",
              "%missing            0.107894        0.838799        0.563155              0.0   \n",
              "\n",
              "           APP_CAR CR_PROD_CNT_PIL SUM_TRAN_CLO_TENDENCY1M APP_POSITION_TYPE  \\\n",
              "count        57256        355190.0                 66296.0             60545   \n",
              "unique           2             NaN                     NaN                 4   \n",
              "top              N             NaN                     NaN        SPECIALIST   \n",
              "freq         32843             NaN                     NaN             36622   \n",
              "mean           NaN        0.056863                0.472949               NaN   \n",
              "std            NaN        0.296068                0.361476               NaN   \n",
              "min            NaN             0.0                0.000001               NaN   \n",
              "25%            NaN             0.0                0.147808               NaN   \n",
              "50%            NaN             0.0                0.363395               NaN   \n",
              "75%            NaN             0.0                0.897347               NaN   \n",
              "max            NaN             9.0                     1.0               NaN   \n",
              "dtype       object           int64                 float64            object   \n",
              "size        355190          355190                  355190            355190   \n",
              "%missing  0.838802             0.0                0.813351          0.829542   \n",
              "\n",
              "          TURNOVER_CC TRANS_COUNT_ATM_PRC AMOUNT_RUB_ATM_PRC    TURNOVER_PAYM  \\\n",
              "count        355190.0            316867.0           316867.0         355190.0   \n",
              "unique            NaN                 NaN                NaN              NaN   \n",
              "top               NaN                 NaN                NaN              NaN   \n",
              "freq              NaN                 NaN                NaN              NaN   \n",
              "mean       496.377249            0.370588           0.598252     14467.664032   \n",
              "std       12340.83026             0.34341           0.363546     145937.81578   \n",
              "min               0.0                 0.0                0.0              0.0   \n",
              "25%               0.0            0.083916           0.268672              0.0   \n",
              "50%               0.0                0.25           0.689935              0.0   \n",
              "75%               0.0            0.615385           0.954526              0.0   \n",
              "max       3753042.345                 1.0                1.0  46280439.311667   \n",
              "dtype         float64             float64            float64          float64   \n",
              "size           355190              355190             355190           355190   \n",
              "%missing          0.0            0.107894           0.107894              0.0   \n",
              "\n",
              "                 AGE CNT_TRAN_MED_TENDENCY3M CR_PROD_CNT_CC  \\\n",
              "count       355190.0                115877.0       355190.0   \n",
              "unique           NaN                     NaN            NaN   \n",
              "top              NaN                     NaN            NaN   \n",
              "freq             NaN                     NaN            NaN   \n",
              "mean      457.148073                0.700676       0.071531   \n",
              "std       136.435457                 0.28179       0.294748   \n",
              "min            168.0                0.032258            0.0   \n",
              "25%            348.0                     0.5            0.0   \n",
              "50%            432.0                0.692308            0.0   \n",
              "75%            552.0                     1.0            0.0   \n",
              "max           1128.0                     1.0            9.0   \n",
              "dtype          int64                 float64          int64   \n",
              "size          355190                  355190         355190   \n",
              "%missing         0.0                0.673761            0.0   \n",
              "\n",
              "         SUM_TRAN_MED_TENDENCY3M REST_DYNAMIC_FDEP_3M REST_DYNAMIC_IL_1M  \\\n",
              "count                   115877.0             355190.0           355190.0   \n",
              "unique                       NaN                  NaN                NaN   \n",
              "top                          NaN                  NaN                NaN   \n",
              "freq                         NaN                  NaN                NaN   \n",
              "mean                    0.696133             0.003942           0.001648   \n",
              "std                     0.322099             0.045568           0.024114   \n",
              "min                          0.0                  0.0                0.0   \n",
              "25%                     0.426034                  0.0                0.0   \n",
              "50%                     0.794423                  0.0                0.0   \n",
              "75%                          1.0                  0.0                0.0   \n",
              "max                          1.0                  1.0                1.0   \n",
              "dtype                    float64              float64            float64   \n",
              "size                      355190               355190             355190   \n",
              "%missing                0.673761                  0.0                0.0   \n",
              "\n",
              "         APP_EMP_TYPE SUM_TRAN_CLO_TENDENCY3M LDEAL_TENOR_MAX LDEAL_YQZ_CHRG  \\\n",
              "count           67362                114898.0          8001.0         1241.0   \n",
              "unique              4                     NaN             NaN            NaN   \n",
              "top           PRIVATE                     NaN             NaN            NaN   \n",
              "freq            59087                     NaN             NaN            NaN   \n",
              "mean              NaN                0.697152       22.366329       0.005201   \n",
              "std               NaN                0.322353       17.480072       0.008383   \n",
              "min               NaN                     0.0             0.0            0.0   \n",
              "25%               NaN                0.423332            10.0       0.000214   \n",
              "50%               NaN                0.795943            18.0       0.001633   \n",
              "75%               NaN                     1.0            36.0       0.006617   \n",
              "max               NaN                     1.0           242.0       0.081181   \n",
              "dtype          object                 float64         float64        float64   \n",
              "size           355190                  355190          355190         355190   \n",
              "%missing     0.810349                0.676517        0.977474       0.996506   \n",
              "\n",
              "         CR_PROD_CNT_CCFP DEAL_YQZ_IR_MAX LDEAL_YQZ_COM DEAL_YQZ_IR_MIN  \\\n",
              "count            355190.0          8001.0        1240.0          8001.0   \n",
              "unique                NaN             NaN           NaN             NaN   \n",
              "top                   NaN             NaN           NaN             NaN   \n",
              "freq                  NaN             NaN           NaN             NaN   \n",
              "mean             0.004944       26.303878      0.072366       25.594477   \n",
              "std              0.076439        14.74645      0.106017       14.261929   \n",
              "min                   0.0             6.0           0.0             6.0   \n",
              "25%                   0.0           17.49      0.007634            17.0   \n",
              "50%                   0.0            23.0       0.01726           22.99   \n",
              "75%                   0.0           29.98      0.166667            29.0   \n",
              "max                   4.0            94.0           1.0            94.0   \n",
              "dtype               int64         float64       float64         float64   \n",
              "size               355190          355190        355190          355190   \n",
              "%missing              0.0        0.977474      0.996509        0.977474   \n",
              "\n",
              "         CNT_TRAN_CLO_TENDENCY3M REST_DYNAMIC_CUR_1M  REST_AVG_PAYM  \\\n",
              "count                   114898.0            355190.0       355190.0   \n",
              "unique                       NaN                 NaN            NaN   \n",
              "top                          NaN                 NaN            NaN   \n",
              "freq                         NaN                 NaN            NaN   \n",
              "mean                    0.699966            0.212339     7108.81579   \n",
              "std                     0.289665            0.236098   47167.847223   \n",
              "min                     0.019608                 0.0            0.0   \n",
              "25%                          0.5            0.063104            0.0   \n",
              "50%                     0.714286            0.148145            0.0   \n",
              "75%                          1.0            0.251909            0.0   \n",
              "max                          1.0                 1.0  5798600.31145   \n",
              "dtype                    float64             float64        float64   \n",
              "size                      355190              355190         355190   \n",
              "%missing                0.676517                 0.0            0.0   \n",
              "\n",
              "         LDEAL_TENOR_MIN LDEAL_AMT_MONTH APP_COMP_TYPE  \\\n",
              "count             8001.0          1888.0         67362   \n",
              "unique               NaN             NaN             4   \n",
              "top                  NaN             NaN       PRIVATE   \n",
              "freq                 NaN             NaN         59087   \n",
              "mean           15.799525    194395.51737           NaN   \n",
              "std            13.148424   475613.043792           NaN   \n",
              "min                  0.0             0.0           NaN   \n",
              "25%                  6.0             0.0           NaN   \n",
              "50%                 12.0         8652.93           NaN   \n",
              "75%                 24.0       178311.74           NaN   \n",
              "max                182.0     5579639.262           NaN   \n",
              "dtype            float64         float64        object   \n",
              "size              355190          355190        355190   \n",
              "%missing        0.977474        0.994685      0.810349   \n",
              "\n",
              "         LDEAL_GRACE_DAYS_PCT_MED REST_DYNAMIC_CUR_3M CNT_TRAN_SUP_TENDENCY3M  \\\n",
              "count                    355190.0            355190.0                198718.0   \n",
              "unique                        NaN                 NaN                     NaN   \n",
              "top                           NaN                 NaN                     NaN   \n",
              "freq                          NaN                 NaN                     NaN   \n",
              "mean                     0.001809            0.498813                0.642714   \n",
              "std                      0.039279            0.299354                0.267432   \n",
              "min                           0.0                 0.0                0.008696   \n",
              "25%                           0.0            0.317004                0.448276   \n",
              "50%                           0.0            0.492649                0.606742   \n",
              "75%                           0.0            0.690929                0.956522   \n",
              "max                           1.0                 1.0                     1.0   \n",
              "dtype                     float64             float64                 float64   \n",
              "size                       355190              355190                  355190   \n",
              "%missing                      0.0                 0.0                 0.44053   \n",
              "\n",
              "         TURNOVER_DYNAMIC_CUR_1M REST_DYNAMIC_PAYM_3M SUM_TRAN_SUP_TENDENCY3M  \\\n",
              "count                   355190.0             355190.0                198718.0   \n",
              "unique                       NaN                  NaN                     NaN   \n",
              "top                          NaN                  NaN                     NaN   \n",
              "freq                         NaN                  NaN                     NaN   \n",
              "mean                    0.204254              0.07657                0.627334   \n",
              "std                     0.254973             0.204059                0.294924   \n",
              "min                          0.0                  0.0                     0.0   \n",
              "25%                     0.014579                  0.0                0.402697   \n",
              "50%                     0.127528                  0.0                0.604691   \n",
              "75%                     0.255788                  0.0                 0.98595   \n",
              "max                          1.0                  1.0                     1.0   \n",
              "dtype                    float64              float64                 float64   \n",
              "size                      355190               355190                  355190   \n",
              "%missing                     0.0                  0.0                 0.44053   \n",
              "\n",
              "         REST_DYNAMIC_IL_3M CNT_TRAN_ATM_TENDENCY3M CNT_TRAN_ATM_TENDENCY1M  \\\n",
              "count              355190.0                255595.0                205874.0   \n",
              "unique                  NaN                     NaN                     NaN   \n",
              "top                     NaN                     NaN                     NaN   \n",
              "freq                    NaN                     NaN                     NaN   \n",
              "mean                0.00565                0.628063                0.327222   \n",
              "std                0.056637                0.263095                0.274738   \n",
              "min                     0.0                0.005525                0.003171   \n",
              "25%                     0.0                0.433333                0.142857   \n",
              "50%                     0.0                0.588235                0.227273   \n",
              "75%                     0.0                0.888889                     0.4   \n",
              "max                     1.0                     1.0                     1.0   \n",
              "dtype               float64                 float64                 float64   \n",
              "size                 355190                  355190                  355190   \n",
              "%missing                0.0                0.280399                0.420383   \n",
              "\n",
              "         TURNOVER_DYNAMIC_IL_3M SUM_TRAN_ATM_TENDENCY3M  \\\n",
              "count                  355190.0                255595.0   \n",
              "unique                      NaN                     NaN   \n",
              "top                         NaN                     NaN   \n",
              "freq                        NaN                     NaN   \n",
              "mean                   0.003976                0.617506   \n",
              "std                    0.052736                0.302799   \n",
              "min                         0.0                0.000021   \n",
              "25%                         0.0                 0.39119   \n",
              "50%                         0.0                0.599272   \n",
              "75%                         0.0                 0.96701   \n",
              "max                         1.0                     1.0   \n",
              "dtype                   float64                 float64   \n",
              "size                     355190                  355190   \n",
              "%missing                    0.0                0.280399   \n",
              "\n",
              "         DEAL_GRACE_DAYS_ACC_S1X1 AVG_PCT_MONTH_TO_PCLOSE DEAL_YWZ_IR_MIN  \\\n",
              "count                     70449.0                  1628.0         95713.0   \n",
              "unique                        NaN                     NaN             NaN   \n",
              "top                           NaN                     NaN             NaN   \n",
              "freq                          NaN                     NaN             NaN   \n",
              "mean                     0.028795               -2.673593       37.204468   \n",
              "std                      0.145742                4.516854       12.446143   \n",
              "min                           0.0              -33.623656             0.0   \n",
              "25%                           0.0               -3.227529           25.99   \n",
              "50%                           0.0               -0.941628            45.0   \n",
              "75%                           0.0               -0.214083            45.0   \n",
              "max                      4.379252                 0.99552            59.9   \n",
              "dtype                     float64                 float64         float64   \n",
              "size                       355190                  355190          355190   \n",
              "%missing                 0.801658                0.995417         0.73053   \n",
              "\n",
              "         SUM_TRAN_SUP_TENDENCY1M DEAL_YWZ_IR_MAX SUM_TRAN_ATM_TENDENCY1M  \\\n",
              "count                   155995.0         95713.0                205874.0   \n",
              "unique                       NaN             NaN                     NaN   \n",
              "top                          NaN             NaN                     NaN   \n",
              "freq                         NaN             NaN                     NaN   \n",
              "mean                    0.327835       39.498242                0.322222   \n",
              "std                      0.30134         10.5039                0.306137   \n",
              "min                          0.0             0.0                0.000006   \n",
              "25%                      0.11174            45.0                0.100436   \n",
              "50%                     0.212212            45.0                0.203741   \n",
              "75%                     0.440708            45.0                0.440194   \n",
              "max                          1.0            59.9                     1.0   \n",
              "dtype                    float64         float64                 float64   \n",
              "size                      355190          355190                  355190   \n",
              "%missing                0.560813         0.73053                0.420383   \n",
              "\n",
              "         REST_DYNAMIC_PAYM_1M CNT_TRAN_SUP_TENDENCY1M DEAL_GRACE_DAYS_ACC_AVG  \\\n",
              "count                355190.0                155995.0                 69433.0   \n",
              "unique                    NaN                     NaN                     NaN   \n",
              "top                       NaN                     NaN                     NaN   \n",
              "freq                      NaN                     NaN                     NaN   \n",
              "mean                 0.027683                0.340662                0.024472   \n",
              "std                  0.097298                 0.28128                0.140826   \n",
              "min                       0.0                0.004255                     0.0   \n",
              "25%                       0.0                0.142857                     0.0   \n",
              "50%                       0.0                 0.23913                     0.0   \n",
              "75%                       0.0                0.444444                     0.0   \n",
              "max                       1.0                     1.0                4.535714   \n",
              "dtype                 float64                 float64                 float64   \n",
              "size                   355190                  355190                  355190   \n",
              "%missing                  0.0                0.560813                0.804519   \n",
              "\n",
              "         TURNOVER_DYNAMIC_CUR_3M    PACK MAX_PCLOSE_DATE LDEAL_YQZ_PC  \\\n",
              "count                   355190.0  355190          1881.0       2808.0   \n",
              "unique                       NaN      12             NaN          NaN   \n",
              "top                          NaN     102             NaN          NaN   \n",
              "freq                         NaN  116986             NaN          NaN   \n",
              "mean                    0.484825     NaN      -21.320917     0.023978   \n",
              "std                     0.331606     NaN       28.965629      0.04731   \n",
              "min                          0.0     NaN     -103.354839          0.0   \n",
              "25%                     0.218582     NaN      -33.451613     0.006047   \n",
              "50%                     0.494372     NaN       -10.16129     0.016745   \n",
              "75%                     0.726874     NaN       -3.612903     0.033154   \n",
              "max                          1.0     NaN      112.193548     1.287601   \n",
              "dtype                    float64  object         float64      float64   \n",
              "size                      355190  355190          355190       355190   \n",
              "%missing                     0.0     0.0        0.994704     0.992094   \n",
              "\n",
              "         CLNT_SETUP_TENOR DEAL_GRACE_DAYS_ACC_MAX TURNOVER_DYNAMIC_PAYM_3M  \\\n",
              "count            355190.0                 69433.0                 355190.0   \n",
              "unique                NaN                     NaN                      NaN   \n",
              "top                   NaN                     NaN                      NaN   \n",
              "freq                  NaN                     NaN                      NaN   \n",
              "mean             4.377001                0.029233                 0.071906   \n",
              "std               2.93653                0.175279                 0.206863   \n",
              "min              0.345592                     0.0                      0.0   \n",
              "25%              1.781187                     0.0                      0.0   \n",
              "50%              3.894098                     0.0                      0.0   \n",
              "75%              6.555388                     0.0                      0.0   \n",
              "max             13.748937                9.071429                      1.0   \n",
              "dtype             float64                 float64                  float64   \n",
              "size               355190                  355190                   355190   \n",
              "%missing              0.0                0.804519                      0.0   \n",
              "\n",
              "         LDEAL_DELINQ_PER_MAXYQZ TURNOVER_DYNAMIC_PAYM_1M CLNT_SALARY_VALUE  \\\n",
              "count                     8001.0                 355190.0             712.0   \n",
              "unique                       NaN                      NaN               NaN   \n",
              "top                          NaN                      NaN               NaN   \n",
              "freq                         NaN                      NaN               NaN   \n",
              "mean                    0.195601                 0.025108      37060.533806   \n",
              "std                     0.386715                 0.104482      55084.111834   \n",
              "min                          0.0                      0.0              0.75   \n",
              "25%                          0.0                      0.0         10178.715   \n",
              "50%                          0.0                      0.0          19396.33   \n",
              "75%                          0.0                      0.0           39234.5   \n",
              "max                          1.0                      1.0          487300.0   \n",
              "dtype                    float64                  float64           float64   \n",
              "size                      355190                   355190            355190   \n",
              "%missing                0.977474                      0.0          0.997995   \n",
              "\n",
              "         TRANS_AMOUNT_TENDENCY3M MED_DEBT_PRC_YQZ TRANS_CNT_TENDENCY3M  \\\n",
              "count                   303194.0           8001.0             303194.0   \n",
              "unique                       NaN              NaN                  NaN   \n",
              "top                          NaN              NaN                  NaN   \n",
              "freq                         NaN              NaN                  NaN   \n",
              "mean                    0.582237          0.92076             0.597247   \n",
              "std                     0.282157         0.264637             0.247131   \n",
              "min                          0.0              0.0             0.005747   \n",
              "25%                     0.383673              1.0             0.428571   \n",
              "50%                     0.552169              1.0             0.558824   \n",
              "75%                     0.820226              1.0             0.766225   \n",
              "max                          1.0              1.0                  1.0   \n",
              "dtype                    float64          float64              float64   \n",
              "size                      355190           355190               355190   \n",
              "%missing                0.146389         0.977474             0.146389   \n",
              "\n",
              "         LDEAL_USED_AMT_AVG_YQZ REST_DYNAMIC_CC_1M LDEAL_USED_AMT_AVG_YWZ  \\\n",
              "count                    8001.0           355190.0                95713.0   \n",
              "unique                      NaN                NaN                    NaN   \n",
              "top                         NaN                NaN                    NaN   \n",
              "freq                        NaN                NaN                    NaN   \n",
              "mean                   0.432061           0.002191                  0.901   \n",
              "std                    0.322284            0.02636               0.296082   \n",
              "min                         0.0                0.0                    0.0   \n",
              "25%                    0.133742                0.0                    1.0   \n",
              "50%                    0.365802                0.0                    1.0   \n",
              "75%                    0.720972                0.0                    1.0   \n",
              "max                         1.0                1.0                    1.0   \n",
              "dtype                   float64            float64                float64   \n",
              "size                     355190             355190                 355190   \n",
              "%missing               0.977474                0.0                0.73053   \n",
              "\n",
              "         TURNOVER_DYNAMIC_CC_1M AVG_PCT_DEBT_TO_DEAL_AMT  \\\n",
              "count                  355190.0                   1888.0   \n",
              "unique                      NaN                      NaN   \n",
              "top                         NaN                      NaN   \n",
              "freq                        NaN                      NaN   \n",
              "mean                   0.000883                 0.322192   \n",
              "std                    0.027321                 0.363298   \n",
              "min                         0.0                      0.0   \n",
              "25%                         0.0                      0.0   \n",
              "50%                         0.0                 0.089863   \n",
              "75%                         0.0                 0.697071   \n",
              "max                         1.0                 1.226928   \n",
              "dtype                   float64                  float64   \n",
              "size                     355190                   355190   \n",
              "%missing                    0.0                 0.994685   \n",
              "\n",
              "         LDEAL_ACT_DAYS_ACC_PCT_AVG REST_DYNAMIC_CC_3M MED_DEBT_PRC_YWZ  \\\n",
              "count                       93448.0           355190.0          95713.0   \n",
              "unique                          NaN                NaN              NaN   \n",
              "top                             NaN                NaN              NaN   \n",
              "freq                            NaN                NaN              NaN   \n",
              "mean                       0.051419           0.007309         0.055074   \n",
              "std                         0.13496           0.066681         0.215909   \n",
              "min                             0.0                0.0              0.0   \n",
              "25%                             0.0                0.0              0.0   \n",
              "50%                        0.008822                0.0              0.0   \n",
              "75%                        0.033563                0.0              0.0   \n",
              "max                             1.0                1.0              1.0   \n",
              "dtype                       float64            float64          float64   \n",
              "size                         355190             355190           355190   \n",
              "%missing                   0.736907                0.0          0.73053   \n",
              "\n",
              "         LDEAL_ACT_DAYS_PCT_TR3 LDEAL_ACT_DAYS_PCT_AAVG  \\\n",
              "count                   93448.0                 98175.0   \n",
              "unique                      NaN                     NaN   \n",
              "top                         NaN                     NaN   \n",
              "freq                        NaN                     NaN   \n",
              "mean                   0.025707                0.049943   \n",
              "std                    0.115732                 0.18583   \n",
              "min                         0.0                     0.0   \n",
              "25%                         0.0                     0.0   \n",
              "50%                         0.0                     0.0   \n",
              "75%                         0.0                     0.0   \n",
              "max                         1.0                     1.0   \n",
              "dtype                   float64                 float64   \n",
              "size                     355190                  355190   \n",
              "%missing               0.736907                0.723599   \n",
              "\n",
              "         LDEAL_DELINQ_PER_MAXYWZ TURNOVER_DYNAMIC_CC_3M LDEAL_ACT_DAYS_PCT_TR  \\\n",
              "count                    95713.0               355190.0               93448.0   \n",
              "unique                       NaN                    NaN                   NaN   \n",
              "top                          NaN                    NaN                   NaN   \n",
              "freq                         NaN                    NaN                   NaN   \n",
              "mean                    0.009252               0.004309              0.013938   \n",
              "std                     0.092789               0.059852              0.097099   \n",
              "min                          0.0                    0.0                   0.0   \n",
              "25%                          0.0                    0.0                   0.0   \n",
              "50%                          0.0                    0.0                   0.0   \n",
              "75%                          0.0                    0.0                   0.0   \n",
              "max                          1.0                    1.0                   1.0   \n",
              "dtype                    float64                float64               float64   \n",
              "size                      355190                 355190                355190   \n",
              "%missing                 0.73053                    0.0              0.736907   \n",
              "\n",
              "         LDEAL_ACT_DAYS_PCT_TR4 LDEAL_ACT_DAYS_PCT_CURR    TARGET  \n",
              "count                   93448.0                 93448.0  355190.0  \n",
              "unique                      NaN                     NaN       NaN  \n",
              "top                         NaN                     NaN       NaN  \n",
              "freq                        NaN                     NaN       NaN  \n",
              "mean                   0.013938                0.013938  0.081435  \n",
              "std                    0.097099                0.097099  0.273503  \n",
              "min                         0.0                     0.0       0.0  \n",
              "25%                         0.0                     0.0       0.0  \n",
              "50%                         0.0                     0.0       0.0  \n",
              "75%                         0.0                     0.0       0.0  \n",
              "max                         1.0                     1.0       1.0  \n",
              "dtype                   float64                 float64     int64  \n",
              "size                     355190                  355190    355190  \n",
              "%missing               0.736907                0.736907       0.0  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#train statistic\n",
        "df_train = pd.read_csv('data/bank_data_train.csv')\n",
        "\n",
        "train_stat = df_train.describe(include = 'all')\n",
        "train_stat.loc['dtype'] = df_train.dtypes\n",
        "train_stat.loc['size'] = len(df_train)\n",
        "train_stat.loc['%missing'] = df_train.isnull().mean()\n",
        "train_stat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        },
        "id": "Tn3VfdtXt50U",
        "outputId": "d6fd5818-c232-4af2-8c6a-9db1705ace49"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>CR_PROD_CNT_IL</th>\n",
              "      <th>AMOUNT_RUB_CLO_PRC</th>\n",
              "      <th>PRC_ACCEPTS_A_EMAIL_LINK</th>\n",
              "      <th>APP_REGISTR_RGN_CODE</th>\n",
              "      <th>PRC_ACCEPTS_A_POS</th>\n",
              "      <th>PRC_ACCEPTS_A_TK</th>\n",
              "      <th>TURNOVER_DYNAMIC_IL_1M</th>\n",
              "      <th>CNT_TRAN_AUT_TENDENCY1M</th>\n",
              "      <th>SUM_TRAN_AUT_TENDENCY1M</th>\n",
              "      <th>AMOUNT_RUB_SUP_PRC</th>\n",
              "      <th>PRC_ACCEPTS_A_AMOBILE</th>\n",
              "      <th>SUM_TRAN_AUT_TENDENCY3M</th>\n",
              "      <th>CLNT_TRUST_RELATION</th>\n",
              "      <th>PRC_ACCEPTS_TK</th>\n",
              "      <th>PRC_ACCEPTS_A_MTP</th>\n",
              "      <th>REST_DYNAMIC_FDEP_1M</th>\n",
              "      <th>CNT_TRAN_AUT_TENDENCY3M</th>\n",
              "      <th>CNT_ACCEPTS_TK</th>\n",
              "      <th>APP_MARITAL_STATUS</th>\n",
              "      <th>REST_DYNAMIC_SAVE_3M</th>\n",
              "      <th>CR_PROD_CNT_VCU</th>\n",
              "      <th>REST_AVG_CUR</th>\n",
              "      <th>CNT_TRAN_MED_TENDENCY1M</th>\n",
              "      <th>APP_KIND_OF_PROP_HABITATION</th>\n",
              "      <th>CLNT_JOB_POSITION_TYPE</th>\n",
              "      <th>AMOUNT_RUB_NAS_PRC</th>\n",
              "      <th>CLNT_JOB_POSITION</th>\n",
              "      <th>APP_DRIVING_LICENSE</th>\n",
              "      <th>TRANS_COUNT_SUP_PRC</th>\n",
              "      <th>APP_EDUCATION</th>\n",
              "      <th>CNT_TRAN_CLO_TENDENCY1M</th>\n",
              "      <th>SUM_TRAN_MED_TENDENCY1M</th>\n",
              "      <th>PRC_ACCEPTS_A_ATM</th>\n",
              "      <th>PRC_ACCEPTS_MTP</th>\n",
              "      <th>TRANS_COUNT_NAS_PRC</th>\n",
              "      <th>APP_TRAVEL_PASS</th>\n",
              "      <th>CNT_ACCEPTS_MTP</th>\n",
              "      <th>CR_PROD_CNT_TOVR</th>\n",
              "      <th>APP_CAR</th>\n",
              "      <th>CR_PROD_CNT_PIL</th>\n",
              "      <th>SUM_TRAN_CLO_TENDENCY1M</th>\n",
              "      <th>APP_POSITION_TYPE</th>\n",
              "      <th>TURNOVER_CC</th>\n",
              "      <th>TRANS_COUNT_ATM_PRC</th>\n",
              "      <th>AMOUNT_RUB_ATM_PRC</th>\n",
              "      <th>TURNOVER_PAYM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>CNT_TRAN_MED_TENDENCY3M</th>\n",
              "      <th>CR_PROD_CNT_CC</th>\n",
              "      <th>SUM_TRAN_MED_TENDENCY3M</th>\n",
              "      <th>REST_DYNAMIC_FDEP_3M</th>\n",
              "      <th>REST_DYNAMIC_IL_1M</th>\n",
              "      <th>APP_EMP_TYPE</th>\n",
              "      <th>SUM_TRAN_CLO_TENDENCY3M</th>\n",
              "      <th>LDEAL_TENOR_MAX</th>\n",
              "      <th>LDEAL_YQZ_CHRG</th>\n",
              "      <th>CR_PROD_CNT_CCFP</th>\n",
              "      <th>DEAL_YQZ_IR_MAX</th>\n",
              "      <th>LDEAL_YQZ_COM</th>\n",
              "      <th>DEAL_YQZ_IR_MIN</th>\n",
              "      <th>CNT_TRAN_CLO_TENDENCY3M</th>\n",
              "      <th>REST_DYNAMIC_CUR_1M</th>\n",
              "      <th>REST_AVG_PAYM</th>\n",
              "      <th>LDEAL_TENOR_MIN</th>\n",
              "      <th>LDEAL_AMT_MONTH</th>\n",
              "      <th>APP_COMP_TYPE</th>\n",
              "      <th>LDEAL_GRACE_DAYS_PCT_MED</th>\n",
              "      <th>REST_DYNAMIC_CUR_3M</th>\n",
              "      <th>CNT_TRAN_SUP_TENDENCY3M</th>\n",
              "      <th>TURNOVER_DYNAMIC_CUR_1M</th>\n",
              "      <th>REST_DYNAMIC_PAYM_3M</th>\n",
              "      <th>SUM_TRAN_SUP_TENDENCY3M</th>\n",
              "      <th>REST_DYNAMIC_IL_3M</th>\n",
              "      <th>CNT_TRAN_ATM_TENDENCY3M</th>\n",
              "      <th>CNT_TRAN_ATM_TENDENCY1M</th>\n",
              "      <th>TURNOVER_DYNAMIC_IL_3M</th>\n",
              "      <th>SUM_TRAN_ATM_TENDENCY3M</th>\n",
              "      <th>DEAL_GRACE_DAYS_ACC_S1X1</th>\n",
              "      <th>AVG_PCT_MONTH_TO_PCLOSE</th>\n",
              "      <th>DEAL_YWZ_IR_MIN</th>\n",
              "      <th>SUM_TRAN_SUP_TENDENCY1M</th>\n",
              "      <th>DEAL_YWZ_IR_MAX</th>\n",
              "      <th>SUM_TRAN_ATM_TENDENCY1M</th>\n",
              "      <th>REST_DYNAMIC_PAYM_1M</th>\n",
              "      <th>CNT_TRAN_SUP_TENDENCY1M</th>\n",
              "      <th>DEAL_GRACE_DAYS_ACC_AVG</th>\n",
              "      <th>TURNOVER_DYNAMIC_CUR_3M</th>\n",
              "      <th>PACK</th>\n",
              "      <th>MAX_PCLOSE_DATE</th>\n",
              "      <th>LDEAL_YQZ_PC</th>\n",
              "      <th>CLNT_SETUP_TENOR</th>\n",
              "      <th>DEAL_GRACE_DAYS_ACC_MAX</th>\n",
              "      <th>TURNOVER_DYNAMIC_PAYM_3M</th>\n",
              "      <th>LDEAL_DELINQ_PER_MAXYQZ</th>\n",
              "      <th>TURNOVER_DYNAMIC_PAYM_1M</th>\n",
              "      <th>CLNT_SALARY_VALUE</th>\n",
              "      <th>TRANS_AMOUNT_TENDENCY3M</th>\n",
              "      <th>MED_DEBT_PRC_YQZ</th>\n",
              "      <th>TRANS_CNT_TENDENCY3M</th>\n",
              "      <th>LDEAL_USED_AMT_AVG_YQZ</th>\n",
              "      <th>REST_DYNAMIC_CC_1M</th>\n",
              "      <th>LDEAL_USED_AMT_AVG_YWZ</th>\n",
              "      <th>TURNOVER_DYNAMIC_CC_1M</th>\n",
              "      <th>AVG_PCT_DEBT_TO_DEAL_AMT</th>\n",
              "      <th>LDEAL_ACT_DAYS_ACC_PCT_AVG</th>\n",
              "      <th>REST_DYNAMIC_CC_3M</th>\n",
              "      <th>MED_DEBT_PRC_YWZ</th>\n",
              "      <th>LDEAL_ACT_DAYS_PCT_TR3</th>\n",
              "      <th>LDEAL_ACT_DAYS_PCT_AAVG</th>\n",
              "      <th>LDEAL_DELINQ_PER_MAXYWZ</th>\n",
              "      <th>TURNOVER_DYNAMIC_CC_3M</th>\n",
              "      <th>LDEAL_ACT_DAYS_PCT_TR</th>\n",
              "      <th>LDEAL_ACT_DAYS_PCT_TR4</th>\n",
              "      <th>LDEAL_ACT_DAYS_PCT_CURR</th>\n",
              "      <th>TARGET</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>88798.0</td>\n",
              "      <td>88798.0</td>\n",
              "      <td>79198.0</td>\n",
              "      <td>38520.0</td>\n",
              "      <td>15332.0</td>\n",
              "      <td>38520.0</td>\n",
              "      <td>38520.0</td>\n",
              "      <td>88798.0</td>\n",
              "      <td>19282.0</td>\n",
              "      <td>19282.0</td>\n",
              "      <td>79198.0</td>\n",
              "      <td>38520.0</td>\n",
              "      <td>27584.0</td>\n",
              "      <td>17497</td>\n",
              "      <td>38520.0</td>\n",
              "      <td>38520.0</td>\n",
              "      <td>88798.0</td>\n",
              "      <td>27584.0</td>\n",
              "      <td>38520.0</td>\n",
              "      <td>17208</td>\n",
              "      <td>88798.0</td>\n",
              "      <td>88798.0</td>\n",
              "      <td>88798.0</td>\n",
              "      <td>17240.0</td>\n",
              "      <td>14982</td>\n",
              "      <td>11302</td>\n",
              "      <td>79198.0</td>\n",
              "      <td>52852</td>\n",
              "      <td>14520</td>\n",
              "      <td>79198.0</td>\n",
              "      <td>17188</td>\n",
              "      <td>16500.0</td>\n",
              "      <td>17240.0</td>\n",
              "      <td>38520.0</td>\n",
              "      <td>38520.0</td>\n",
              "      <td>79198.0</td>\n",
              "      <td>14520</td>\n",
              "      <td>38520.0</td>\n",
              "      <td>88798.0</td>\n",
              "      <td>14520</td>\n",
              "      <td>88798.0</td>\n",
              "      <td>16500.0</td>\n",
              "      <td>15255</td>\n",
              "      <td>88798.0</td>\n",
              "      <td>79198.0</td>\n",
              "      <td>79198.0</td>\n",
              "      <td>88798.0</td>\n",
              "      <td>88798.0</td>\n",
              "      <td>28901.0</td>\n",
              "      <td>88798.0</td>\n",
              "      <td>28901.0</td>\n",
              "      <td>88798.0</td>\n",
              "      <td>88798.0</td>\n",
              "      <td>17001</td>\n",
              "      <td>28456.0</td>\n",
              "      <td>2108.0</td>\n",
              "      <td>333.0</td>\n",
              "      <td>88798.0</td>\n",
              "      <td>2108.0</td>\n",
              "      <td>323.0</td>\n",
              "      <td>2108.0</td>\n",
              "      <td>28456.0</td>\n",
              "      <td>88798.0</td>\n",
              "      <td>88798.0</td>\n",
              "      <td>2108.0</td>\n",
              "      <td>504.0</td>\n",
              "      <td>17001</td>\n",
              "      <td>88798.0</td>\n",
              "      <td>88798.0</td>\n",
              "      <td>49663.0</td>\n",
              "      <td>88798.0</td>\n",
              "      <td>88798.0</td>\n",
              "      <td>49663.0</td>\n",
              "      <td>88798.0</td>\n",
              "      <td>63824.0</td>\n",
              "      <td>51451.0</td>\n",
              "      <td>88798.0</td>\n",
              "      <td>63824.0</td>\n",
              "      <td>17445.0</td>\n",
              "      <td>426.0</td>\n",
              "      <td>23817.0</td>\n",
              "      <td>38927.0</td>\n",
              "      <td>23817.0</td>\n",
              "      <td>51451.0</td>\n",
              "      <td>88798.0</td>\n",
              "      <td>38927.0</td>\n",
              "      <td>17222.0</td>\n",
              "      <td>88798.0</td>\n",
              "      <td>88798</td>\n",
              "      <td>504.0</td>\n",
              "      <td>751.0</td>\n",
              "      <td>88798.0</td>\n",
              "      <td>17222.0</td>\n",
              "      <td>88798.0</td>\n",
              "      <td>2108.0</td>\n",
              "      <td>88798.0</td>\n",
              "      <td>173.0</td>\n",
              "      <td>75844.0</td>\n",
              "      <td>2108.0</td>\n",
              "      <td>75844.0</td>\n",
              "      <td>2108.0</td>\n",
              "      <td>88798.0</td>\n",
              "      <td>23817.0</td>\n",
              "      <td>88798.0</td>\n",
              "      <td>504.0</td>\n",
              "      <td>23281.0</td>\n",
              "      <td>88798.0</td>\n",
              "      <td>23817.0</td>\n",
              "      <td>23281.0</td>\n",
              "      <td>24452.0</td>\n",
              "      <td>23817.0</td>\n",
              "      <td>88798.0</td>\n",
              "      <td>23281.0</td>\n",
              "      <td>23281.0</td>\n",
              "      <td>23281.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>20</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9857</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>17</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>11</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>FRIEND</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>M</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>SO</td>\n",
              "      <td>SPECIALIST</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ДИРЕКТОР</td>\n",
              "      <td>N</td>\n",
              "      <td>NaN</td>\n",
              "      <td>H</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>N</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>N</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>SPECIALIST</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>PRIVATE</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>PRIVATE</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>102</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6387</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7783</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7111</td>\n",
              "      <td>6323</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2896</td>\n",
              "      <td>9171</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10658</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13354</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8302</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9172</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>14943</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>14943</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>29190</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>368993.799601</td>\n",
              "      <td>0.105948</td>\n",
              "      <td>0.04454</td>\n",
              "      <td>0.0</td>\n",
              "      <td>51.234151</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001395</td>\n",
              "      <td>0.417814</td>\n",
              "      <td>0.415055</td>\n",
              "      <td>0.085612</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.690076</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000751</td>\n",
              "      <td>0.692178</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.064653</td>\n",
              "      <td>0.031577</td>\n",
              "      <td>66097.052316</td>\n",
              "      <td>0.450684</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.023622</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.191779</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.47977</td>\n",
              "      <td>0.444461</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.050823</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.30923</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.057051</td>\n",
              "      <td>0.472748</td>\n",
              "      <td>NaN</td>\n",
              "      <td>492.377672</td>\n",
              "      <td>0.370995</td>\n",
              "      <td>0.597782</td>\n",
              "      <td>14390.484559</td>\n",
              "      <td>457.822338</td>\n",
              "      <td>0.703231</td>\n",
              "      <td>0.071274</td>\n",
              "      <td>0.697479</td>\n",
              "      <td>0.003828</td>\n",
              "      <td>0.001671</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.701108</td>\n",
              "      <td>22.257116</td>\n",
              "      <td>0.003808</td>\n",
              "      <td>0.004876</td>\n",
              "      <td>26.421559</td>\n",
              "      <td>0.074842</td>\n",
              "      <td>25.755292</td>\n",
              "      <td>0.703422</td>\n",
              "      <td>0.213429</td>\n",
              "      <td>7145.520329</td>\n",
              "      <td>15.765655</td>\n",
              "      <td>206577.214111</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.499548</td>\n",
              "      <td>0.645491</td>\n",
              "      <td>0.205116</td>\n",
              "      <td>0.077136</td>\n",
              "      <td>0.629988</td>\n",
              "      <td>0.005898</td>\n",
              "      <td>0.630854</td>\n",
              "      <td>0.328071</td>\n",
              "      <td>0.004375</td>\n",
              "      <td>0.619648</td>\n",
              "      <td>0.027779</td>\n",
              "      <td>-2.313821</td>\n",
              "      <td>37.211825</td>\n",
              "      <td>0.332284</td>\n",
              "      <td>39.488849</td>\n",
              "      <td>0.322246</td>\n",
              "      <td>0.027967</td>\n",
              "      <td>0.345375</td>\n",
              "      <td>0.024202</td>\n",
              "      <td>0.484631</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-19.882616</td>\n",
              "      <td>0.028175</td>\n",
              "      <td>4.372644</td>\n",
              "      <td>0.029376</td>\n",
              "      <td>0.072363</td>\n",
              "      <td>0.202324</td>\n",
              "      <td>0.025442</td>\n",
              "      <td>35077.98526</td>\n",
              "      <td>0.583426</td>\n",
              "      <td>0.919355</td>\n",
              "      <td>0.599044</td>\n",
              "      <td>0.442068</td>\n",
              "      <td>0.002143</td>\n",
              "      <td>0.900927</td>\n",
              "      <td>0.000996</td>\n",
              "      <td>0.383045</td>\n",
              "      <td>0.051227</td>\n",
              "      <td>0.007159</td>\n",
              "      <td>0.053328</td>\n",
              "      <td>0.025664</td>\n",
              "      <td>0.050934</td>\n",
              "      <td>0.009531</td>\n",
              "      <td>0.004479</td>\n",
              "      <td>0.01386</td>\n",
              "      <td>0.01386</td>\n",
              "      <td>0.01386</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>128247.549453</td>\n",
              "      <td>0.433893</td>\n",
              "      <td>0.110796</td>\n",
              "      <td>0.0</td>\n",
              "      <td>21.54533</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.030099</td>\n",
              "      <td>0.315638</td>\n",
              "      <td>0.337138</td>\n",
              "      <td>0.142358</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.300702</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.014808</td>\n",
              "      <td>0.276734</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.205522</td>\n",
              "      <td>0.183729</td>\n",
              "      <td>173380.217827</td>\n",
              "      <td>0.327417</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.089373</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.196888</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.329938</td>\n",
              "      <td>0.363818</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.117188</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.589099</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.295065</td>\n",
              "      <td>0.36107</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12593.535621</td>\n",
              "      <td>0.343645</td>\n",
              "      <td>0.364865</td>\n",
              "      <td>89822.408834</td>\n",
              "      <td>136.326732</td>\n",
              "      <td>0.28273</td>\n",
              "      <td>0.291898</td>\n",
              "      <td>0.323004</td>\n",
              "      <td>0.045202</td>\n",
              "      <td>0.024163</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.320048</td>\n",
              "      <td>16.907249</td>\n",
              "      <td>0.006145</td>\n",
              "      <td>0.075703</td>\n",
              "      <td>14.924138</td>\n",
              "      <td>0.109409</td>\n",
              "      <td>14.550783</td>\n",
              "      <td>0.288372</td>\n",
              "      <td>0.236967</td>\n",
              "      <td>45472.775178</td>\n",
              "      <td>12.891542</td>\n",
              "      <td>589978.528038</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.038435</td>\n",
              "      <td>0.299771</td>\n",
              "      <td>0.267797</td>\n",
              "      <td>0.25607</td>\n",
              "      <td>0.205299</td>\n",
              "      <td>0.294779</td>\n",
              "      <td>0.058358</td>\n",
              "      <td>0.263788</td>\n",
              "      <td>0.27461</td>\n",
              "      <td>0.055848</td>\n",
              "      <td>0.303434</td>\n",
              "      <td>0.142541</td>\n",
              "      <td>3.888261</td>\n",
              "      <td>12.393002</td>\n",
              "      <td>0.302832</td>\n",
              "      <td>10.463102</td>\n",
              "      <td>0.305741</td>\n",
              "      <td>0.09798</td>\n",
              "      <td>0.283195</td>\n",
              "      <td>0.138537</td>\n",
              "      <td>0.332864</td>\n",
              "      <td>NaN</td>\n",
              "      <td>28.154802</td>\n",
              "      <td>0.091066</td>\n",
              "      <td>2.930534</td>\n",
              "      <td>0.16852</td>\n",
              "      <td>0.207623</td>\n",
              "      <td>0.393323</td>\n",
              "      <td>0.105643</td>\n",
              "      <td>63662.432793</td>\n",
              "      <td>0.283194</td>\n",
              "      <td>0.267075</td>\n",
              "      <td>0.248581</td>\n",
              "      <td>0.321917</td>\n",
              "      <td>0.026029</td>\n",
              "      <td>0.296047</td>\n",
              "      <td>0.029733</td>\n",
              "      <td>1.799198</td>\n",
              "      <td>0.133796</td>\n",
              "      <td>0.065267</td>\n",
              "      <td>0.212901</td>\n",
              "      <td>0.114417</td>\n",
              "      <td>0.187097</td>\n",
              "      <td>0.094423</td>\n",
              "      <td>0.061463</td>\n",
              "      <td>0.096474</td>\n",
              "      <td>0.096474</td>\n",
              "      <td>0.096474</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>146849.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.010204</td>\n",
              "      <td>0.000169</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000169</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.029412</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.016129</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000071</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>156.0</td>\n",
              "      <td>0.021739</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000064</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.5</td>\n",
              "      <td>0.000033</td>\n",
              "      <td>7.5</td>\n",
              "      <td>0.035714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.012048</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000479</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.011561</td>\n",
              "      <td>0.004484</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-32.387097</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.005319</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-97.967742</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.345592</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>57.5</td>\n",
              "      <td>0.000025</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.005714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>257792.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.141839</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.447351</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3937.443396</td>\n",
              "      <td>0.185185</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.121295</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.150036</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.263948</td>\n",
              "      <td>0.0</td>\n",
              "      <td>348.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.426072</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.43154</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.00015</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17.49</td>\n",
              "      <td>0.011194</td>\n",
              "      <td>16.99</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.064186</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.317738</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.014197</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.404522</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4375</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.392857</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.885611</td>\n",
              "      <td>25.99</td>\n",
              "      <td>0.114161</td>\n",
              "      <td>45.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.145833</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.213908</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-30.685484</td>\n",
              "      <td>0.006775</td>\n",
              "      <td>1.778388</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13050.0</td>\n",
              "      <td>0.382451</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.143684</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>369065.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.288088</td>\n",
              "      <td>0.027147</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.724026</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16038.629883</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.146728</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.363636</td>\n",
              "      <td>0.31959</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.36043</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.693357</td>\n",
              "      <td>0.0</td>\n",
              "      <td>432.0</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.801437</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.800849</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0.000966</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>0.016787</td>\n",
              "      <td>22.99</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.148427</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.493811</td>\n",
              "      <td>0.611111</td>\n",
              "      <td>0.127708</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.60751</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.230769</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.66349</td>\n",
              "      <td>45.0</td>\n",
              "      <td>0.21641</td>\n",
              "      <td>45.0</td>\n",
              "      <td>0.204545</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.24498</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.494627</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-8.548387</td>\n",
              "      <td>0.017337</td>\n",
              "      <td>3.915596</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>24359.98</td>\n",
              "      <td>0.552171</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.389941</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00885</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>480225.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.037049</td>\n",
              "      <td>0.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.655653</td>\n",
              "      <td>0.111004</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>56498.893404</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.010422</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.3125</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.83048</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.047619</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.892901</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.62069</td>\n",
              "      <td>0.95514</td>\n",
              "      <td>0.0</td>\n",
              "      <td>552.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>0.004852</td>\n",
              "      <td>0.0</td>\n",
              "      <td>29.9925</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>29.9</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.253245</td>\n",
              "      <td>0.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>178309.925</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.692648</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.25647</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.894737</td>\n",
              "      <td>0.411765</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.974079</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.22127</td>\n",
              "      <td>45.0</td>\n",
              "      <td>0.449181</td>\n",
              "      <td>45.0</td>\n",
              "      <td>0.440945</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.454545</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.729929</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-3.604839</td>\n",
              "      <td>0.033714</td>\n",
              "      <td>6.555381</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>38570.32</td>\n",
              "      <td>0.825137</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.774194</td>\n",
              "      <td>0.728719</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.659676</td>\n",
              "      <td>0.033926</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>590827.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>89.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>9079305.32255</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2018915.26</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7513826.486667</td>\n",
              "      <td>1128.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>182.0</td>\n",
              "      <td>0.039407</td>\n",
              "      <td>3.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2411099.834467</td>\n",
              "      <td>182.0</td>\n",
              "      <td>7337638.1775</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.534014</td>\n",
              "      <td>0.915323</td>\n",
              "      <td>59.9</td>\n",
              "      <td>1.0</td>\n",
              "      <td>59.9</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.688776</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>113.193548</td>\n",
              "      <td>2.283844</td>\n",
              "      <td>13.748937</td>\n",
              "      <td>9.377551</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>786218.42</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>39.878468</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dtype</th>\n",
              "      <td>int64</td>\n",
              "      <td>int64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>object</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>object</td>\n",
              "      <td>float64</td>\n",
              "      <td>int64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>object</td>\n",
              "      <td>object</td>\n",
              "      <td>float64</td>\n",
              "      <td>object</td>\n",
              "      <td>object</td>\n",
              "      <td>float64</td>\n",
              "      <td>object</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>object</td>\n",
              "      <td>float64</td>\n",
              "      <td>int64</td>\n",
              "      <td>object</td>\n",
              "      <td>int64</td>\n",
              "      <td>float64</td>\n",
              "      <td>object</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>int64</td>\n",
              "      <td>float64</td>\n",
              "      <td>int64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>object</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>int64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>object</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>object</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "      <td>float64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>size</th>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "      <td>88798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>%missing</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.108111</td>\n",
              "      <td>0.566206</td>\n",
              "      <td>0.827338</td>\n",
              "      <td>0.566206</td>\n",
              "      <td>0.566206</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.782855</td>\n",
              "      <td>0.782855</td>\n",
              "      <td>0.108111</td>\n",
              "      <td>0.566206</td>\n",
              "      <td>0.689362</td>\n",
              "      <td>0.802957</td>\n",
              "      <td>0.566206</td>\n",
              "      <td>0.566206</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.689362</td>\n",
              "      <td>0.566206</td>\n",
              "      <td>0.806212</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.805851</td>\n",
              "      <td>0.83128</td>\n",
              "      <td>0.872722</td>\n",
              "      <td>0.108111</td>\n",
              "      <td>0.404806</td>\n",
              "      <td>0.836483</td>\n",
              "      <td>0.108111</td>\n",
              "      <td>0.806437</td>\n",
              "      <td>0.814185</td>\n",
              "      <td>0.805851</td>\n",
              "      <td>0.566206</td>\n",
              "      <td>0.566206</td>\n",
              "      <td>0.108111</td>\n",
              "      <td>0.836483</td>\n",
              "      <td>0.566206</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.836483</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.814185</td>\n",
              "      <td>0.828206</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.108111</td>\n",
              "      <td>0.108111</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.674531</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.674531</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.808543</td>\n",
              "      <td>0.679542</td>\n",
              "      <td>0.976261</td>\n",
              "      <td>0.99625</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.976261</td>\n",
              "      <td>0.996363</td>\n",
              "      <td>0.976261</td>\n",
              "      <td>0.679542</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.976261</td>\n",
              "      <td>0.994324</td>\n",
              "      <td>0.808543</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.440719</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.440719</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.281245</td>\n",
              "      <td>0.420584</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.281245</td>\n",
              "      <td>0.803543</td>\n",
              "      <td>0.995203</td>\n",
              "      <td>0.731784</td>\n",
              "      <td>0.561623</td>\n",
              "      <td>0.731784</td>\n",
              "      <td>0.420584</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.561623</td>\n",
              "      <td>0.806054</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.994324</td>\n",
              "      <td>0.991543</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.806054</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.976261</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.998052</td>\n",
              "      <td>0.145882</td>\n",
              "      <td>0.976261</td>\n",
              "      <td>0.145882</td>\n",
              "      <td>0.976261</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.731784</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.994324</td>\n",
              "      <td>0.737821</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.731784</td>\n",
              "      <td>0.737821</td>\n",
              "      <td>0.724633</td>\n",
              "      <td>0.731784</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.737821</td>\n",
              "      <td>0.737821</td>\n",
              "      <td>0.737821</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     ID CR_PROD_CNT_IL AMOUNT_RUB_CLO_PRC  \\\n",
              "count           88798.0        88798.0            79198.0   \n",
              "unique              NaN            NaN                NaN   \n",
              "top                 NaN            NaN                NaN   \n",
              "freq                NaN            NaN                NaN   \n",
              "mean      368993.799601       0.105948            0.04454   \n",
              "std       128247.549453       0.433893           0.110796   \n",
              "min            146849.0            0.0                0.0   \n",
              "25%            257792.0            0.0                0.0   \n",
              "50%            369065.5            0.0                0.0   \n",
              "75%            480225.0            0.0           0.037049   \n",
              "max            590827.0           11.0                1.0   \n",
              "dtype             int64          int64            float64   \n",
              "size              88798          88798              88798   \n",
              "%missing            0.0            0.0           0.108111   \n",
              "\n",
              "         PRC_ACCEPTS_A_EMAIL_LINK APP_REGISTR_RGN_CODE PRC_ACCEPTS_A_POS  \\\n",
              "count                     38520.0              15332.0           38520.0   \n",
              "unique                        NaN                  NaN               NaN   \n",
              "top                           NaN                  NaN               NaN   \n",
              "freq                          NaN                  NaN               NaN   \n",
              "mean                          0.0            51.234151               0.0   \n",
              "std                           0.0             21.54533               0.0   \n",
              "min                           0.0                  1.0               0.0   \n",
              "25%                           0.0                 34.0               0.0   \n",
              "50%                           0.0                 54.0               0.0   \n",
              "75%                           0.0                 72.0               0.0   \n",
              "max                           0.0                 89.0               0.0   \n",
              "dtype                     float64              float64           float64   \n",
              "size                        88798                88798             88798   \n",
              "%missing                 0.566206             0.827338          0.566206   \n",
              "\n",
              "         PRC_ACCEPTS_A_TK TURNOVER_DYNAMIC_IL_1M CNT_TRAN_AUT_TENDENCY1M  \\\n",
              "count             38520.0                88798.0                 19282.0   \n",
              "unique                NaN                    NaN                     NaN   \n",
              "top                   NaN                    NaN                     NaN   \n",
              "freq                  NaN                    NaN                     NaN   \n",
              "mean                  0.0               0.001395                0.417814   \n",
              "std                   0.0               0.030099                0.315638   \n",
              "min                   0.0                    0.0                0.010204   \n",
              "25%                   0.0                    0.0                0.166667   \n",
              "50%                   0.0                    0.0                     0.3   \n",
              "75%                   0.0                    0.0                0.571429   \n",
              "max                   0.0                    1.0                     1.0   \n",
              "dtype             float64                float64                 float64   \n",
              "size                88798                  88798                   88798   \n",
              "%missing         0.566206                    0.0                0.782855   \n",
              "\n",
              "         SUM_TRAN_AUT_TENDENCY1M AMOUNT_RUB_SUP_PRC PRC_ACCEPTS_A_AMOBILE  \\\n",
              "count                    19282.0            79198.0               38520.0   \n",
              "unique                       NaN                NaN                   NaN   \n",
              "top                          NaN                NaN                   NaN   \n",
              "freq                         NaN                NaN                   NaN   \n",
              "mean                    0.415055           0.085612                   0.0   \n",
              "std                     0.337138           0.142358                   0.0   \n",
              "min                     0.000169                0.0                   0.0   \n",
              "25%                     0.141839                0.0                   0.0   \n",
              "50%                     0.288088           0.027147                   0.0   \n",
              "75%                     0.655653           0.111004                   0.0   \n",
              "max                          1.0                1.0                   0.0   \n",
              "dtype                    float64            float64               float64   \n",
              "size                       88798              88798                 88798   \n",
              "%missing                0.782855           0.108111              0.566206   \n",
              "\n",
              "         SUM_TRAN_AUT_TENDENCY3M CLNT_TRUST_RELATION PRC_ACCEPTS_TK  \\\n",
              "count                    27584.0               17497        38520.0   \n",
              "unique                       NaN                  20            NaN   \n",
              "top                          NaN              FRIEND            NaN   \n",
              "freq                         NaN                6387            NaN   \n",
              "mean                    0.690076                 NaN            0.0   \n",
              "std                     0.300702                 NaN            0.0   \n",
              "min                     0.000169                 NaN            0.0   \n",
              "25%                     0.447351                 NaN            0.0   \n",
              "50%                     0.724026                 NaN            0.0   \n",
              "75%                          1.0                 NaN            0.0   \n",
              "max                          1.0                 NaN            0.0   \n",
              "dtype                    float64              object        float64   \n",
              "size                       88798               88798          88798   \n",
              "%missing                0.689362            0.802957       0.566206   \n",
              "\n",
              "         PRC_ACCEPTS_A_MTP REST_DYNAMIC_FDEP_1M CNT_TRAN_AUT_TENDENCY3M  \\\n",
              "count              38520.0              88798.0                 27584.0   \n",
              "unique                 NaN                  NaN                     NaN   \n",
              "top                    NaN                  NaN                     NaN   \n",
              "freq                   NaN                  NaN                     NaN   \n",
              "mean                   0.0             0.000751                0.692178   \n",
              "std                    0.0             0.014808                0.276734   \n",
              "min                    0.0                  0.0                0.029412   \n",
              "25%                    0.0                  0.0                     0.5   \n",
              "50%                    0.0                  0.0                0.666667   \n",
              "75%                    0.0                  0.0                     1.0   \n",
              "max                    0.0                  1.0                     1.0   \n",
              "dtype              float64              float64                 float64   \n",
              "size                 88798                88798                   88798   \n",
              "%missing          0.566206                  0.0                0.689362   \n",
              "\n",
              "         CNT_ACCEPTS_TK APP_MARITAL_STATUS REST_DYNAMIC_SAVE_3M  \\\n",
              "count           38520.0              17208              88798.0   \n",
              "unique              NaN                 12                  NaN   \n",
              "top                 NaN                  M                  NaN   \n",
              "freq                NaN               7783                  NaN   \n",
              "mean                0.0                NaN             0.064653   \n",
              "std                 0.0                NaN             0.205522   \n",
              "min                 0.0                NaN                  0.0   \n",
              "25%                 0.0                NaN                  0.0   \n",
              "50%                 0.0                NaN                  0.0   \n",
              "75%                 0.0                NaN                  0.0   \n",
              "max                 0.0                NaN                  1.0   \n",
              "dtype           float64             object              float64   \n",
              "size              88798              88798                88798   \n",
              "%missing       0.566206           0.806212                  0.0   \n",
              "\n",
              "         CR_PROD_CNT_VCU   REST_AVG_CUR CNT_TRAN_MED_TENDENCY1M  \\\n",
              "count            88798.0        88798.0                 17240.0   \n",
              "unique               NaN            NaN                     NaN   \n",
              "top                  NaN            NaN                     NaN   \n",
              "freq                 NaN            NaN                     NaN   \n",
              "mean            0.031577   66097.052316                0.450684   \n",
              "std             0.183729  173380.217827                0.327417   \n",
              "min                  0.0            0.0                0.016129   \n",
              "25%                  0.0    3937.443396                0.185185   \n",
              "50%                  0.0   16038.629883                0.333333   \n",
              "75%                  0.0   56498.893404                0.666667   \n",
              "max                  3.0  9079305.32255                     1.0   \n",
              "dtype              int64        float64                 float64   \n",
              "size               88798          88798                   88798   \n",
              "%missing             0.0            0.0                0.805851   \n",
              "\n",
              "         APP_KIND_OF_PROP_HABITATION CLNT_JOB_POSITION_TYPE  \\\n",
              "count                          14982                  11302   \n",
              "unique                             5                      4   \n",
              "top                               SO             SPECIALIST   \n",
              "freq                            7111                   6323   \n",
              "mean                             NaN                    NaN   \n",
              "std                              NaN                    NaN   \n",
              "min                              NaN                    NaN   \n",
              "25%                              NaN                    NaN   \n",
              "50%                              NaN                    NaN   \n",
              "75%                              NaN                    NaN   \n",
              "max                              NaN                    NaN   \n",
              "dtype                         object                 object   \n",
              "size                           88798                  88798   \n",
              "%missing                     0.83128               0.872722   \n",
              "\n",
              "         AMOUNT_RUB_NAS_PRC CLNT_JOB_POSITION APP_DRIVING_LICENSE  \\\n",
              "count               79198.0             52852               14520   \n",
              "unique                  NaN              9857                   2   \n",
              "top                     NaN          ДИРЕКТОР                   N   \n",
              "freq                    NaN              2896                9171   \n",
              "mean               0.023622               NaN                 NaN   \n",
              "std                0.089373               NaN                 NaN   \n",
              "min                     0.0               NaN                 NaN   \n",
              "25%                     0.0               NaN                 NaN   \n",
              "50%                     0.0               NaN                 NaN   \n",
              "75%                0.010422               NaN                 NaN   \n",
              "max                     1.0               NaN                 NaN   \n",
              "dtype               float64            object              object   \n",
              "size                  88798             88798               88798   \n",
              "%missing           0.108111          0.404806            0.836483   \n",
              "\n",
              "         TRANS_COUNT_SUP_PRC APP_EDUCATION CNT_TRAN_CLO_TENDENCY1M  \\\n",
              "count                79198.0         17188                 16500.0   \n",
              "unique                   NaN            17                     NaN   \n",
              "top                      NaN             H                     NaN   \n",
              "freq                     NaN         10658                     NaN   \n",
              "mean                0.191779           NaN                 0.47977   \n",
              "std                 0.196888           NaN                0.329938   \n",
              "min                      0.0           NaN                0.018519   \n",
              "25%                      0.0           NaN                     0.2   \n",
              "50%                 0.146728           NaN                0.363636   \n",
              "75%                   0.3125           NaN                    0.75   \n",
              "max                      1.0           NaN                     1.0   \n",
              "dtype                float64        object                 float64   \n",
              "size                   88798         88798                   88798   \n",
              "%missing            0.108111      0.806437                0.814185   \n",
              "\n",
              "         SUM_TRAN_MED_TENDENCY1M PRC_ACCEPTS_A_ATM PRC_ACCEPTS_MTP  \\\n",
              "count                    17240.0           38520.0         38520.0   \n",
              "unique                       NaN               NaN             NaN   \n",
              "top                          NaN               NaN             NaN   \n",
              "freq                         NaN               NaN             NaN   \n",
              "mean                    0.444461               0.0             0.0   \n",
              "std                     0.363818               0.0             0.0   \n",
              "min                          0.0               0.0             0.0   \n",
              "25%                     0.121295               0.0             0.0   \n",
              "50%                      0.31959               0.0             0.0   \n",
              "75%                      0.83048               0.0             0.0   \n",
              "max                          1.0               0.0             0.0   \n",
              "dtype                    float64           float64         float64   \n",
              "size                       88798             88798           88798   \n",
              "%missing                0.805851          0.566206        0.566206   \n",
              "\n",
              "         TRANS_COUNT_NAS_PRC APP_TRAVEL_PASS CNT_ACCEPTS_MTP CR_PROD_CNT_TOVR  \\\n",
              "count                79198.0           14520         38520.0          88798.0   \n",
              "unique                   NaN               2             NaN              NaN   \n",
              "top                      NaN               N             NaN              NaN   \n",
              "freq                     NaN           13354             NaN              NaN   \n",
              "mean                0.050823             NaN             0.0          0.30923   \n",
              "std                 0.117188             NaN             0.0         0.589099   \n",
              "min                      0.0             NaN             0.0              0.0   \n",
              "25%                      0.0             NaN             0.0              0.0   \n",
              "50%                      0.0             NaN             0.0              0.0   \n",
              "75%                 0.047619             NaN             0.0              1.0   \n",
              "max                      1.0             NaN             0.0             13.0   \n",
              "dtype                float64          object         float64            int64   \n",
              "size                   88798           88798           88798            88798   \n",
              "%missing            0.108111        0.836483        0.566206              0.0   \n",
              "\n",
              "           APP_CAR CR_PROD_CNT_PIL SUM_TRAN_CLO_TENDENCY1M APP_POSITION_TYPE  \\\n",
              "count        14520         88798.0                 16500.0             15255   \n",
              "unique           2             NaN                     NaN                 4   \n",
              "top              N             NaN                     NaN        SPECIALIST   \n",
              "freq          8302             NaN                     NaN              9172   \n",
              "mean           NaN        0.057051                0.472748               NaN   \n",
              "std            NaN        0.295065                 0.36107               NaN   \n",
              "min            NaN             0.0                0.000071               NaN   \n",
              "25%            NaN             0.0                0.150036               NaN   \n",
              "50%            NaN             0.0                 0.36043               NaN   \n",
              "75%            NaN             0.0                0.892901               NaN   \n",
              "max            NaN             6.0                     1.0               NaN   \n",
              "dtype       object           int64                 float64            object   \n",
              "size         88798           88798                   88798             88798   \n",
              "%missing  0.836483             0.0                0.814185          0.828206   \n",
              "\n",
              "           TURNOVER_CC TRANS_COUNT_ATM_PRC AMOUNT_RUB_ATM_PRC   TURNOVER_PAYM  \\\n",
              "count          88798.0             79198.0            79198.0         88798.0   \n",
              "unique             NaN                 NaN                NaN             NaN   \n",
              "top                NaN                 NaN                NaN             NaN   \n",
              "freq               NaN                 NaN                NaN             NaN   \n",
              "mean        492.377672            0.370995           0.597782    14390.484559   \n",
              "std       12593.535621            0.343645           0.364865    89822.408834   \n",
              "min                0.0                 0.0                0.0             0.0   \n",
              "25%                0.0            0.083333           0.263948             0.0   \n",
              "50%                0.0                0.25           0.693357             0.0   \n",
              "75%                0.0             0.62069            0.95514             0.0   \n",
              "max         2018915.26                 1.0                1.0  7513826.486667   \n",
              "dtype          float64             float64            float64         float64   \n",
              "size             88798               88798              88798           88798   \n",
              "%missing           0.0            0.108111           0.108111             0.0   \n",
              "\n",
              "                 AGE CNT_TRAN_MED_TENDENCY3M CR_PROD_CNT_CC  \\\n",
              "count        88798.0                 28901.0        88798.0   \n",
              "unique           NaN                     NaN            NaN   \n",
              "top              NaN                     NaN            NaN   \n",
              "freq             NaN                     NaN            NaN   \n",
              "mean      457.822338                0.703231       0.071274   \n",
              "std       136.326732                 0.28273       0.291898   \n",
              "min            156.0                0.021739            0.0   \n",
              "25%            348.0                     0.5            0.0   \n",
              "50%            432.0                0.714286            0.0   \n",
              "75%            552.0                     1.0            0.0   \n",
              "max           1128.0                     1.0            7.0   \n",
              "dtype          int64                 float64          int64   \n",
              "size           88798                   88798          88798   \n",
              "%missing         0.0                0.674531            0.0   \n",
              "\n",
              "         SUM_TRAN_MED_TENDENCY3M REST_DYNAMIC_FDEP_3M REST_DYNAMIC_IL_1M  \\\n",
              "count                    28901.0              88798.0            88798.0   \n",
              "unique                       NaN                  NaN                NaN   \n",
              "top                          NaN                  NaN                NaN   \n",
              "freq                         NaN                  NaN                NaN   \n",
              "mean                    0.697479             0.003828           0.001671   \n",
              "std                     0.323004             0.045202           0.024163   \n",
              "min                          0.0                  0.0                0.0   \n",
              "25%                     0.426072                  0.0                0.0   \n",
              "50%                     0.801437                  0.0                0.0   \n",
              "75%                          1.0                  0.0                0.0   \n",
              "max                          1.0                  1.0                1.0   \n",
              "dtype                    float64              float64            float64   \n",
              "size                       88798                88798              88798   \n",
              "%missing                0.674531                  0.0                0.0   \n",
              "\n",
              "         APP_EMP_TYPE SUM_TRAN_CLO_TENDENCY3M LDEAL_TENOR_MAX LDEAL_YQZ_CHRG  \\\n",
              "count           17001                 28456.0          2108.0          333.0   \n",
              "unique              4                     NaN             NaN            NaN   \n",
              "top           PRIVATE                     NaN             NaN            NaN   \n",
              "freq            14943                     NaN             NaN            NaN   \n",
              "mean              NaN                0.701108       22.257116       0.003808   \n",
              "std               NaN                0.320048       16.907249       0.006145   \n",
              "min               NaN                0.000064             0.0            0.0   \n",
              "25%               NaN                 0.43154            10.0        0.00015   \n",
              "50%               NaN                0.800849            19.0       0.000966   \n",
              "75%               NaN                     1.0            36.0       0.004852   \n",
              "max               NaN                     1.0           182.0       0.039407   \n",
              "dtype          object                 float64         float64        float64   \n",
              "size            88798                   88798           88798          88798   \n",
              "%missing     0.808543                0.679542        0.976261        0.99625   \n",
              "\n",
              "         CR_PROD_CNT_CCFP DEAL_YQZ_IR_MAX LDEAL_YQZ_COM DEAL_YQZ_IR_MIN  \\\n",
              "count             88798.0          2108.0         323.0          2108.0   \n",
              "unique                NaN             NaN           NaN             NaN   \n",
              "top                   NaN             NaN           NaN             NaN   \n",
              "freq                  NaN             NaN           NaN             NaN   \n",
              "mean             0.004876       26.421559      0.074842       25.755292   \n",
              "std              0.075703       14.924138      0.109409       14.550783   \n",
              "min                   0.0             7.5      0.000033             7.5   \n",
              "25%                   0.0           17.49      0.011194           16.99   \n",
              "50%                   0.0            23.0      0.016787           22.99   \n",
              "75%                   0.0         29.9925      0.166667            29.9   \n",
              "max                   3.0            94.0           1.0            94.0   \n",
              "dtype               int64         float64       float64         float64   \n",
              "size                88798           88798         88798           88798   \n",
              "%missing              0.0        0.976261      0.996363        0.976261   \n",
              "\n",
              "         CNT_TRAN_CLO_TENDENCY3M REST_DYNAMIC_CUR_1M   REST_AVG_PAYM  \\\n",
              "count                    28456.0             88798.0         88798.0   \n",
              "unique                       NaN                 NaN             NaN   \n",
              "top                          NaN                 NaN             NaN   \n",
              "freq                         NaN                 NaN             NaN   \n",
              "mean                    0.703422            0.213429     7145.520329   \n",
              "std                     0.288372            0.236967    45472.775178   \n",
              "min                     0.035714                 0.0             0.0   \n",
              "25%                          0.5            0.064186             0.0   \n",
              "50%                     0.714286            0.148427             0.0   \n",
              "75%                          1.0            0.253245             0.0   \n",
              "max                          1.0                 1.0  2411099.834467   \n",
              "dtype                    float64             float64         float64   \n",
              "size                       88798               88798           88798   \n",
              "%missing                0.679542                 0.0             0.0   \n",
              "\n",
              "         LDEAL_TENOR_MIN LDEAL_AMT_MONTH APP_COMP_TYPE  \\\n",
              "count             2108.0           504.0         17001   \n",
              "unique               NaN             NaN             4   \n",
              "top                  NaN             NaN       PRIVATE   \n",
              "freq                 NaN             NaN         14943   \n",
              "mean           15.765655   206577.214111           NaN   \n",
              "std            12.891542   589978.528038           NaN   \n",
              "min                  0.0             0.0           NaN   \n",
              "25%                  6.0             0.0           NaN   \n",
              "50%                 12.0             0.0           NaN   \n",
              "75%                 24.0      178309.925           NaN   \n",
              "max                182.0    7337638.1775           NaN   \n",
              "dtype            float64         float64        object   \n",
              "size               88798           88798         88798   \n",
              "%missing        0.976261        0.994324      0.808543   \n",
              "\n",
              "         LDEAL_GRACE_DAYS_PCT_MED REST_DYNAMIC_CUR_3M CNT_TRAN_SUP_TENDENCY3M  \\\n",
              "count                     88798.0             88798.0                 49663.0   \n",
              "unique                        NaN                 NaN                     NaN   \n",
              "top                           NaN                 NaN                     NaN   \n",
              "freq                          NaN                 NaN                     NaN   \n",
              "mean                       0.0017            0.499548                0.645491   \n",
              "std                      0.038435            0.299771                0.267797   \n",
              "min                           0.0                 0.0                0.012048   \n",
              "25%                           0.0            0.317738                    0.45   \n",
              "50%                           0.0            0.493811                0.611111   \n",
              "75%                           0.0            0.692648                     1.0   \n",
              "max                           1.0                 1.0                     1.0   \n",
              "dtype                     float64             float64                 float64   \n",
              "size                        88798               88798                   88798   \n",
              "%missing                      0.0                 0.0                0.440719   \n",
              "\n",
              "         TURNOVER_DYNAMIC_CUR_1M REST_DYNAMIC_PAYM_3M SUM_TRAN_SUP_TENDENCY3M  \\\n",
              "count                    88798.0              88798.0                 49663.0   \n",
              "unique                       NaN                  NaN                     NaN   \n",
              "top                          NaN                  NaN                     NaN   \n",
              "freq                         NaN                  NaN                     NaN   \n",
              "mean                    0.205116             0.077136                0.629988   \n",
              "std                      0.25607             0.205299                0.294779   \n",
              "min                          0.0                  0.0                0.000479   \n",
              "25%                     0.014197                  0.0                0.404522   \n",
              "50%                     0.127708                  0.0                 0.60751   \n",
              "75%                      0.25647                  0.0                     1.0   \n",
              "max                          1.0                  1.0                     1.0   \n",
              "dtype                    float64              float64                 float64   \n",
              "size                       88798                88798                   88798   \n",
              "%missing                     0.0                  0.0                0.440719   \n",
              "\n",
              "         REST_DYNAMIC_IL_3M CNT_TRAN_ATM_TENDENCY3M CNT_TRAN_ATM_TENDENCY1M  \\\n",
              "count               88798.0                 63824.0                 51451.0   \n",
              "unique                  NaN                     NaN                     NaN   \n",
              "top                     NaN                     NaN                     NaN   \n",
              "freq                    NaN                     NaN                     NaN   \n",
              "mean               0.005898                0.630854                0.328071   \n",
              "std                0.058358                0.263788                 0.27461   \n",
              "min                     0.0                0.011561                0.004484   \n",
              "25%                     0.0                  0.4375                0.142857   \n",
              "50%                     0.0                     0.6                0.230769   \n",
              "75%                     0.0                0.894737                0.411765   \n",
              "max                     1.0                     1.0                     1.0   \n",
              "dtype               float64                 float64                 float64   \n",
              "size                  88798                   88798                   88798   \n",
              "%missing                0.0                0.281245                0.420584   \n",
              "\n",
              "         TURNOVER_DYNAMIC_IL_3M SUM_TRAN_ATM_TENDENCY3M  \\\n",
              "count                   88798.0                 63824.0   \n",
              "unique                      NaN                     NaN   \n",
              "top                         NaN                     NaN   \n",
              "freq                        NaN                     NaN   \n",
              "mean                   0.004375                0.619648   \n",
              "std                    0.055848                0.303434   \n",
              "min                         0.0                0.000056   \n",
              "25%                         0.0                0.392857   \n",
              "50%                         0.0                     0.6   \n",
              "75%                         0.0                0.974079   \n",
              "max                         1.0                     1.0   \n",
              "dtype                   float64                 float64   \n",
              "size                      88798                   88798   \n",
              "%missing                    0.0                0.281245   \n",
              "\n",
              "         DEAL_GRACE_DAYS_ACC_S1X1 AVG_PCT_MONTH_TO_PCLOSE DEAL_YWZ_IR_MIN  \\\n",
              "count                     17445.0                   426.0         23817.0   \n",
              "unique                        NaN                     NaN             NaN   \n",
              "top                           NaN                     NaN             NaN   \n",
              "freq                          NaN                     NaN             NaN   \n",
              "mean                     0.027779               -2.313821       37.211825   \n",
              "std                      0.142541                3.888261       12.393002   \n",
              "min                           0.0              -32.387097             0.0   \n",
              "25%                           0.0               -2.885611           25.99   \n",
              "50%                           0.0                -0.66349            45.0   \n",
              "75%                           0.0                -0.22127            45.0   \n",
              "max                      4.534014                0.915323            59.9   \n",
              "dtype                     float64                 float64         float64   \n",
              "size                        88798                   88798           88798   \n",
              "%missing                 0.803543                0.995203        0.731784   \n",
              "\n",
              "         SUM_TRAN_SUP_TENDENCY1M DEAL_YWZ_IR_MAX SUM_TRAN_ATM_TENDENCY1M  \\\n",
              "count                    38927.0         23817.0                 51451.0   \n",
              "unique                       NaN             NaN                     NaN   \n",
              "top                          NaN             NaN                     NaN   \n",
              "freq                         NaN             NaN                     NaN   \n",
              "mean                    0.332284       39.488849                0.322246   \n",
              "std                     0.302832       10.463102                0.305741   \n",
              "min                     0.000014             0.0                0.000012   \n",
              "25%                     0.114161            45.0                     0.1   \n",
              "50%                      0.21641            45.0                0.204545   \n",
              "75%                     0.449181            45.0                0.440945   \n",
              "max                          1.0            59.9                     1.0   \n",
              "dtype                    float64         float64                 float64   \n",
              "size                       88798           88798                   88798   \n",
              "%missing                0.561623        0.731784                0.420584   \n",
              "\n",
              "         REST_DYNAMIC_PAYM_1M CNT_TRAN_SUP_TENDENCY1M DEAL_GRACE_DAYS_ACC_AVG  \\\n",
              "count                 88798.0                 38927.0                 17222.0   \n",
              "unique                    NaN                     NaN                     NaN   \n",
              "top                       NaN                     NaN                     NaN   \n",
              "freq                      NaN                     NaN                     NaN   \n",
              "mean                 0.027967                0.345375                0.024202   \n",
              "std                   0.09798                0.283195                0.138537   \n",
              "min                       0.0                0.005319                     0.0   \n",
              "25%                       0.0                0.145833                     0.0   \n",
              "50%                       0.0                 0.24498                     0.0   \n",
              "75%                       0.0                0.454545                     0.0   \n",
              "max                       1.0                     1.0                4.688776   \n",
              "dtype                 float64                 float64                 float64   \n",
              "size                    88798                   88798                   88798   \n",
              "%missing                  0.0                0.561623                0.806054   \n",
              "\n",
              "         TURNOVER_DYNAMIC_CUR_3M    PACK MAX_PCLOSE_DATE LDEAL_YQZ_PC  \\\n",
              "count                    88798.0   88798           504.0        751.0   \n",
              "unique                       NaN      11             NaN          NaN   \n",
              "top                          NaN     102             NaN          NaN   \n",
              "freq                         NaN   29190             NaN          NaN   \n",
              "mean                    0.484631     NaN      -19.882616     0.028175   \n",
              "std                     0.332864     NaN       28.154802     0.091066   \n",
              "min                          0.0     NaN      -97.967742          0.0   \n",
              "25%                     0.213908     NaN      -30.685484     0.006775   \n",
              "50%                     0.494627     NaN       -8.548387     0.017337   \n",
              "75%                     0.729929     NaN       -3.604839     0.033714   \n",
              "max                          1.0     NaN      113.193548     2.283844   \n",
              "dtype                    float64  object         float64      float64   \n",
              "size                       88798   88798           88798        88798   \n",
              "%missing                     0.0     0.0        0.994324     0.991543   \n",
              "\n",
              "         CLNT_SETUP_TENOR DEAL_GRACE_DAYS_ACC_MAX TURNOVER_DYNAMIC_PAYM_3M  \\\n",
              "count             88798.0                 17222.0                  88798.0   \n",
              "unique                NaN                     NaN                      NaN   \n",
              "top                   NaN                     NaN                      NaN   \n",
              "freq                  NaN                     NaN                      NaN   \n",
              "mean             4.372644                0.029376                 0.072363   \n",
              "std              2.930534                 0.16852                 0.207623   \n",
              "min              0.345592                     0.0                      0.0   \n",
              "25%              1.778388                     0.0                      0.0   \n",
              "50%              3.915596                     0.0                      0.0   \n",
              "75%              6.555381                     0.0                      0.0   \n",
              "max             13.748937                9.377551                      1.0   \n",
              "dtype             float64                 float64                  float64   \n",
              "size                88798                   88798                    88798   \n",
              "%missing              0.0                0.806054                      0.0   \n",
              "\n",
              "         LDEAL_DELINQ_PER_MAXYQZ TURNOVER_DYNAMIC_PAYM_1M CLNT_SALARY_VALUE  \\\n",
              "count                     2108.0                  88798.0             173.0   \n",
              "unique                       NaN                      NaN               NaN   \n",
              "top                          NaN                      NaN               NaN   \n",
              "freq                         NaN                      NaN               NaN   \n",
              "mean                    0.202324                 0.025442       35077.98526   \n",
              "std                     0.393323                 0.105643      63662.432793   \n",
              "min                          0.0                      0.0              57.5   \n",
              "25%                          0.0                      0.0           13050.0   \n",
              "50%                          0.0                      0.0          24359.98   \n",
              "75%                          0.0                      0.0          38570.32   \n",
              "max                          1.0                      1.0         786218.42   \n",
              "dtype                    float64                  float64           float64   \n",
              "size                       88798                    88798             88798   \n",
              "%missing                0.976261                      0.0          0.998052   \n",
              "\n",
              "         TRANS_AMOUNT_TENDENCY3M MED_DEBT_PRC_YQZ TRANS_CNT_TENDENCY3M  \\\n",
              "count                    75844.0           2108.0              75844.0   \n",
              "unique                       NaN              NaN                  NaN   \n",
              "top                          NaN              NaN                  NaN   \n",
              "freq                         NaN              NaN                  NaN   \n",
              "mean                    0.583426         0.919355             0.599044   \n",
              "std                     0.283194         0.267075             0.248581   \n",
              "min                     0.000025              0.0             0.005714   \n",
              "25%                     0.382451              1.0             0.428571   \n",
              "50%                     0.552171              1.0                 0.56   \n",
              "75%                     0.825137              1.0             0.774194   \n",
              "max                          1.0              1.0                  1.0   \n",
              "dtype                    float64          float64              float64   \n",
              "size                       88798            88798                88798   \n",
              "%missing                0.145882         0.976261             0.145882   \n",
              "\n",
              "         LDEAL_USED_AMT_AVG_YQZ REST_DYNAMIC_CC_1M LDEAL_USED_AMT_AVG_YWZ  \\\n",
              "count                    2108.0            88798.0                23817.0   \n",
              "unique                      NaN                NaN                    NaN   \n",
              "top                         NaN                NaN                    NaN   \n",
              "freq                        NaN                NaN                    NaN   \n",
              "mean                   0.442068           0.002143               0.900927   \n",
              "std                    0.321917           0.026029               0.296047   \n",
              "min                         0.0                0.0                    0.0   \n",
              "25%                    0.143684                0.0                    1.0   \n",
              "50%                    0.389941                0.0                    1.0   \n",
              "75%                    0.728719                0.0                    1.0   \n",
              "max                         1.0                1.0                    1.0   \n",
              "dtype                   float64            float64                float64   \n",
              "size                      88798              88798                  88798   \n",
              "%missing               0.976261                0.0               0.731784   \n",
              "\n",
              "         TURNOVER_DYNAMIC_CC_1M AVG_PCT_DEBT_TO_DEAL_AMT  \\\n",
              "count                   88798.0                    504.0   \n",
              "unique                      NaN                      NaN   \n",
              "top                         NaN                      NaN   \n",
              "freq                        NaN                      NaN   \n",
              "mean                   0.000996                 0.383045   \n",
              "std                    0.029733                 1.799198   \n",
              "min                         0.0                      0.0   \n",
              "25%                         0.0                      0.0   \n",
              "50%                         0.0                      0.0   \n",
              "75%                         0.0                 0.659676   \n",
              "max                         1.0                39.878468   \n",
              "dtype                   float64                  float64   \n",
              "size                      88798                    88798   \n",
              "%missing                    0.0                 0.994324   \n",
              "\n",
              "         LDEAL_ACT_DAYS_ACC_PCT_AVG REST_DYNAMIC_CC_3M MED_DEBT_PRC_YWZ  \\\n",
              "count                       23281.0            88798.0          23817.0   \n",
              "unique                          NaN                NaN              NaN   \n",
              "top                             NaN                NaN              NaN   \n",
              "freq                            NaN                NaN              NaN   \n",
              "mean                       0.051227           0.007159         0.053328   \n",
              "std                        0.133796           0.065267         0.212901   \n",
              "min                             0.0                0.0              0.0   \n",
              "25%                             0.0                0.0              0.0   \n",
              "50%                         0.00885                0.0              0.0   \n",
              "75%                        0.033926                0.0              0.0   \n",
              "max                             1.0                1.0              1.0   \n",
              "dtype                       float64            float64          float64   \n",
              "size                          88798              88798            88798   \n",
              "%missing                   0.737821                0.0         0.731784   \n",
              "\n",
              "         LDEAL_ACT_DAYS_PCT_TR3 LDEAL_ACT_DAYS_PCT_AAVG  \\\n",
              "count                   23281.0                 24452.0   \n",
              "unique                      NaN                     NaN   \n",
              "top                         NaN                     NaN   \n",
              "freq                        NaN                     NaN   \n",
              "mean                   0.025664                0.050934   \n",
              "std                    0.114417                0.187097   \n",
              "min                         0.0                     0.0   \n",
              "25%                         0.0                     0.0   \n",
              "50%                         0.0                     0.0   \n",
              "75%                         0.0                     0.0   \n",
              "max                         1.0                     1.0   \n",
              "dtype                   float64                 float64   \n",
              "size                      88798                   88798   \n",
              "%missing               0.737821                0.724633   \n",
              "\n",
              "         LDEAL_DELINQ_PER_MAXYWZ TURNOVER_DYNAMIC_CC_3M LDEAL_ACT_DAYS_PCT_TR  \\\n",
              "count                    23817.0                88798.0               23281.0   \n",
              "unique                       NaN                    NaN                   NaN   \n",
              "top                          NaN                    NaN                   NaN   \n",
              "freq                         NaN                    NaN                   NaN   \n",
              "mean                    0.009531               0.004479               0.01386   \n",
              "std                     0.094423               0.061463              0.096474   \n",
              "min                          0.0                    0.0                   0.0   \n",
              "25%                          0.0                    0.0                   0.0   \n",
              "50%                          0.0                    0.0                   0.0   \n",
              "75%                          0.0                    0.0                   0.0   \n",
              "max                          1.0                    1.0                   1.0   \n",
              "dtype                    float64                float64               float64   \n",
              "size                       88798                  88798                 88798   \n",
              "%missing                0.731784                    0.0              0.737821   \n",
              "\n",
              "         LDEAL_ACT_DAYS_PCT_TR4 LDEAL_ACT_DAYS_PCT_CURR   TARGET  \n",
              "count                   23281.0                 23281.0      0.0  \n",
              "unique                      NaN                     NaN      NaN  \n",
              "top                         NaN                     NaN      NaN  \n",
              "freq                        NaN                     NaN      NaN  \n",
              "mean                    0.01386                 0.01386      NaN  \n",
              "std                    0.096474                0.096474      NaN  \n",
              "min                         0.0                     0.0      NaN  \n",
              "25%                         0.0                     0.0      NaN  \n",
              "50%                         0.0                     0.0      NaN  \n",
              "75%                         0.0                     0.0      NaN  \n",
              "max                         1.0                     1.0      NaN  \n",
              "dtype                   float64                 float64  float64  \n",
              "size                      88798                   88798    88798  \n",
              "%missing               0.737821                0.737821      1.0  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#test statistic\n",
        "\n",
        "df_test = pd.read_csv('data/bank_data_test.csv')\n",
        "\n",
        "test_stat = df_test.describe(include = 'all')\n",
        "test_stat.loc['dtype'] = df_test.dtypes\n",
        "test_stat.loc['size'] = len(df_test)\n",
        "test_stat.loc['%missing'] = df_test.isnull().mean()\n",
        "test_stat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNy0sWjO4thV"
      },
      "source": [
        "# Data preprocessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-PhERRcyFe0"
      },
      "source": [
        "## df_train processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PGiYlXIG4xTl"
      },
      "outputs": [],
      "source": [
        "train_indices = df_train['ID']\n",
        "train_y = df_train['TARGET']\n",
        "train_all_X = df_train.drop(['ID','TARGET'], axis = 1) \n",
        "\n",
        "#split into numeric/chars\n",
        "train_stat = df_train.describe(include = 'all')\n",
        "uniq = train_stat.loc['unique']\n",
        "cat_columns = uniq[uniq.notnull()].index.tolist()\n",
        "num_columns = list(set(train_all_X.columns).difference(set(cat_columns)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8_wlIQhf61kr"
      },
      "outputs": [],
      "source": [
        "#split train-validation\n",
        "train_X, val_X, train_y, val_y = train_test_split(train_all_X, train_y, test_size = 0.2, random_state=21)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0-rTCC3i7Q2H"
      },
      "outputs": [],
      "source": [
        "#use only numeric\n",
        "\n",
        "train_X_num = train_X[num_columns]\n",
        "val_X_num = val_X[num_columns]\n",
        "\n",
        "#1 - fillnan = -1 for constant columns \n",
        "train_stat = train_X_num.describe()\n",
        "non_constant_columns_stat = ((train_stat.loc['max'] - train_stat.loc['min']) > 0)\n",
        "non_constant_columns = non_constant_columns_stat[non_constant_columns_stat == 1].index.tolist()\n",
        "\n",
        "constant_columns_stat = ((train_stat.loc['max'] - train_stat.loc['min']) <= 0)\n",
        "constant_column = constant_columns_stat[constant_columns_stat == 1].index.tolist()[0]\n",
        "\n",
        "train_new_column = train_X_num[constant_column].map(lambda x: 1, na_action='ignore').fillna(-1)\n",
        "val_new_column = val_X_num[constant_column].map(lambda x: 1, na_action='ignore').fillna(-1)\n",
        "\n",
        "train_X1 = pd.concat([train_X_num[non_constant_columns], train_new_column], axis = 1)\n",
        "val_X1 = pd.concat([val_X_num[non_constant_columns], val_new_column], axis = 1)\n",
        "\n",
        "#2 - scaler - StandardScaler\n",
        "\n",
        "sc = StandardScaler()\n",
        "sc.fit(train_X1)\n",
        "\n",
        "train_X2 = pd.DataFrame(sc.transform(train_X1), columns = sc.feature_names_in_)\n",
        "val_X2 = pd.DataFrame(sc.transform(val_X1), columns = sc.feature_names_in_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "l7iZlMf7KTUL"
      },
      "outputs": [],
      "source": [
        "#3 - median imputation\n",
        "\n",
        "train_X3 = train_X2.fillna(train_X2.median())\n",
        "val_X3 = val_X2.fillna(val_X2.median())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MEjvFcl7Qd0B"
      },
      "outputs": [],
      "source": [
        "#4 add categorical vars\n",
        "\n",
        "#4.1 apply upper case\n",
        "train_X_cat = train_X[cat_columns].apply(lambda x: x.str.upper())\n",
        "val_X_cat = val_X[cat_columns].apply(lambda x: x.str.upper())\n",
        "\n",
        "#4.2 join similar meaning  \n",
        "\n",
        "TRUST_RELATION = {\"Друг\": \"FRIEND\",\n",
        "                   \"Мать\": \"MOTHER\",\n",
        "                   \"мать\": \"MOTHER\",\n",
        "                   \"Отец\": \"FATHER\",\n",
        "                   \"Дочь\": \"DAUGHTER\",\n",
        "                   'Брат': 'BROTHER',\n",
        "                   'Сестра': 'SISTER',\n",
        "                   'Сын' : 'SON'}\n",
        "\n",
        "train_X_cat['CLNT_TRUST_RELATION'] = train_X_cat['CLNT_TRUST_RELATION'].replace(TRUST_RELATION)\n",
        "val_X_cat['CLNT_TRUST_RELATION'] = val_X_cat['CLNT_TRUST_RELATION'].replace(TRUST_RELATION)\n",
        "\n",
        "#4.3 remove values with freq < 1000 in train\n",
        "for col in cat_columns:\n",
        "    vc = train_X_cat[col].value_counts()\n",
        "    remove_list = vc[vc < 1000].index.tolist()\n",
        "    keep_list = list(set(vc.index) - set(remove_list))\n",
        "    train_X_cat[col] = train_X_cat[col].replace(remove_list, 'remove')\n",
        "    remove_list_val = list(set(val_X_cat[col].value_counts().index) - set(keep_list))\n",
        "    val_X_cat[col] = val_X_cat[col].replace(remove_list_val, 'remove')\n",
        "    train_X_cat[col] = train_X_cat[col].fillna('remove')\n",
        "    val_X_cat[col] = val_X_cat[col].fillna('remove')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bm0LgqyWRm7",
        "outputId": "efaa6744-eb56-4e5b-ab21-e35c12546476"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New colomns: \n",
            " ['BROTHER', 'DAUGHTER', 'FATHER', 'FRIEND', 'MOTHER', 'OTHER', 'RELATIVE', 'SISTER', 'SON', 'remove', 'D', 'M', 'T', 'V', 'remove', 'JO', 'NPRIVAT', 'OTHER', 'RENT', 'SO', 'remove', 'MANAGER', 'SELF_EMPL', 'SPECIALIST', 'TOP_MANAGER', 'remove', 'remove', 'АДМИНИСТРАТОР', 'БУХГАЛТЕР', 'ВОДИТЕЛЬ', 'ГЕН. ДИРЕКТОР', 'ГЕН.ДИРЕКТОР', 'ГЕНЕРАЛЬНЫЙ ДИРЕКТОР', 'ГЛАВНЫЙ БУХГАЛТЕР', 'ДИЗАЙНЕР', 'ДИРЕКТОР', 'ЗАМ. ДИРЕКТОРА', 'ЗАМ.ДИРЕКТОРА', 'ЗАМЕСТИТЕЛЬ ДИРЕКТОРА', 'ИНДИВИДУАЛЬНЫЙ ПРЕДПРИНИМАТЕЛЬ', 'ИНЖЕНЕР', 'КОММЕРЧЕСКИЙ ДИРЕКТОР', 'МЕНЕДЖЕР', 'МЕНЕДЖЕР ПО ПРОДАЖАМ', 'НАЧАЛЬНИК ОТДЕЛА', 'ПРЕДПРИНИМАТЕЛЬ', 'ПРОГРАММИСТ', 'ПРОДАВЕЦ', 'РУКОВОДИТЕЛЬ', 'СПЕЦИАЛИСТ', 'СТУДЕНТ', 'N', 'Y', 'remove', 'H', 'HH', 'S', 'SS', 'UH', 'remove', 'N', 'Y', 'remove', 'N', 'Y', 'remove', 'MANAGER', 'SELF_EMPL', 'SPECIALIST', 'TOP_MANAGER', 'remove', 'INTER', 'PRIVATE', 'STATE', 'remove', 'INTER', 'PRIVATE', 'STATE', 'remove', '101', '102', '103', '104', '105', '107', '301', 'K01', 'O01', 'remove']\n"
          ]
        }
      ],
      "source": [
        "#4.5 - OneHotEncoder\n",
        "\n",
        "oe_style = OneHotEncoder()\n",
        "oe_results = oe_style.fit(train_X_cat)\n",
        "train_oe_results = oe_style.transform(train_X_cat)\n",
        "val_oe_results = oe_style.transform(val_X_cat)\n",
        "\n",
        "codded_names = list()\n",
        "for cat in oe_style.categories_:\n",
        "    codded_names += cat.tolist()\n",
        "print(\"New colomns: \\n\", codded_names)\n",
        "train_X_ohe = pd.DataFrame(train_oe_results.toarray(), columns=codded_names).drop(['remove'], axis = 1)\n",
        "val_X_ohe = pd.DataFrame(val_oe_results.toarray(), columns=codded_names).drop(['remove'], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Lcb80h8fXC5j"
      },
      "outputs": [],
      "source": [
        "#4.6 - add categorial feachers\n",
        "train_X4 = pd.concat([train_X3, train_X_ohe], axis = 1)\n",
        "val_X4 = pd.concat([val_X3, val_X_ohe], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hCZ69rYYUXVt"
      },
      "outputs": [],
      "source": [
        "#5 - select best feachers\n",
        "\n",
        "#k = 85 tested on log reg with binar search\n",
        "np.random.seed(21)\n",
        "sb = SelectKBest(f_classif, k=85)\n",
        "sb.fit(train_X4, train_y)\n",
        "train_X5 = sb.transform(train_X4)\n",
        "val_X5 = sb.transform(val_X4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDIxEGvtyTFp"
      },
      "source": [
        "## test_df best processing apply "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gFvZkVauyjIY"
      },
      "outputs": [],
      "source": [
        "test_indices = df_test['ID']\n",
        "test_all_X = df_test.drop(['ID'], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "akG4ke8nzmDC"
      },
      "outputs": [],
      "source": [
        "#use only numeric\n",
        "\n",
        "test_X_num = test_all_X[num_columns]\n",
        "\n",
        "#1 - fillnan = -1 for constant columns \n",
        "\n",
        "test_new_column = test_X_num[constant_column].map(lambda x: 1, na_action='ignore').fillna(-1)\n",
        "test_X1 = pd.concat([test_X_num[non_constant_columns], test_new_column], axis = 1)\n",
        "\n",
        "#2 - scaler - StandardScaler\n",
        "\n",
        "sc = StandardScaler()\n",
        "sc.fit(train_X1)\n",
        "\n",
        "test_X2 = pd.DataFrame(sc.transform(test_X1), columns = sc.feature_names_in_)\n",
        "\n",
        "#3 - median imputation\n",
        "\n",
        "test_X3 = test_X2.fillna(test_X2.median())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "IfzPZsZ3zmJm"
      },
      "outputs": [],
      "source": [
        "#4 add categorical vars\n",
        "\n",
        "#4.1 apply upper case\n",
        "test_X_cat = test_all_X[cat_columns].apply(lambda x: x.str.upper())\n",
        "\n",
        "#4.2 join similar meaning  \n",
        "\n",
        "TRUST_RELATION = {\"Друг\": \"FRIEND\",\n",
        "                   \"Мать\": \"MOTHER\",\n",
        "                   \"мать\": \"MOTHER\",\n",
        "                   \"Отец\": \"FATHER\",\n",
        "                   \"Дочь\": \"DAUGHTER\",\n",
        "                   'Брат': 'BROTHER',\n",
        "                   'Сестра': 'SISTER',\n",
        "                   'Сын' : 'SON'}\n",
        "\n",
        "test_X_cat['CLNT_TRUST_RELATION'] = test_X_cat['CLNT_TRUST_RELATION'].replace(TRUST_RELATION)\n",
        "\n",
        "#4.2 remove values with freq < 1000 in train\n",
        "for col in cat_columns:\n",
        "    vc = train_X_cat[col].value_counts()\n",
        "    remove_list = vc[vc < 1000].index.tolist()\n",
        "    keep_list = list(set(vc.index) - set(remove_list))\n",
        "    train_X_cat[col] = train_X_cat[col].replace(remove_list, 'remove')\n",
        "    remove_list_test = list(set(test_X_cat[col].value_counts().index) - set(keep_list))\n",
        "    test_X_cat[col] = test_X_cat[col].replace(remove_list_test, 'remove')\n",
        "    train_X_cat[col] = train_X_cat[col].fillna('remove')\n",
        "    test_X_cat[col] = test_X_cat[col].fillna('remove')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "LCn2Z6Yp3qzD"
      },
      "outputs": [],
      "source": [
        "#4.5 - OneHotEncoder\n",
        "\n",
        "oe_style = OneHotEncoder()\n",
        "oe_results = oe_style.fit(train_X_cat)\n",
        "test_oe_results = oe_style.transform(test_X_cat)\n",
        "\n",
        "codded_names = list()\n",
        "for cat in oe_style.categories_:\n",
        "    codded_names += cat.tolist()\n",
        "test_X_ohe = pd.DataFrame(test_oe_results.toarray(), columns=codded_names).drop(['remove'], axis = 1)\n",
        "\n",
        "#4.6 - add categorial feachers\n",
        "test_X4 = pd.concat([test_X3, test_X_ohe], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "h31yNdFb4TeR"
      },
      "outputs": [],
      "source": [
        "#5 - select best feachers\n",
        "\n",
        "#k = 85 tested on log reg with binar search\n",
        "sb = SelectKBest(f_classif, k=85)\n",
        "sb.fit(train_X4, train_y)\n",
        "test_X5 = sb.transform(test_X4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGj2Ej0-9GbB"
      },
      "source": [
        "# Define global table for metrics saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "bs-t_URc9XCB"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>hyperparams</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>roc_auc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [model, hyperparams, accuracy, roc_auc]\n",
              "Index: []"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_metrics = pd.DataFrame(columns=['model', 'hyperparams', 'accuracy', 'roc_auc'])\n",
        "df_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBHasCcwJyI3"
      },
      "source": [
        "# Naive classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "8P0HGTDCJ4ub"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "class ft_naive_classifier():\n",
        "    def __init__(self):\n",
        "        self.major_label = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        y_np = np.array(y)\n",
        "        counter = Counter(y_np)\n",
        "        self.major_label = counter.most_common(1)[0][0]\n",
        "\n",
        "    def predict(self, X):\n",
        "        len = X.shape[0]\n",
        "        return np.ones([len]) * self.major_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBY6geA_PvH3",
        "outputId": "a1aebe70-ffba-4b39-cd43-59f92decac43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9196064078380585\n",
            "ROC AUC: 0.5\n"
          ]
        }
      ],
      "source": [
        "nc = ft_naive_classifier()\n",
        "nc.fit(train_X5, train_y)\n",
        "predicted = nc.predict(val_X5)\n",
        "\n",
        "ac =  accuracy_score(np.array(val_y), predicted)\n",
        "rocauc = roc_auc_score(np.array(val_y), predicted)\n",
        "print(\"Accuracy:\", ac)\n",
        "print(\"ROC AUC:\", rocauc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "h3t_Rs8f_gO8",
        "outputId": "4e38f1f0-cdbf-4506-ff2c-fed6dca787f1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/9j/mrbb80v92lxdv_3kldctnqq00000gn/T/ipykernel_19384/4283971078.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df_metrics = df_metrics.append(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>hyperparams</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>roc_auc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Naive classifier</td>\n",
              "      <td>baseline</td>\n",
              "      <td>0.919606</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              model hyperparams  accuracy roc_auc\n",
              "0  Naive classifier    baseline  0.919606     0.5"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_metrics = df_metrics.append(\n",
        "    pd.DataFrame({'model' : [\"Naive classifier\"],\n",
        "                                \"hyperparams\" : [\"baseline\"],\n",
        "                                'accuracy' : \"0.919606\", \n",
        "                                'roc_auc' : \"0.5\"})\n",
        ")\n",
        "\n",
        "df_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmETunTHYnQF"
      },
      "source": [
        "# Random forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "kvjst7Ts2ypm"
      },
      "outputs": [],
      "source": [
        "#decrease train counts to run gread search\n",
        "\n",
        "train_X5_rf, _, train_y_rf, _ = train_test_split(train_X5, train_y, test_size = 0.8, random_state=21)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ZqSFXcCrYq4q"
      },
      "outputs": [],
      "source": [
        "# Number of trees in random forest\n",
        "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "# The function to measure the quality of a split. \n",
        "criterion = ['gini', 'entropy']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(10, 300, num = 11)]\n",
        "max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]\n",
        "# Create the random grid\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'criterion': criterion,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvoDcuDrCOX7",
        "outputId": "814b225c-1b08-4a17-8f3b-2b6f3c26b62e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 2 folds for each of 30 candidates, totalling 60 fits\n",
            "[CV] END bootstrap=True, criterion=gini, max_depth=271, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  12.5s\n",
            "[CV] END bootstrap=True, criterion=gini, max_depth=271, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  13.1s\n",
            "[CV] END bootstrap=False, criterion=entropy, max_depth=155, min_samples_leaf=4, min_samples_split=10, n_estimators=600; total time=  56.1s\n",
            "[CV] END bootstrap=False, criterion=entropy, max_depth=155, min_samples_leaf=4, min_samples_split=10, n_estimators=600; total time=  57.5s\n",
            "[CV] END bootstrap=False, criterion=gini, max_depth=300, min_samples_leaf=2, min_samples_split=2, n_estimators=600; total time=  58.6s\n",
            "[CV] END bootstrap=False, criterion=gini, max_depth=300, min_samples_leaf=2, min_samples_split=2, n_estimators=600; total time= 1.0min\n",
            "[CV] END bootstrap=True, criterion=gini, max_depth=155, min_samples_leaf=1, min_samples_split=5, n_estimators=1400; total time= 1.5min\n",
            "[CV] END bootstrap=False, criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=400; total time=  40.9s\n",
            "[CV] END bootstrap=True, criterion=gini, max_depth=155, min_samples_leaf=1, min_samples_split=5, n_estimators=1400; total time= 1.6min\n",
            "[CV] END bootstrap=False, criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=400; total time=  43.7s\n",
            "[CV] END bootstrap=True, criterion=gini, max_depth=39, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  13.0s\n",
            "[CV] END bootstrap=True, criterion=gini, max_depth=39, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  13.5s\n",
            "[CV] END bootstrap=False, criterion=entropy, max_depth=68, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time= 1.6min\n",
            "[CV] END bootstrap=False, criterion=entropy, max_depth=68, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time= 1.7min\n",
            "[CV] END bootstrap=False, criterion=gini, max_depth=68, min_samples_leaf=2, min_samples_split=10, n_estimators=400; total time=  38.5s\n",
            "[CV] END bootstrap=False, criterion=gini, max_depth=68, min_samples_leaf=2, min_samples_split=10, n_estimators=400; total time=  40.4s\n",
            "[CV] END bootstrap=True, criterion=entropy, max_depth=184, min_samples_leaf=4, min_samples_split=2, n_estimators=1200; total time= 1.2min\n",
            "[CV] END bootstrap=True, criterion=entropy, max_depth=184, min_samples_leaf=4, min_samples_split=2, n_estimators=1200; total time= 1.3min\n",
            "[CV] END bootstrap=True, criterion=gini, max_depth=242, min_samples_leaf=2, min_samples_split=5, n_estimators=2000; total time= 2.1min\n",
            "[CV] END bootstrap=False, criterion=gini, max_depth=97, min_samples_leaf=1, min_samples_split=2, n_estimators=800; total time= 1.4min\n",
            "[CV] END bootstrap=True, criterion=gini, max_depth=242, min_samples_leaf=2, min_samples_split=5, n_estimators=2000; total time= 2.1min\n",
            "[CV] END bootstrap=True, criterion=entropy, max_depth=39, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time= 1.2min\n",
            "[CV] END bootstrap=False, criterion=gini, max_depth=97, min_samples_leaf=1, min_samples_split=2, n_estimators=800; total time= 1.5min\n",
            "[CV] END bootstrap=True, criterion=entropy, max_depth=39, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time= 1.2min\n",
            "[CV] END bootstrap=False, criterion=gini, max_depth=242, min_samples_leaf=2, min_samples_split=2, n_estimators=2000; total time= 3.3min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/olgaborisova/Library/Python/3.10/lib/python/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END bootstrap=False, criterion=gini, max_depth=242, min_samples_leaf=2, min_samples_split=2, n_estimators=2000; total time= 3.4min\n",
            "[CV] END bootstrap=True, criterion=gini, max_depth=155, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time= 1.1min\n",
            "[CV] END bootstrap=False, criterion=entropy, max_depth=39, min_samples_leaf=2, min_samples_split=10, n_estimators=1200; total time= 1.7min\n",
            "[CV] END bootstrap=True, criterion=gini, max_depth=155, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time= 1.2min\n",
            "[CV] END bootstrap=False, criterion=gini, max_depth=242, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time= 1.8min\n",
            "[CV] END bootstrap=True, criterion=entropy, max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=600; total time=  28.8s\n",
            "[CV] END bootstrap=True, criterion=entropy, max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=600; total time=  23.6s\n",
            "[CV] END bootstrap=False, criterion=entropy, max_depth=39, min_samples_leaf=2, min_samples_split=10, n_estimators=1200; total time= 2.1min\n",
            "[CV] END bootstrap=False, criterion=gini, max_depth=242, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time= 1.9min\n",
            "[CV] END bootstrap=False, criterion=entropy, max_depth=271, min_samples_leaf=4, min_samples_split=2, n_estimators=1600; total time= 2.2min\n",
            "[CV] END bootstrap=False, criterion=entropy, max_depth=271, min_samples_leaf=4, min_samples_split=2, n_estimators=1600; total time= 2.7min\n",
            "[CV] END bootstrap=True, criterion=gini, max_depth=300, min_samples_leaf=4, min_samples_split=5, n_estimators=1200; total time= 1.1min\n",
            "[CV] END bootstrap=True, criterion=entropy, max_depth=300, min_samples_leaf=4, min_samples_split=2, n_estimators=1600; total time= 1.5min\n",
            "[CV] END bootstrap=True, criterion=entropy, max_depth=300, min_samples_leaf=4, min_samples_split=2, n_estimators=1600; total time= 1.7min\n",
            "[CV] END bootstrap=True, criterion=gini, max_depth=300, min_samples_leaf=4, min_samples_split=5, n_estimators=1200; total time= 1.3min\n",
            "[CV] END bootstrap=True, criterion=entropy, max_depth=242, min_samples_leaf=1, min_samples_split=2, n_estimators=1400; total time= 1.3min\n",
            "[CV] END bootstrap=False, criterion=gini, max_depth=126, min_samples_leaf=2, min_samples_split=2, n_estimators=1800; total time= 3.1min\n",
            "[CV] END bootstrap=False, criterion=gini, max_depth=184, min_samples_leaf=2, min_samples_split=5, n_estimators=1600; total time= 2.8min\n",
            "[CV] END bootstrap=True, criterion=entropy, max_depth=242, min_samples_leaf=1, min_samples_split=2, n_estimators=1400; total time= 1.4min\n",
            "[CV] END bootstrap=False, criterion=gini, max_depth=184, min_samples_leaf=2, min_samples_split=5, n_estimators=1600; total time= 2.9min\n",
            "[CV] END bootstrap=False, criterion=gini, max_depth=126, min_samples_leaf=2, min_samples_split=2, n_estimators=1800; total time= 3.2min\n",
            "[CV] END bootstrap=False, criterion=gini, max_depth=184, min_samples_leaf=2, min_samples_split=2, n_estimators=1000; total time= 1.4min\n",
            "[CV] END bootstrap=True, criterion=gini, max_depth=68, min_samples_leaf=1, min_samples_split=2, n_estimators=2000; total time= 2.3min\n",
            "[CV] END bootstrap=True, criterion=gini, max_depth=68, min_samples_leaf=1, min_samples_split=2, n_estimators=2000; total time= 2.4min\n",
            "[CV] END bootstrap=False, criterion=gini, max_depth=184, min_samples_leaf=2, min_samples_split=2, n_estimators=1000; total time= 1.7min\n",
            "[CV] END bootstrap=False, criterion=gini, max_depth=271, min_samples_leaf=2, min_samples_split=2, n_estimators=1200; total time= 2.0min\n",
            "[CV] END bootstrap=False, criterion=gini, max_depth=271, min_samples_leaf=2, min_samples_split=2, n_estimators=1200; total time= 2.1min\n",
            "[CV] END bootstrap=True, criterion=entropy, max_depth=271, min_samples_leaf=1, min_samples_split=2, n_estimators=2000; total time= 2.3min\n",
            "[CV] END bootstrap=True, criterion=entropy, max_depth=68, min_samples_leaf=1, min_samples_split=2, n_estimators=1600; total time= 1.5min\n",
            "[CV] END bootstrap=True, criterion=entropy, max_depth=271, min_samples_leaf=1, min_samples_split=2, n_estimators=2000; total time= 2.3min\n",
            "[CV] END bootstrap=False, criterion=gini, max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=600; total time=  54.3s\n",
            "[CV] END bootstrap=True, criterion=entropy, max_depth=68, min_samples_leaf=1, min_samples_split=2, n_estimators=1600; total time= 1.8min\n",
            "[CV] END bootstrap=False, criterion=gini, max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=600; total time=  55.1s\n",
            "[CV] END bootstrap=True, criterion=entropy, max_depth=126, min_samples_leaf=4, min_samples_split=10, n_estimators=1800; total time= 1.7min\n",
            "[CV] END bootstrap=True, criterion=entropy, max_depth=126, min_samples_leaf=4, min_samples_split=10, n_estimators=1800; total time= 1.7min\n",
            "0.8216421062809166\n",
            "{'n_estimators': 1600, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_depth': 300, 'criterion': 'entropy', 'bootstrap': True}\n",
            "Running time:  15.465190811951954\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import time\n",
        "\n",
        "rf = RandomForestClassifier()\n",
        "# Random search of parameters, using 3 fold cross validation, \n",
        "# search across 100 different combinations, and use all available cores\n",
        "start_time = time.time()\n",
        "rf_random = RandomizedSearchCV(estimator = rf, scoring = 'roc_auc', param_distributions = random_grid, n_iter = 30, cv = 2, verbose=2, random_state=42, n_jobs = -1)\n",
        "# # Fit the random search model\n",
        "rf_random.fit(train_X5_rf, train_y_rf)\n",
        "\n",
        "print(rf_random.best_score_)\n",
        "print(rf_random.best_params_)\n",
        "print(\"Running time: \", (time.time() - start_time) / 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4wsjNZ3D9Dz",
        "outputId": "e7d83ec1-566f-41ec-8c40-de226ecf7c81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val acc:  0.9202117176722318\n",
            "Val roc_auc:  0.5072801123661398\n"
          ]
        }
      ],
      "source": [
        "rand_for_class = RandomForestClassifier(**rf_random.best_params_)\n",
        "rand_for_class.fit(train_X5, train_y)\n",
        "val_pred = rand_for_class.predict(val_X5)\n",
        "acc = accuracy_score(val_y, val_pred)\n",
        "print(\"Val acc: \", acc)\n",
        "roc_auc = roc_auc_score(val_y, val_pred)\n",
        "print(\"Val roc_auc: \", roc_auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZ8m1E7zipIJ"
      },
      "source": [
        "0.8273161597526945\n",
        "{'n_estimators': 1200, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_depth': 300, 'criterion': 'gini', 'bootstrap': True}\n",
        "Running time:  167.47443417310714"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "ml47Yl6QtBKZ",
        "outputId": "acf42c4d-5ca6-4e98-fac6-b0c5713fd1d9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/9j/mrbb80v92lxdv_3kldctnqq00000gn/T/ipykernel_19384/184664107.py:2: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df_metrics = df_metrics.append(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>hyperparams</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>roc_auc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Naive classifier</td>\n",
              "      <td>baseline</td>\n",
              "      <td>0.919606</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Rand_forest</td>\n",
              "      <td>{'n_est': 1600, 'min_samp_splt': 2, 'min_samp_...</td>\n",
              "      <td>0.920211</td>\n",
              "      <td>0.507280</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              model                                        hyperparams  \\\n",
              "0  Naive classifier                                           baseline   \n",
              "0       Rand_forest  {'n_est': 1600, 'min_samp_splt': 2, 'min_samp_...   \n",
              "\n",
              "   accuracy   roc_auc  \n",
              "0  0.919606       0.5  \n",
              "0  0.920211  0.507280  "
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#hyperparams = rf_random.best_params_.items()\n",
        "df_metrics = df_metrics.append(\n",
        "    pd.DataFrame({'model' : [\"Rand_forest\"],\n",
        "                                \"hyperparams\" : \"{'n_est': 1600, 'min_samp_splt': 2, 'min_samp_leaf': 4, 'max_depth': 300, 'criterion': 'entropy', 'bootstrap': True}\",\n",
        "                                'accuracy' : \"0.920211\", \n",
        "                                'roc_auc' : \"0.507280\"})\n",
        ")\n",
        "df_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Bf1bZNEg5Jq"
      },
      "source": [
        "# Scikit-learn MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwjZh-_RBKWG"
      },
      "source": [
        "## tuning layear_sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "56QyRWUsBkwf",
        "outputId": "90155807-8101-413e-8ec0-7ebdd68bfd5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10\n",
            "acc train =  0.9187406740054619 acc val =  0.9185224809257018\n",
            "roc_auc =  0.8171258111848652\n",
            "30\n",
            "acc train =  0.9199653706466961 acc val =  0.919465638109181\n",
            "roc_auc =  0.820335967141337\n",
            "50\n",
            "acc train =  0.9205882766969791 acc val =  0.917382246121794\n",
            "roc_auc =  0.8174322769154191\n",
            "70\n",
            "acc train =  0.9214223373405783 acc val =  0.9190010980038853\n",
            "roc_auc =  0.8082751815612513\n",
            "90\n",
            "acc train =  0.9230763816548889 acc val =  0.9139896956558462\n",
            "roc_auc =  0.8018605332750306\n"
          ]
        }
      ],
      "source": [
        "#for one hidden layer\n",
        "for i in range(10, 100, 20):\n",
        "\n",
        "    arr = [i]\n",
        "    clf = MLPClassifier(random_state=1, hidden_layer_sizes = arr)\n",
        "    clf.fit(train_X5, train_y)\n",
        "    proba_y = clf.predict_proba(val_X5)[:, 1]\n",
        "    predict_y = clf.predict(val_X5)\n",
        "    print(i)\n",
        "    print(\"acc train = \", clf.score(train_X5, train_y), \"acc val = \", accuracy_score(val_y, predict_y))\n",
        "    print(\"roc_auc = \", roc_auc_score(val_y, proba_y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rPCGsjRt_MMp",
        "outputId": "6cdcd931-7b3e-4ca0-d9ef-6cc342cce511"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15\n",
            "acc train =  0.9189694248148879 acc val =  0.9190010980038853\n",
            "roc_auc =  0.8240707657748952\n",
            "18\n",
            "acc train =  0.9187441932486838 acc val =  0.9180157099017427\n",
            "roc_auc =  0.8250556980699097\n",
            "20\n",
            "acc train =  0.9193882147582983 acc val =  0.9183254033052732\n",
            "roc_auc =  0.8273038482960513\n",
            "22\n",
            "acc train =  0.9191770601649821 acc val =  0.9162279343449985\n",
            "roc_auc =  0.8198764548313828\n",
            "24\n",
            "acc train =  0.9192087333539796 acc val =  0.91828317238661\n",
            "roc_auc =  0.8222536904485229\n",
            "25\n",
            "acc train =  0.9197331005940482 acc val =  0.9178749401728652\n",
            "roc_auc =  0.8257964725694436\n"
          ]
        }
      ],
      "source": [
        "#for one hidden layer\n",
        "for i in [15, 18, 20, 22, 24, 25]:\n",
        "\n",
        "    arr = [i]\n",
        "    clf = MLPClassifier(random_state=1, hidden_layer_sizes = arr)\n",
        "    clf.fit(train_X5, train_y)\n",
        "    proba_y = clf.predict_proba(val_X5)[:, 1]\n",
        "    predict_y = clf.predict(val_X5)\n",
        "    print(i)\n",
        "    print(\"acc train = \", clf.score(train_X5, train_y), \"acc val = \", accuracy_score(val_y, predict_y))\n",
        "    print(\"roc_auc = \", roc_auc_score(val_y, proba_y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JadnhlAP_VaP",
        "outputId": "aace6696-f556-4bd8-ccd3-ad6532ca2606"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10, 10)\n",
            "acc train =  0.9192122525972015 acc val =  0.9195923308651708\n",
            "roc_auc =  0.821961579451957\n",
            "(10, 30)\n",
            "acc train =  0.9192192910836454 acc val =  0.9196064078380585\n",
            "roc_auc =  0.8214547424882277\n",
            "(10, 50)\n",
            "acc train =  0.9193107914074158 acc val =  0.9196486387567218\n",
            "roc_auc =  0.7947591092701409\n",
            "(30, 10)\n",
            "acc train =  0.9206340268588643 acc val =  0.9198457163771503\n",
            "roc_auc =  0.8131626394684499\n",
            "(30, 30)\n",
            "acc train =  0.9214786452321293 acc val =  0.918789943410569\n",
            "roc_auc =  0.8110711181393213\n",
            "(30, 50)\n",
            "acc train =  0.9222423210112898 acc val =  0.9192826374616403\n",
            "roc_auc =  0.8161048198409586\n",
            "(50, 10)\n",
            "acc train =  0.9215631070694558 acc val =  0.9192685604887525\n",
            "roc_auc =  0.7949153026066511\n",
            "(50, 30)\n",
            "acc train =  0.9235409217601847 acc val =  0.9167910132605085\n",
            "roc_auc =  0.8040276143000082\n",
            "(50, 50)\n",
            "acc train =  0.9251210619668346 acc val =  0.9182409414679468\n",
            "roc_auc =  0.7895254223089432\n"
          ]
        }
      ],
      "source": [
        "#try two layers\n",
        "import itertools\n",
        "one_layer = list(range(10, 60, 20))\n",
        "layer_size_cases = list(itertools.product(one_layer, one_layer))\n",
        "\n",
        "for size in layer_size_cases:\n",
        "    clf = MLPClassifier(random_state=1, hidden_layer_sizes = size)\n",
        "    clf.fit(train_X5, train_y)\n",
        "    proba_y = clf.predict_proba(val_X5)[:, 1]\n",
        "    predict_y = clf.predict(val_X5)\n",
        "    print(size)\n",
        "    print(\"acc train = \", clf.score(train_X5, train_y), \"acc val = \", accuracy_score(val_y, predict_y))\n",
        "    print(\"roc_auc = \", roc_auc_score(val_y, proba_y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqiVFIqkB-8p"
      },
      "source": [
        "best is 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "ddQ2XcKW8Gx4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "identity\n",
            "acc train =  0.9184274613587095 acc val =  0.9193389453531913\n",
            "roc_auc =  0.738539578285282\n",
            "logistic\n",
            "acc train =  0.9195430614600636 acc val =  0.9180579408204059\n",
            "roc_auc =  0.8130700888924307\n",
            "tanh\n",
            "acc train =  0.919578253892283 acc val =  0.917832709254202\n",
            "roc_auc =  0.8176415014183847\n",
            "relu\n",
            "acc train =  0.9193882147582983 acc val =  0.9183254033052732\n",
            "roc_auc =  0.8273038482960513\n"
          ]
        }
      ],
      "source": [
        "#compare activation function\n",
        "\n",
        "for act in ['identity', 'logistic', 'tanh', 'relu']:\n",
        "\n",
        "    clf = MLPClassifier( random_state = 1, hidden_layer_sizes = [20], activation = act)\n",
        "    clf.fit(train_X5, train_y)\n",
        "    proba_y = clf.predict_proba(val_X5)[:, 1]\n",
        "    predict_y = clf.predict(val_X5)\n",
        "    print(act)\n",
        "    print(\"acc train = \", clf.score(train_X5, train_y), \"acc val = \", accuracy_score(val_y, predict_y))\n",
        "    print(\"roc_auc = \", roc_auc_score(val_y, proba_y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "YWXLvv71NLo_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "acc train =  0.9193882147582983 acc val =  0.9183254033052732\n",
            "roc_auc =  0.8273038482960513\n",
            "2\n",
            "acc train =  0.9187371547622399 acc val =  0.9156226245108252\n",
            "roc_auc =  0.825431346890551\n",
            "31\n",
            "acc train =  0.9190750021115459 acc val =  0.9181001717390692\n",
            "roc_auc =  0.8234608175681852\n",
            "21\n",
            "acc train =  0.9193002336777499 acc val =  0.9176919395253245\n",
            "roc_auc =  0.8277295490493085\n",
            "42\n",
            "acc train =  0.919243925786199 acc val =  0.9187054815732425\n",
            "roc_auc =  0.8233001265669132\n"
          ]
        }
      ],
      "source": [
        "for rand in [1, 2, 31, 21, 42]:\n",
        "\n",
        "    clf = MLPClassifier( random_state = rand, hidden_layer_sizes = [20])\n",
        "    clf.fit(train_X5, train_y)\n",
        "    proba_y = clf.predict_proba(val_X5)[:, 1]\n",
        "    predict_y = clf.predict(val_X5)\n",
        "    print(rand)\n",
        "    print(\"acc train = \", clf.score(train_X5, train_y), \"acc val = \", accuracy_score(val_y, predict_y))\n",
        "    print(\"roc_auc = \", roc_auc_score(val_y, proba_y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "S7VX4W2qxGR6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final: random_state=21, hidden_layer_sizes = [20]\n",
            "acc train =  0.9193002336777499 acc val =  0.9176919395253245\n",
            "roc_auc =  0.8277295490493085\n"
          ]
        }
      ],
      "source": [
        "#final\n",
        "clf = MLPClassifier(random_state=21, hidden_layer_sizes = [20])\n",
        "clf.fit(train_X5, train_y)\n",
        "proba_y = clf.predict_proba(val_X5)[:, 1]\n",
        "predict_y = clf.predict(val_X5)\n",
        "print(\"Final: random_state=21, hidden_layer_sizes = [20]\")\n",
        "acc = accuracy_score(val_y, predict_y)\n",
        "print(\"acc train = \", clf.score(train_X5, train_y), \"acc val = \", acc)\n",
        "roc_auc = roc_auc_score(val_y, proba_y)\n",
        "print(\"roc_auc = \", roc_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "3-hUA5xVmdzp"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/9j/mrbb80v92lxdv_3kldctnqq00000gn/T/ipykernel_19384/2869709709.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df_metrics = df_metrics.append(\n"
          ]
        }
      ],
      "source": [
        "df_metrics = df_metrics.append(\n",
        "    pd.DataFrame({'model' : [\"MLP\"],\n",
        "                                \"hyperparams\" : [\"rand_st=21, layers = [20], act_f = relu\"],\n",
        "                                'accuracy' : \"0.917691\", \n",
        "                                'roc_auc' : \"0.82772\"})\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "HPZhiU5hqvjE"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>hyperparams</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>roc_auc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Naive classifier</td>\n",
              "      <td>baseline</td>\n",
              "      <td>0.919606</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Rand_forest</td>\n",
              "      <td>{'n_est': 1600, 'min_samp_splt': 2, 'min_samp_...</td>\n",
              "      <td>0.920211</td>\n",
              "      <td>0.507280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>MLP</td>\n",
              "      <td>rand_st=21, layers = [20], act_f = relu</td>\n",
              "      <td>0.917691</td>\n",
              "      <td>0.82772</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              model                                        hyperparams  \\\n",
              "0  Naive classifier                                           baseline   \n",
              "0       Rand_forest  {'n_est': 1600, 'min_samp_splt': 2, 'min_samp_...   \n",
              "0               MLP            rand_st=21, layers = [20], act_f = relu   \n",
              "\n",
              "   accuracy   roc_auc  \n",
              "0  0.919606       0.5  \n",
              "0  0.920211  0.507280  \n",
              "0  0.917691   0.82772  "
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQkTg2El0aFv"
      },
      "source": [
        "# Numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "7ooeB5ArjC5_"
      },
      "outputs": [],
      "source": [
        "from numpy.random import default_rng\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class multilayer_perceptron():  \n",
        "    def __init__(self, shape, lr, gamma = 0.9, epochs = 100, batch_size = 1):\n",
        "        #hyperparameters\n",
        "        self.shape = shape\n",
        "        self.layers = len(shape)\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "        self.epochs = epochs\n",
        "        self.bs = batch_size\n",
        "        \n",
        "        #train atr\n",
        "        self.weights, self.biases = self.init_param()\n",
        "        self.v_weights, self.v_biases = self.init_param(weights_zero = True)\n",
        "        self.a = list()\n",
        "        self.z = list()\n",
        "        self.f_activ = np.vectorize(self.relu)\n",
        "        self.f_activ_der = np.vectorize(self.relu_der)\n",
        "        \n",
        "        #score atr\n",
        "        self.train_loss = list()\n",
        "        self.val_loss = list()\n",
        "        self.train_accuracy = list()\n",
        "        self.val_accuracy = list()\n",
        "    \n",
        "    def init_param(self, weights_zero = False):\n",
        "        weights = list()\n",
        "        biases = list()\n",
        "        np.random.seed(100)\n",
        "        for i in range(self.layers - 1):\n",
        "            if weights_zero == True:\n",
        "                w = np.zeros((self.shape[i+1], self.shape[i]))\n",
        "            else:\n",
        "                w = np.random.randn(self.shape[i+1], self.shape[i]) * np.sqrt(1. / self.shape[i+1])\n",
        "            b = np.zeros((self.shape[i+1], 1))\n",
        "            weights.append(w)\n",
        "            biases.append(b)\n",
        "        return weights, biases\n",
        "        \n",
        "    def relu(self, x):\n",
        "        return max(0.0,x)\n",
        "    \n",
        "    def relu_der(self, x):\n",
        "        return float(x >= 0)\n",
        "\n",
        "    # def sigmoid(self, x):\n",
        "    #     return 1/(1 + np.exp(-x))\n",
        "\n",
        "    # def sigmoid_der(self, x):\n",
        "    #     return (np.exp(-x))/((np.exp(-x)+1)**2)\n",
        "\n",
        "    def softmax(self, vect):\n",
        "        sm = np.exp(vect) / np.sum(np.exp(vect), axis=0, keepdims=True)\n",
        "        return sm\n",
        "\n",
        "    def plot(self, data_train = None, data_val = None, title = None, val_show = False):\n",
        "        \n",
        "        fig, ax = plt.subplots(1, figsize=(8, 6))   \n",
        "        if title is not None:\n",
        "            fig.suptitle(title, fontsize=15)\n",
        "        if data_train is not None:\n",
        "            ax.plot(data_train, color=\"red\", label=\"Train\")\n",
        "        if data_val is not None and val_show == True:\n",
        "            ax.plot(data_val, color=\"green\", label=\"Validation\")\n",
        "        plt.legend(loc=\"lower right\", title=\"Legend Title\", frameon=False)\n",
        "        plt.show()\n",
        "\n",
        "    def show(self, val_show = False):\n",
        "\n",
        "        if len(self.train_accuracy) == self.epochs and len(self.train_loss) == self.epochs:\n",
        "            for e in range(self.epochs):\n",
        "                print('Epoch: {0}, TRAIN: Loss {1}, Accuracy: {2:.2f}%'.format(e+1, self.train_loss[e], self.train_accuracy[e] * 100))\n",
        "        \n",
        "        if len(self.val_accuracy) == self.epochs and len(self.val_loss) == self.epochs and val_show == True:\n",
        "            for e in range(self.epochs):\n",
        "                print('Epoch: {0}, VALIDATION: Loss {1}, Accuracy: {2:.2f}%'.format(e+1, self.val_loss[e], self.val_accuracy[e] * 100))\n",
        "            print('REMINDER: Epoch: {0}, TRAIN: Loss {1}, Accuracy: {2:.2f}%'.format(e+1, self.train_loss[e], self.train_accuracy[e] * 100))\n",
        "        \n",
        "        self.plot(data_train = self.train_accuracy, data_val = self.val_accuracy, title = \"Accuracy\", val_show = val_show)\n",
        "        self.plot(data_train = self.train_loss, data_val = self.val_loss, title = \"Loss\", val_show = val_show)\n",
        "\n",
        "    def feedforward(self, x, weights = None, biases = None, show = False):\n",
        "        z = x\n",
        "        if weights == None:\n",
        "            weights = self.weights\n",
        "        if biases == None:\n",
        "            biases = self.biases\n",
        "        self.a = list()\n",
        "        self.z = list()\n",
        "        for i in range(self.layers - 1):\n",
        "            a = np.dot(weights[i], z) + biases[i]\n",
        "            self.a.append(a)\n",
        "            if i != self.layers - 2:\n",
        "                z = self.f_activ(a)\n",
        "            else:\n",
        "                z = self.softmax(a)\n",
        "            self.z.append(z)\n",
        "        return z\n",
        "    \n",
        "    # def predict(self, x):\n",
        "    #     p_predicted = self.feedforward(x.T)\n",
        "    #     y = np.argmax(p_predicted.T, axis = 1)\n",
        "    #     y_df = pd.Series(y)\n",
        "    #     rev_map = dict()\n",
        "    #     for k, i in y_map.items():\n",
        "    #         rev_map[i] = k\n",
        "    #     y_df = y_df.map(rev_map)\n",
        "    #     return y_df\n",
        "   \n",
        "    def backpropagation(self, x, y_target, p_predicted, nst_weights, nst_biases, show = False):\n",
        "        grad_weights = [None] * (self.layers - 1)\n",
        "        grad_biases = [None] * (self.layers - 1)\n",
        "        grad_a = p_predicted - y_target\n",
        "        for i in range(self.layers - 2, 0, -1):\n",
        "            grad_biases[i] = np.sum(grad_a, axis = 1)\n",
        "            grad_biases[i] = grad_biases[i].reshape(len(grad_biases[i]), 1)\n",
        "            grad_weights[i] = np.dot(grad_a, self.z[i-1].T)\n",
        "            grad_z = np.dot(nst_weights[i].T, grad_a)\n",
        "            grad_a = np.multiply(self.f_activ_der(self.a[i - 1]), grad_z)\n",
        "        grad_biases[0] = np.sum(grad_a, axis = 1)\n",
        "        grad_biases[0] = grad_biases[0].reshape(len(grad_biases[0]), 1)\n",
        "        grad_weights[0] = np.dot(grad_a, x.T)\n",
        "        return grad_weights, grad_biases\n",
        "\n",
        "    def compute_accuracy(self, p_predicted, y_val):\n",
        "        y_pr = np.argmax(p_predicted, axis = 1)\n",
        "        y_vl = np.argmax(y_val, axis = 1)\n",
        "        ac = np.mean(y_pr == y_vl)\n",
        "        return ac\n",
        "\n",
        "    def metrics(self, p_predicted, y_val, train_val_label):\n",
        "        def auc(x, y): \n",
        "            if len(x) < 1:\n",
        "                return None\n",
        "            df = pd.DataFrame({'x' : x, 'y': y})\n",
        "            df.sort_values(by = 'x', inplace = True, ignore_index = True)\n",
        "            x_np = np.array(df.x)\n",
        "            y_np = np.array(df.y)\n",
        "            delta_x = x_np[1:] - x_np[:-1]\n",
        "            aver_y = (y_np[:-1] + y_np[1:]) / 2\n",
        "            delta_square = delta_x * aver_y\n",
        "            auc = np.sum(delta_square)\n",
        "            return auc\n",
        "\n",
        "        def auc_roc(y_val, y_probability):\n",
        "            y = pd.concat([y_val, y_probability], axis = 1)\n",
        "            y.columns = [\"targ\", \"prob\"]\n",
        "            y.sort_values([\"prob\"], inplace = True, ignore_index = True)\n",
        "            df_ones = pd.DataFrame(np.ones(len(y['targ'])), columns = ['pred'])\n",
        "            y = pd.concat([y, df_ones], axis = 1)\n",
        "    \n",
        "            FPR_abscissa = list()\n",
        "            TPR_ordinate = list()\n",
        "\n",
        "            TP = y[(y[\"targ\"] == 1) & (y[\"pred\"] == 1)][\"targ\"].count()\n",
        "            FP = y[(y[\"targ\"] == 0) & (y[\"pred\"] == 1)][\"targ\"].count()\n",
        "            TN = y[(y[\"targ\"] == 0) & (y[\"pred\"] == 0)][\"targ\"].count()\n",
        "            FN = y[(y[\"targ\"] == 1) & (y[\"pred\"] == 0)][\"targ\"].count()\n",
        "\n",
        "            TPR = TP / (TP + FN)\n",
        "            FPR = FP / (FP + TN)\n",
        "            TPR_ordinate.append(TPR)\n",
        "            FPR_abscissa.append(FPR)\n",
        "            for i in y.index:\n",
        "                y.loc[i, 'pred'] = 0\n",
        "                #there is the easier calculation but not so visual\n",
        "                TP = y[(y[\"targ\"] == 1) & (y[\"pred\"] == 1)][\"targ\"].count()\n",
        "                FP = y[(y[\"targ\"] == 0) & (y[\"pred\"] == 1)][\"targ\"].count()\n",
        "                TN = y[(y[\"targ\"] == 0) & (y[\"pred\"] == 0)][\"targ\"].count()\n",
        "                FN = y[(y[\"targ\"] == 1) & (y[\"pred\"] == 0)][\"targ\"].count()\n",
        "\n",
        "                TPR = TP / (TP + FN)\n",
        "                FPR = FP / (FP + TN)\n",
        "                TPR_ordinate.append(TPR)\n",
        "                FPR_abscissa.append(FPR)\n",
        "        \n",
        "            plt.plot(FPR_abscissa, TPR_ordinate)\n",
        "            plt.xlabel('x - False Positive Rate')\n",
        "            plt.ylabel('y - True Positive Rate')\n",
        "            plt.title(train_val_label + ' Receiver Operating Curve')\n",
        "            plt.show()\n",
        "            AUC = auc(FPR_abscissa, TPR_ordinate)\n",
        "            return AUC\n",
        "        def auc_pr(y_target, y_probability):\n",
        "            y = pd.concat([y_target, y_probability], axis = 1)\n",
        "            y.columns = [\"targ\", \"prob\"]\n",
        "            y.sort_values([\"prob\"], inplace = True, ignore_index = True)\n",
        "            df_ones = pd.DataFrame(np.ones(len(y['targ'])), columns = ['pred'])\n",
        "            y = pd.concat([y, df_ones], axis = 1)\n",
        "    \n",
        "            recall_abscissa = list()\n",
        "            precision_ordinate = list()\n",
        "\n",
        "            TP = y[(y[\"targ\"] == 1) & (y[\"pred\"] == 1)][\"targ\"].count()\n",
        "            FP = y[(y[\"targ\"] == 0) & (y[\"pred\"] == 1)][\"targ\"].count()\n",
        "            FN = y[(y[\"targ\"] == 1) & (y[\"pred\"] == 0)][\"targ\"].count()\n",
        "\n",
        "            recall = TP / (TP + FN)\n",
        "            precision = TP / (TP + FP)\n",
        "            precision_ordinate.append(precision)\n",
        "            recall_abscissa.append(recall)\n",
        "            for i in y.index - 1:\n",
        "                y.loc[i, 'pred'] = 0\n",
        "                #there is the easier calculation but not so visual\n",
        "                TP = y[(y[\"targ\"] == 1) & (y[\"pred\"] == 1)][\"targ\"].count()\n",
        "                FP = y[(y[\"targ\"] == 0) & (y[\"pred\"] == 1)][\"targ\"].count()\n",
        "                FN = y[(y[\"targ\"] == 1) & (y[\"pred\"] == 0)][\"targ\"].count()\n",
        "\n",
        "                recall = TP / (TP + FN)\n",
        "                precision = TP / (TP + FP)\n",
        "                precision_ordinate.append(precision)\n",
        "                recall_abscissa.append(recall)\n",
        "        \n",
        "            plt.plot(recall_abscissa, precision_ordinate)\n",
        "            plt.xlabel('x - RECALL')\n",
        "            plt.ylabel('y - PRECISION')\n",
        "            plt.title(train_val_label + ' PR Curve')\n",
        "            plt.show()\n",
        "            AUC = auc(recall_abscissa, precision_ordinate)\n",
        "            return AUC\n",
        "\n",
        "        y_pred = np.argmax(p_predicted, axis = 1)\n",
        "        y = pd.concat([pd.DataFrame(y_val), pd.DataFrame(y_pred)], axis = 1)\n",
        "        y.columns = [\"val\", \"pred\"]\n",
        "        TP = y[(y[\"val\"] == 1) & (y[\"pred\"] == 1)][\"val\"].count()\n",
        "        FP = y[(y[\"val\"] == 0) & (y[\"pred\"] == 1)][\"val\"].count()\n",
        "        TN = y[(y[\"val\"] == 0) & (y[\"pred\"] == 0)][\"val\"].count()\n",
        "        FN = y[(y[\"val\"] == 1) & (y[\"pred\"] == 0)][\"val\"].count()\n",
        "        #accuracy\n",
        "        if TP + FP + TN + FN != 0:\n",
        "            acc = (TP + TN) / (TP + FP + TN + FN)\n",
        "            print('{0} FINAL ACCURACY : {1:.2f}%'.format(train_val_label, acc*100))\n",
        "        #recall\n",
        "        if (TP + FP) != 0:\n",
        "            rec = TP / (TP + FP)\n",
        "            print('{0} FINAL RECALL : {1:.2f}%'.format(train_val_label, rec*100))\n",
        "        #precision\n",
        "        if (TP + FN) != 0:\n",
        "            prec = TP / (TP + FN)\n",
        "            print('{0} FINAL PRECISION : {1:.2f}%'.format(train_val_label, prec*100))\n",
        "        #F-score\n",
        "        if rec is not None and rec != 0 and prec is not None and prec != 0:\n",
        "            f_score = 2 / (1/rec + 1/prec)\n",
        "            print(\"{0} FINAL F-SCORE :  {1:.2f}%\".format(train_val_label, f_score))\n",
        "        #AUC_ROC\n",
        "        auc_roc = auc_roc(pd.DataFrame(y_val), pd.DataFrame(p_predicted, columns = [0,1])[1])\n",
        "        print(\"{0} FINAL AUC_ROC : {1:.7f} \".format(train_val_label, auc_roc))\n",
        "        #AUC_PR\n",
        "        auc_pr = auc_pr(pd.DataFrame(y_val), pd.DataFrame(p_predicted, columns = [0,1])[1])\n",
        "        print(\"{0} FINAL AUC_PR : {1:.7f} \".format(train_val_label, auc_pr))\n",
        "\n",
        "    def loss_function(self, p_predict, y_val):\n",
        "        loss = -np.mean(np.log(np.sum(np.multiply(p_predict, y_val), axis = 1)))\n",
        "        return(loss)\n",
        "\n",
        "    def weights_biases_update(self, grad_weights, grad_biases, nst_vect = False):\n",
        "        nst_weights = list()\n",
        "        nst_biases = list()\n",
        "        for i in range(self.layers - 1):\n",
        "            #moment optimization\n",
        "            if nst_vect == True:\n",
        "                nst_weights.append(self.weights[i] - self.gamma*self.v_weights[i])\n",
        "                nst_biases.append(self.biases[i] - self.gamma*self.v_biases[i])\n",
        "            else:\n",
        "                self.v_weights[i] = self.gamma*self.v_weights[i] + self.lr * grad_weights[i]\n",
        "                self.v_biases[i] = self.gamma*self.v_biases[i] + self.lr * grad_biases[i]\n",
        "                self.weights[i] -= self.v_weights[i]\n",
        "                self.biases[i] -= self.v_biases[i]\n",
        "        return nst_weights, nst_biases\n",
        "\n",
        "    def train(self, x_train, y_train, x_val = None, y_val = None):\n",
        "        xy = np.c_[x_train, y_train]\n",
        "        rng = np.random.default_rng(speed)\n",
        "        for e in range(self.epochs):\n",
        "            rng.shuffle(xy)\n",
        "            x_train, y_train= xy[:,:-2], xy[:,-2:]\n",
        "            for i in range(x_train.shape[0] // self.bs):\n",
        "                x,y = x_train[i * self.bs: (i + 1) * self.bs], y_train[i * self.bs: (i + 1) * self.bs]\n",
        "                x, y = x.T, y.T\n",
        "                #nesterom momentum\n",
        "                nst_weights, nst_biases = self.weights_biases_update(None, None, nst_vect = True)\n",
        "                p_predicted = self.feedforward(x, nst_weights, nst_biases)\n",
        "                grad_weights, grad_biases = self.backpropagation(x, y, p_predicted, nst_weights, nst_biases, show = False)\n",
        "                self.weights_biases_update(grad_weights, grad_biases)\n",
        "            \n",
        "            #train scores\n",
        "            p_predicted = self.feedforward(x_train.T)\n",
        "            accuracy = self.compute_accuracy(p_predicted.T, y_train)\n",
        "            loss = self.loss_function(p_predicted.T, y_train)\n",
        "            roc_auc = roc_auc_score(y_train[:, 0], p_predicted.T[:, 0])\n",
        "            print(e, \"TRAIN: loss =\", loss, \"accuracu =\", accuracy, \"roc_auc =\", roc_auc)\n",
        "            self.train_accuracy.append(accuracy)\n",
        "            self.train_loss.append(loss)\n",
        "            \n",
        "            #validation score\n",
        "            if x_val is not None and y_val is not None:\n",
        "                p_predicted = self.feedforward(x_val.T)\n",
        "                accuracy = self.compute_accuracy(p_predicted.T, y_val)\n",
        "                loss = self.loss_function(p_predicted.T, y_val)\n",
        "                roc_auc = roc_auc_score(y_val[:, 0], p_predicted.T[:, 0])\n",
        "                print(\"VAL: loss = \", loss, \"accuracu: \", accuracy, \"roc_auc =\", roc_auc)\n",
        "                self.val_accuracy.append(accuracy)\n",
        "                self.val_loss.append(loss)\n",
        "        \n",
        "        # #final metrics\n",
        "        # if show_full_train_metrics == True:\n",
        "        #     p_predicted = self.feedforward(x_train.T)\n",
        "        #     self.metrics(p_predicted.T, y_train[:, 1], \"TRAIN\")\n",
        "\n",
        "        #final metrics\n",
        "        # if x_val is not None and y_val is not None and show_full_validation_metrics == True:\n",
        "        #     p_predicted = self.feedforward(x_val.T)\n",
        "        #     self.metrics(p_predicted.T, y_val[:, 1], \"VALIDATION\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "KZJGtwUIjZN2"
      },
      "outputs": [],
      "source": [
        "#general\n",
        "speed = 100\n",
        "show_validation_score = True\n",
        "show_full_train_metrics = False\n",
        "show_full_validation_metrics = True\n",
        "show_defaul_accuracy_track_plot = True\n",
        "\n",
        "#preprocessing params\n",
        "split_frac = 0.70\n",
        "y_label = 1\n",
        "index_label = 0\n",
        "y_map = {0 : 1, 1 : 0}\n",
        "\n",
        "#perceptron params\n",
        "hidden_layers_topology = [25]\n",
        "epochs = 80\n",
        "batch_size = 1000\n",
        "lr = 0.0001\n",
        "accumulation_rate = 0.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "QXYnqXl_6mZP"
      },
      "outputs": [],
      "source": [
        "train_y_ohe = pd.concat([train_y, (1+train_y)%2], axis = 1)\n",
        "val_y_ohe = pd.concat([val_y, (1+val_y)%2], axis = 1)\n",
        "\n",
        "train_y_ohe.columns = ['1', '0']\n",
        "val_y_ohe.columns = ['1', '0']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "QOt2gqwGjkOP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 TRAIN: loss = 0.2543301487589276 accuracu = 0.9183007686027197 roc_auc = 0.7412412706827352\n",
            "VAL: loss =  0.25233771940124133 accuracu:  0.9192826374616403 roc_auc = 0.739906273330212\n",
            "1 TRAIN: loss = 0.24849529464421286 accuracu = 0.9181424026577325 roc_auc = 0.7651633799341445\n",
            "VAL: loss =  0.24701843132213172 accuracu:  0.9193389453531913 roc_auc = 0.7622546093337635\n",
            "2 TRAIN: loss = 0.2449290575461483 accuracu = 0.918448576818041 roc_auc = 0.775619456627293\n",
            "VAL: loss =  0.24354690060237164 accuracu:  0.9195923308651708 roc_auc = 0.7729200708121132\n",
            "3 TRAIN: loss = 0.24321132567375467 accuracu = 0.9186175004926941 roc_auc = 0.7816011629497255\n",
            "VAL: loss =  0.24182192049825868 accuracu:  0.919803485458487 roc_auc = 0.7793046694441953\n",
            "4 TRAIN: loss = 0.24220554054626073 accuracu = 0.9185435963850334 roc_auc = 0.7850669243229885\n",
            "VAL: loss =  0.2409202361524819 accuracu:  0.9195078690278442 roc_auc = 0.7813800093655962\n",
            "5 TRAIN: loss = 0.24036889405881987 accuracu = 0.9188216165995664 roc_auc = 0.7888483013647479\n",
            "VAL: loss =  0.23890402809708183 accuracu:  0.919690869675385 roc_auc = 0.7866743263487914\n",
            "6 TRAIN: loss = 0.2392049948770003 accuracu = 0.9188005011402348 roc_auc = 0.7923256462275201\n",
            "VAL: loss =  0.23731863693984342 accuracu:  0.9199583321602522 roc_auc = 0.7905924436867914\n",
            "7 TRAIN: loss = 0.23768094625417624 accuracu = 0.9189342323826685 roc_auc = 0.7960761074399554\n",
            "VAL: loss =  0.23663160856675652 accuracu:  0.919803485458487 roc_auc = 0.7921525195538722\n",
            "8 TRAIN: loss = 0.23663306357958008 accuracu = 0.919011655733551 roc_auc = 0.7992793888493486\n",
            "VAL: loss =  0.23536628629801704 accuracu:  0.919690869675385 roc_auc = 0.7963533196787841\n",
            "9 TRAIN: loss = 0.2360723039627092 accuracu = 0.9188990399504491 roc_auc = 0.8007980117944482\n",
            "VAL: loss =  0.23554987715986736 accuracu:  0.9196204848109463 roc_auc = 0.7965246624796767\n",
            "10 TRAIN: loss = 0.23568192211421712 accuracu = 0.9186914046003547 roc_auc = 0.8030272235287322\n",
            "VAL: loss =  0.2338537847647513 accuracu:  0.919240406542977 roc_auc = 0.801863832813363\n",
            "11 TRAIN: loss = 0.23332121987496576 accuracu = 0.9190644443818802 roc_auc = 0.8073494131357424\n",
            "VAL: loss =  0.2322956096815643 accuracu:  0.9198738703229258 roc_auc = 0.8045096980253137\n",
            "12 TRAIN: loss = 0.23250876918720972 accuracu = 0.9190820405979898 roc_auc = 0.8099967918789022\n",
            "VAL: loss =  0.23154018544083702 accuracu:  0.9196204848109463 roc_auc = 0.8069073285954768\n",
            "13 TRAIN: loss = 0.23254249348053674 accuracu = 0.9184309806019314 roc_auc = 0.8106490955719491\n",
            "VAL: loss =  0.23280141513217742 accuracu:  0.9183535572510487 roc_auc = 0.8061073473516502\n",
            "14 TRAIN: loss = 0.23203645611851723 accuracu = 0.9188814437343393 roc_auc = 0.8125747443043001\n",
            "VAL: loss =  0.23112741448551996 accuracu:  0.9194797150820687 roc_auc = 0.8092440477045484\n",
            "15 TRAIN: loss = 0.23120829021117384 accuracu = 0.918962386328444 roc_auc = 0.81318349393812\n",
            "VAL: loss =  0.23104819231246482 accuracu:  0.9193248683803035 roc_auc = 0.8090150420538221\n",
            "16 TRAIN: loss = 0.2309754504033113 accuracu = 0.9190186942199949 roc_auc = 0.8155461530316759\n",
            "VAL: loss =  0.23028416489314654 accuracu:  0.9195923308651708 roc_auc = 0.8115644058209464\n",
            "17 TRAIN: loss = 0.2310627015670215 accuracu = 0.9189447901123342 roc_auc = 0.8141617191814232\n",
            "VAL: loss =  0.23080814384215725 accuracu:  0.919184098651426 roc_auc = 0.8106782345246286\n",
            "18 TRAIN: loss = 0.23045386650159216 accuracu = 0.9191559447056505 roc_auc = 0.8168320598656345\n",
            "VAL: loss =  0.2297925198186732 accuracu:  0.9197894084855992 roc_auc = 0.8122750368533103\n",
            "19 TRAIN: loss = 0.2295983675171035 accuracu = 0.9186139812494721 roc_auc = 0.8181621187036794\n",
            "VAL: loss =  0.22926284563456475 accuracu:  0.9188040203834568 roc_auc = 0.8147760520644312\n",
            "20 TRAIN: loss = 0.2295405154767618 accuracu = 0.9189236746530026 roc_auc = 0.8191416535815961\n",
            "VAL: loss =  0.22930363821341015 accuracu:  0.9190574058954363 roc_auc = 0.8153878336458117\n",
            "21 TRAIN: loss = 0.22907225794165845 accuracu = 0.9186456544384696 roc_auc = 0.8199099171405388\n",
            "VAL: loss =  0.22974704739024732 accuracu:  0.9184380190883752 roc_auc = 0.8152612530627509\n",
            "22 TRAIN: loss = 0.22969698405178077 accuracu = 0.9187054815732425 roc_auc = 0.8185639503608616\n",
            "VAL: loss =  0.23096275774368039 accuracu:  0.9182972493594977 roc_auc = 0.8142886504804324\n",
            "23 TRAIN: loss = 0.22898147857947015 accuracu = 0.9190538866522143 roc_auc = 0.8205724398796209\n",
            "VAL: loss =  0.22990415455605764 accuracu:  0.9189307131394465 roc_auc = 0.8158476756415619\n",
            "24 TRAIN: loss = 0.22881042939337212 accuracu = 0.9189307131394465 roc_auc = 0.8203782613311741\n",
            "VAL: loss =  0.23022531138939112 accuracu:  0.9190855598412118 roc_auc = 0.814159772818289\n",
            "25 TRAIN: loss = 0.2286040601233654 accuracu = 0.9188708860046736 roc_auc = 0.8213603515896755\n",
            "VAL: loss =  0.22820099617047718 accuracu:  0.9184943269799263 roc_auc = 0.8189384933809962\n",
            "26 TRAIN: loss = 0.22730285665049432 accuracu = 0.9189870210309975 roc_auc = 0.8234882029206911\n",
            "VAL: loss =  0.22869676905971054 accuracu:  0.918395788169712 roc_auc = 0.8183660409027442\n",
            "27 TRAIN: loss = 0.22793335561735414 accuracu = 0.9189905402742194 roc_auc = 0.8223327585092286\n",
            "VAL: loss =  0.23004945161392623 accuracu:  0.9186491736816915 roc_auc = 0.8149569155478231\n",
            "28 TRAIN: loss = 0.22800584424209142 accuracu = 0.9188462513021199 roc_auc = 0.821887488683844\n",
            "VAL: loss =  0.22938605658081493 accuracu:  0.9188040203834568 roc_auc = 0.8158685557419758\n",
            "29 TRAIN: loss = 0.22794749893489516 accuracu = 0.918958867085222 roc_auc = 0.8221084872548892\n",
            "VAL: loss =  0.22956364568173862 accuracu:  0.9188884822207832 roc_auc = 0.8164470551401932\n",
            "30 TRAIN: loss = 0.22640217541664232 accuracu = 0.9191594639488724 roc_auc = 0.8255618543967314\n",
            "VAL: loss =  0.22741087721684838 accuracu:  0.9188462513021199 roc_auc = 0.8201629142629009\n",
            "31 TRAIN: loss = 0.22673259768996137 accuracu = 0.9190925983276557 roc_auc = 0.8254972424265568\n",
            "VAL: loss =  0.2283530818931264 accuracu:  0.9187617894647935 roc_auc = 0.8201441168117838\n",
            "32 TRAIN: loss = 0.2266228426895318 accuracu = 0.9187688279512374 roc_auc = 0.8263284143411035\n",
            "VAL: loss =  0.22938993810079658 accuracu:  0.9179171710915285 roc_auc = 0.819414417610698\n",
            "33 TRAIN: loss = 0.22636584078942004 accuracu = 0.919240406542977 roc_auc = 0.8266744277961385\n",
            "VAL: loss =  0.22767069567330236 accuracu:  0.9188040203834568 roc_auc = 0.8198900510200027\n",
            "34 TRAIN: loss = 0.22668444364006968 accuracu = 0.9189553478420001 roc_auc = 0.8262243967811581\n",
            "VAL: loss =  0.22928396438529644 accuracu:  0.9184943269799263 roc_auc = 0.8188324739340425\n",
            "35 TRAIN: loss = 0.22690911471952394 accuracu = 0.9192720797319744 roc_auc = 0.8262083109583588\n",
            "VAL: loss =  0.22767170904258258 accuracu:  0.9187477124919058 roc_auc = 0.8206773286391937\n",
            "36 TRAIN: loss = 0.22583761024429166 accuracu = 0.9193248683803035 roc_auc = 0.8280852957782914\n",
            "VAL: loss =  0.2288748741547802 accuracu:  0.9184661730341508 roc_auc = 0.8206364020341592\n",
            "37 TRAIN: loss = 0.2258680201258209 accuracu = 0.9187547509783496 roc_auc = 0.8283323533211892\n",
            "VAL: loss =  0.2274734320639898 accuracu:  0.918452096061263 roc_auc = 0.8223637116913581\n",
            "38 TRAIN: loss = 0.22648079002750893 accuracu = 0.9186949238435766 roc_auc = 0.8268981629306738\n",
            "VAL: loss =  0.23031603430258774 accuracu:  0.9174807849320082 roc_auc = 0.8184910561483671\n",
            "39 TRAIN: loss = 0.22704758344004144 accuracu = 0.9192157718404235 roc_auc = 0.8252682344980304\n",
            "VAL: loss =  0.22882349887176426 accuracu:  0.918226864495059 roc_auc = 0.816933410038799\n",
            "40 TRAIN: loss = 0.22838393341863256 accuracu = 0.9179664404966356 roc_auc = 0.8240339892249597\n",
            "VAL: loss =  0.23274865140189416 accuracu:  0.9158900869956924 roc_auc = 0.8157797724292598\n",
            "41 TRAIN: loss = 0.22685836923484667 accuracu = 0.918786424167347 roc_auc = 0.8255415983955448\n",
            "VAL: loss =  0.22998584973174527 accuracu:  0.9173400152031307 roc_auc = 0.8175323218124597\n",
            "42 TRAIN: loss = 0.22634235145279588 accuracu = 0.9185717503308088 roc_auc = 0.828522823852108\n",
            "VAL: loss =  0.22938412866153593 accuracu:  0.9169317829893859 roc_auc = 0.8226991495663759\n",
            "43 TRAIN: loss = 0.22631117015309613 accuracu = 0.9192826374616403 roc_auc = 0.825894040665796\n",
            "VAL: loss =  0.22877704040588864 accuracu:  0.9186632506545792 roc_auc = 0.8182476783412329\n",
            "44 TRAIN: loss = 0.22531762131872005 accuracu = 0.9188744052478955 roc_auc = 0.8287362112553465\n",
            "VAL: loss =  0.2286225405660657 accuracu:  0.9174104000675695 roc_auc = 0.8202552638109958\n",
            "45 TRAIN: loss = 0.22535903628712964 accuracu = 0.9191489062192066 roc_auc = 0.8288763297774473\n",
            "VAL: loss =  0.22717608755361507 accuracu:  0.918958867085222 roc_auc = 0.8216468273503595\n",
            "46 TRAIN: loss = 0.2256696341223761 accuracu = 0.9192615220023086 roc_auc = 0.8281580274665598\n",
            "VAL: loss =  0.22813860250938914 accuracu:  0.9189729440581097 roc_auc = 0.8204886599115905\n",
            "47 TRAIN: loss = 0.2255899219658006 accuracu = 0.9187230777893521 roc_auc = 0.8283024172827927\n",
            "VAL: loss =  0.22895203852695808 accuracu:  0.9180297868746304 roc_auc = 0.8209115074085076\n",
            "48 TRAIN: loss = 0.2250872783736029 accuracu = 0.9188779244911174 roc_auc = 0.8294773678706073\n",
            "VAL: loss =  0.22799160022091244 accuracu:  0.917889017145753 roc_auc = 0.8219106966039202\n",
            "49 TRAIN: loss = 0.22474437979443992 accuracu = 0.9191946563810918 roc_auc = 0.8306097824902903\n",
            "VAL: loss =  0.22616353001866016 accuracu:  0.918733635519018 roc_auc = 0.8239278443019535\n",
            "50 TRAIN: loss = 0.2257148275631846 accuracu = 0.9188673667614516 roc_auc = 0.830414420020927\n",
            "VAL: loss =  0.22817531454249346 accuracu:  0.918508403952814 roc_auc = 0.8216783297662983\n",
            "51 TRAIN: loss = 0.22470153716402802 accuracu = 0.9190644443818802 roc_auc = 0.830841698920344\n",
            "VAL: loss =  0.2266478299471501 accuracu:  0.9188321743292323 roc_auc = 0.8234607867439036\n",
            "52 TRAIN: loss = 0.2252956834810176 accuracu = 0.9196169655677243 roc_auc = 0.8279278431285725\n",
            "VAL: loss =  0.2281462267817258 accuracu:  0.9187195585461302 roc_auc = 0.8201217491047295\n",
            "53 TRAIN: loss = 0.2252394771629572 accuracu = 0.9185541541146992 roc_auc = 0.830949854879443\n",
            "VAL: loss =  0.22933689407884522 accuracu:  0.9173963230946817 roc_auc = 0.8230847613309503\n",
            "54 TRAIN: loss = 0.22419649311787876 accuracu = 0.9190609251386582 roc_auc = 0.8316122464569592\n",
            "VAL: loss =  0.2269749714538834 accuracu:  0.918114248711957 roc_auc = 0.823201716696991\n",
            "55 TRAIN: loss = 0.22489449251761418 accuracu = 0.9193107914074158 roc_auc = 0.8303420999769179\n",
            "VAL: loss =  0.2272327691047588 accuracu:  0.9192122525972015 roc_auc = 0.8208999509832272\n",
            "56 TRAIN: loss = 0.22374172761834749 accuracu = 0.9194515611362932 roc_auc = 0.8321139996197309\n",
            "VAL: loss =  0.2268596392415813 accuracu:  0.9188884822207832 roc_auc = 0.8220627031988584\n",
            "57 TRAIN: loss = 0.22405977324260734 accuracu = 0.9192157718404235 roc_auc = 0.8317586605691242\n",
            "VAL: loss =  0.2275993078399238 accuracu:  0.9182127875221713 roc_auc = 0.8225098857961166\n",
            "58 TRAIN: loss = 0.2242595128015464 accuracu = 0.9189131169233368 roc_auc = 0.8312123398784641\n",
            "VAL: loss =  0.22814144325115954 accuracu:  0.9176356316337735 roc_auc = 0.8216452593325492\n",
            "59 TRAIN: loss = 0.2241780806002528 accuracu = 0.9195852923787269 roc_auc = 0.8309001980516859\n",
            "VAL: loss =  0.2272471524509607 accuracu:  0.9186069427630282 roc_auc = 0.8217435807501846\n",
            "60 TRAIN: loss = 0.22385248858130286 accuracu = 0.9193811762718545 roc_auc = 0.8319294186475723\n",
            "VAL: loss =  0.22763727750147505 accuracu:  0.9185787888172527 roc_auc = 0.8215856317698013\n",
            "61 TRAIN: loss = 0.22513494480957058 accuracu = 0.9189201554097807 roc_auc = 0.8302304456606917\n",
            "VAL: loss =  0.22934885205741998 accuracu:  0.9180579408204059 roc_auc = 0.8211530545213437\n",
            "62 TRAIN: loss = 0.22385124002784873 accuracu = 0.9194761958388468 roc_auc = 0.8327230533436752\n",
            "VAL: loss =  0.22628804359201243 accuracu:  0.9189870210309975 roc_auc = 0.8238163501945255\n",
            "63 TRAIN: loss = 0.22414910379186384 accuracu = 0.919184098651426 roc_auc = 0.8323567326279616\n",
            "VAL: loss =  0.22789668808627644 accuracu:  0.9180297868746304 roc_auc = 0.8219271447086942\n",
            "64 TRAIN: loss = 0.22358983929634144 accuracu = 0.9191031560573214 roc_auc = 0.8328988085209419\n",
            "VAL: loss =  0.22658013526239437 accuracu:  0.9187195585461302 roc_auc = 0.8238609556105765\n",
            "65 TRAIN: loss = 0.2237099128739606 accuracu = 0.9192685604887525 roc_auc = 0.8328996620203684\n",
            "VAL: loss =  0.2267445145565313 accuracu:  0.9187195585461302 roc_auc = 0.8232890673506992\n",
            "66 TRAIN: loss = 0.22302830617144953 accuracu = 0.9194550803795152 roc_auc = 0.8337874211945242\n",
            "VAL: loss =  0.22634444461202616 accuracu:  0.918452096061263 roc_auc = 0.8243707557259112\n",
            "67 TRAIN: loss = 0.22307053799323814 accuracu = 0.91951842675751 roc_auc = 0.8337903713805013\n",
            "VAL: loss =  0.2265315601738091 accuracu:  0.9188040203834568 roc_auc = 0.8239985927294788\n",
            "68 TRAIN: loss = 0.22324100455780638 accuracu = 0.9194339649201836 roc_auc = 0.8332241710868544\n",
            "VAL: loss =  0.2266158436950118 accuracu:  0.9188884822207832 roc_auc = 0.8239477407057239\n",
            "69 TRAIN: loss = 0.22424893225020218 accuracu = 0.9188638475182297 roc_auc = 0.8316514280237663\n",
            "VAL: loss =  0.22744795675923948 accuracu:  0.9182550184408345 roc_auc = 0.8230475416808418\n",
            "70 TRAIN: loss = 0.22347709947382757 accuracu = 0.9193037529209719 roc_auc = 0.8331876616996348\n",
            "VAL: loss =  0.22665595491099866 accuracu:  0.9186350967088037 roc_auc = 0.8238331132430479\n",
            "71 TRAIN: loss = 0.2243422698748626 accuracu = 0.9191911371378698 roc_auc = 0.8315044737471674\n",
            "VAL: loss =  0.22841714076615055 accuracu:  0.9182409414679468 roc_auc = 0.8203468682155839\n",
            "72 TRAIN: loss = 0.22408230493878367 accuracu = 0.9193178298938596 roc_auc = 0.831103257864233\n",
            "VAL: loss =  0.22747355449693668 accuracu:  0.9185787888172527 roc_auc = 0.822127356459716\n",
            "73 TRAIN: loss = 0.22368628664876794 accuracu = 0.919127790759875 roc_auc = 0.833050456796756\n",
            "VAL: loss =  0.22742739528843575 accuracu:  0.9179594020101918 roc_auc = 0.822570623033007\n",
            "74 TRAIN: loss = 0.22338298937800402 accuracu = 0.9191172330302092 roc_auc = 0.8334174736840523\n",
            "VAL: loss =  0.22616253354858307 accuracu:  0.919240406542977 roc_auc = 0.8246703355799616\n",
            "75 TRAIN: loss = 0.22315971781582788 accuracu = 0.9195923308651708 roc_auc = 0.8330672886473905\n",
            "VAL: loss =  0.22653739860031363 accuracu:  0.9187617894647935 roc_auc = 0.8231864707392048\n",
            "76 TRAIN: loss = 0.22346329512437857 accuracu = 0.9190046172471071 roc_auc = 0.8332139706956423\n",
            "VAL: loss =  0.22660578033070036 accuracu:  0.9182409414679468 roc_auc = 0.8240699777454314\n",
            "77 TRAIN: loss = 0.22404931983044793 accuracu = 0.9188673667614516 roc_auc = 0.8327190245622429\n",
            "VAL: loss =  0.2288018471104247 accuracu:  0.9170584757453757 roc_auc = 0.8222223783390192\n",
            "78 TRAIN: loss = 0.22325137283590646 accuracu = 0.9189307131394465 roc_auc = 0.8336091949135505\n",
            "VAL: loss =  0.2265082300815595 accuracu:  0.9180860947661815 roc_auc = 0.8248365856734362\n",
            "79 TRAIN: loss = 0.22281194324764814 accuracu = 0.9191101945437653 roc_auc = 0.834272996085816\n",
            "VAL: loss =  0.22617462132537658 accuracu:  0.9180860947661815 roc_auc = 0.8255813860922026\n",
            "Epoch: 1, TRAIN: Loss 0.2543301487589276, Accuracy: 91.83%\n",
            "Epoch: 2, TRAIN: Loss 0.24849529464421286, Accuracy: 91.81%\n",
            "Epoch: 3, TRAIN: Loss 0.2449290575461483, Accuracy: 91.84%\n",
            "Epoch: 4, TRAIN: Loss 0.24321132567375467, Accuracy: 91.86%\n",
            "Epoch: 5, TRAIN: Loss 0.24220554054626073, Accuracy: 91.85%\n",
            "Epoch: 6, TRAIN: Loss 0.24036889405881987, Accuracy: 91.88%\n",
            "Epoch: 7, TRAIN: Loss 0.2392049948770003, Accuracy: 91.88%\n",
            "Epoch: 8, TRAIN: Loss 0.23768094625417624, Accuracy: 91.89%\n",
            "Epoch: 9, TRAIN: Loss 0.23663306357958008, Accuracy: 91.90%\n",
            "Epoch: 10, TRAIN: Loss 0.2360723039627092, Accuracy: 91.89%\n",
            "Epoch: 11, TRAIN: Loss 0.23568192211421712, Accuracy: 91.87%\n",
            "Epoch: 12, TRAIN: Loss 0.23332121987496576, Accuracy: 91.91%\n",
            "Epoch: 13, TRAIN: Loss 0.23250876918720972, Accuracy: 91.91%\n",
            "Epoch: 14, TRAIN: Loss 0.23254249348053674, Accuracy: 91.84%\n",
            "Epoch: 15, TRAIN: Loss 0.23203645611851723, Accuracy: 91.89%\n",
            "Epoch: 16, TRAIN: Loss 0.23120829021117384, Accuracy: 91.90%\n",
            "Epoch: 17, TRAIN: Loss 0.2309754504033113, Accuracy: 91.90%\n",
            "Epoch: 18, TRAIN: Loss 0.2310627015670215, Accuracy: 91.89%\n",
            "Epoch: 19, TRAIN: Loss 0.23045386650159216, Accuracy: 91.92%\n",
            "Epoch: 20, TRAIN: Loss 0.2295983675171035, Accuracy: 91.86%\n",
            "Epoch: 21, TRAIN: Loss 0.2295405154767618, Accuracy: 91.89%\n",
            "Epoch: 22, TRAIN: Loss 0.22907225794165845, Accuracy: 91.86%\n",
            "Epoch: 23, TRAIN: Loss 0.22969698405178077, Accuracy: 91.87%\n",
            "Epoch: 24, TRAIN: Loss 0.22898147857947015, Accuracy: 91.91%\n",
            "Epoch: 25, TRAIN: Loss 0.22881042939337212, Accuracy: 91.89%\n",
            "Epoch: 26, TRAIN: Loss 0.2286040601233654, Accuracy: 91.89%\n",
            "Epoch: 27, TRAIN: Loss 0.22730285665049432, Accuracy: 91.90%\n",
            "Epoch: 28, TRAIN: Loss 0.22793335561735414, Accuracy: 91.90%\n",
            "Epoch: 29, TRAIN: Loss 0.22800584424209142, Accuracy: 91.88%\n",
            "Epoch: 30, TRAIN: Loss 0.22794749893489516, Accuracy: 91.90%\n",
            "Epoch: 31, TRAIN: Loss 0.22640217541664232, Accuracy: 91.92%\n",
            "Epoch: 32, TRAIN: Loss 0.22673259768996137, Accuracy: 91.91%\n",
            "Epoch: 33, TRAIN: Loss 0.2266228426895318, Accuracy: 91.88%\n",
            "Epoch: 34, TRAIN: Loss 0.22636584078942004, Accuracy: 91.92%\n",
            "Epoch: 35, TRAIN: Loss 0.22668444364006968, Accuracy: 91.90%\n",
            "Epoch: 36, TRAIN: Loss 0.22690911471952394, Accuracy: 91.93%\n",
            "Epoch: 37, TRAIN: Loss 0.22583761024429166, Accuracy: 91.93%\n",
            "Epoch: 38, TRAIN: Loss 0.2258680201258209, Accuracy: 91.88%\n",
            "Epoch: 39, TRAIN: Loss 0.22648079002750893, Accuracy: 91.87%\n",
            "Epoch: 40, TRAIN: Loss 0.22704758344004144, Accuracy: 91.92%\n",
            "Epoch: 41, TRAIN: Loss 0.22838393341863256, Accuracy: 91.80%\n",
            "Epoch: 42, TRAIN: Loss 0.22685836923484667, Accuracy: 91.88%\n",
            "Epoch: 43, TRAIN: Loss 0.22634235145279588, Accuracy: 91.86%\n",
            "Epoch: 44, TRAIN: Loss 0.22631117015309613, Accuracy: 91.93%\n",
            "Epoch: 45, TRAIN: Loss 0.22531762131872005, Accuracy: 91.89%\n",
            "Epoch: 46, TRAIN: Loss 0.22535903628712964, Accuracy: 91.91%\n",
            "Epoch: 47, TRAIN: Loss 0.2256696341223761, Accuracy: 91.93%\n",
            "Epoch: 48, TRAIN: Loss 0.2255899219658006, Accuracy: 91.87%\n",
            "Epoch: 49, TRAIN: Loss 0.2250872783736029, Accuracy: 91.89%\n",
            "Epoch: 50, TRAIN: Loss 0.22474437979443992, Accuracy: 91.92%\n",
            "Epoch: 51, TRAIN: Loss 0.2257148275631846, Accuracy: 91.89%\n",
            "Epoch: 52, TRAIN: Loss 0.22470153716402802, Accuracy: 91.91%\n",
            "Epoch: 53, TRAIN: Loss 0.2252956834810176, Accuracy: 91.96%\n",
            "Epoch: 54, TRAIN: Loss 0.2252394771629572, Accuracy: 91.86%\n",
            "Epoch: 55, TRAIN: Loss 0.22419649311787876, Accuracy: 91.91%\n",
            "Epoch: 56, TRAIN: Loss 0.22489449251761418, Accuracy: 91.93%\n",
            "Epoch: 57, TRAIN: Loss 0.22374172761834749, Accuracy: 91.95%\n",
            "Epoch: 58, TRAIN: Loss 0.22405977324260734, Accuracy: 91.92%\n",
            "Epoch: 59, TRAIN: Loss 0.2242595128015464, Accuracy: 91.89%\n",
            "Epoch: 60, TRAIN: Loss 0.2241780806002528, Accuracy: 91.96%\n",
            "Epoch: 61, TRAIN: Loss 0.22385248858130286, Accuracy: 91.94%\n",
            "Epoch: 62, TRAIN: Loss 0.22513494480957058, Accuracy: 91.89%\n",
            "Epoch: 63, TRAIN: Loss 0.22385124002784873, Accuracy: 91.95%\n",
            "Epoch: 64, TRAIN: Loss 0.22414910379186384, Accuracy: 91.92%\n",
            "Epoch: 65, TRAIN: Loss 0.22358983929634144, Accuracy: 91.91%\n",
            "Epoch: 66, TRAIN: Loss 0.2237099128739606, Accuracy: 91.93%\n",
            "Epoch: 67, TRAIN: Loss 0.22302830617144953, Accuracy: 91.95%\n",
            "Epoch: 68, TRAIN: Loss 0.22307053799323814, Accuracy: 91.95%\n",
            "Epoch: 69, TRAIN: Loss 0.22324100455780638, Accuracy: 91.94%\n",
            "Epoch: 70, TRAIN: Loss 0.22424893225020218, Accuracy: 91.89%\n",
            "Epoch: 71, TRAIN: Loss 0.22347709947382757, Accuracy: 91.93%\n",
            "Epoch: 72, TRAIN: Loss 0.2243422698748626, Accuracy: 91.92%\n",
            "Epoch: 73, TRAIN: Loss 0.22408230493878367, Accuracy: 91.93%\n",
            "Epoch: 74, TRAIN: Loss 0.22368628664876794, Accuracy: 91.91%\n",
            "Epoch: 75, TRAIN: Loss 0.22338298937800402, Accuracy: 91.91%\n",
            "Epoch: 76, TRAIN: Loss 0.22315971781582788, Accuracy: 91.96%\n",
            "Epoch: 77, TRAIN: Loss 0.22346329512437857, Accuracy: 91.90%\n",
            "Epoch: 78, TRAIN: Loss 0.22404931983044793, Accuracy: 91.89%\n",
            "Epoch: 79, TRAIN: Loss 0.22325137283590646, Accuracy: 91.89%\n",
            "Epoch: 80, TRAIN: Loss 0.22281194324764814, Accuracy: 91.91%\n",
            "Epoch: 1, VALIDATION: Loss 0.25233771940124133, Accuracy: 91.93%\n",
            "Epoch: 2, VALIDATION: Loss 0.24701843132213172, Accuracy: 91.93%\n",
            "Epoch: 3, VALIDATION: Loss 0.24354690060237164, Accuracy: 91.96%\n",
            "Epoch: 4, VALIDATION: Loss 0.24182192049825868, Accuracy: 91.98%\n",
            "Epoch: 5, VALIDATION: Loss 0.2409202361524819, Accuracy: 91.95%\n",
            "Epoch: 6, VALIDATION: Loss 0.23890402809708183, Accuracy: 91.97%\n",
            "Epoch: 7, VALIDATION: Loss 0.23731863693984342, Accuracy: 92.00%\n",
            "Epoch: 8, VALIDATION: Loss 0.23663160856675652, Accuracy: 91.98%\n",
            "Epoch: 9, VALIDATION: Loss 0.23536628629801704, Accuracy: 91.97%\n",
            "Epoch: 10, VALIDATION: Loss 0.23554987715986736, Accuracy: 91.96%\n",
            "Epoch: 11, VALIDATION: Loss 0.2338537847647513, Accuracy: 91.92%\n",
            "Epoch: 12, VALIDATION: Loss 0.2322956096815643, Accuracy: 91.99%\n",
            "Epoch: 13, VALIDATION: Loss 0.23154018544083702, Accuracy: 91.96%\n",
            "Epoch: 14, VALIDATION: Loss 0.23280141513217742, Accuracy: 91.84%\n",
            "Epoch: 15, VALIDATION: Loss 0.23112741448551996, Accuracy: 91.95%\n",
            "Epoch: 16, VALIDATION: Loss 0.23104819231246482, Accuracy: 91.93%\n",
            "Epoch: 17, VALIDATION: Loss 0.23028416489314654, Accuracy: 91.96%\n",
            "Epoch: 18, VALIDATION: Loss 0.23080814384215725, Accuracy: 91.92%\n",
            "Epoch: 19, VALIDATION: Loss 0.2297925198186732, Accuracy: 91.98%\n",
            "Epoch: 20, VALIDATION: Loss 0.22926284563456475, Accuracy: 91.88%\n",
            "Epoch: 21, VALIDATION: Loss 0.22930363821341015, Accuracy: 91.91%\n",
            "Epoch: 22, VALIDATION: Loss 0.22974704739024732, Accuracy: 91.84%\n",
            "Epoch: 23, VALIDATION: Loss 0.23096275774368039, Accuracy: 91.83%\n",
            "Epoch: 24, VALIDATION: Loss 0.22990415455605764, Accuracy: 91.89%\n",
            "Epoch: 25, VALIDATION: Loss 0.23022531138939112, Accuracy: 91.91%\n",
            "Epoch: 26, VALIDATION: Loss 0.22820099617047718, Accuracy: 91.85%\n",
            "Epoch: 27, VALIDATION: Loss 0.22869676905971054, Accuracy: 91.84%\n",
            "Epoch: 28, VALIDATION: Loss 0.23004945161392623, Accuracy: 91.86%\n",
            "Epoch: 29, VALIDATION: Loss 0.22938605658081493, Accuracy: 91.88%\n",
            "Epoch: 30, VALIDATION: Loss 0.22956364568173862, Accuracy: 91.89%\n",
            "Epoch: 31, VALIDATION: Loss 0.22741087721684838, Accuracy: 91.88%\n",
            "Epoch: 32, VALIDATION: Loss 0.2283530818931264, Accuracy: 91.88%\n",
            "Epoch: 33, VALIDATION: Loss 0.22938993810079658, Accuracy: 91.79%\n",
            "Epoch: 34, VALIDATION: Loss 0.22767069567330236, Accuracy: 91.88%\n",
            "Epoch: 35, VALIDATION: Loss 0.22928396438529644, Accuracy: 91.85%\n",
            "Epoch: 36, VALIDATION: Loss 0.22767170904258258, Accuracy: 91.87%\n",
            "Epoch: 37, VALIDATION: Loss 0.2288748741547802, Accuracy: 91.85%\n",
            "Epoch: 38, VALIDATION: Loss 0.2274734320639898, Accuracy: 91.85%\n",
            "Epoch: 39, VALIDATION: Loss 0.23031603430258774, Accuracy: 91.75%\n",
            "Epoch: 40, VALIDATION: Loss 0.22882349887176426, Accuracy: 91.82%\n",
            "Epoch: 41, VALIDATION: Loss 0.23274865140189416, Accuracy: 91.59%\n",
            "Epoch: 42, VALIDATION: Loss 0.22998584973174527, Accuracy: 91.73%\n",
            "Epoch: 43, VALIDATION: Loss 0.22938412866153593, Accuracy: 91.69%\n",
            "Epoch: 44, VALIDATION: Loss 0.22877704040588864, Accuracy: 91.87%\n",
            "Epoch: 45, VALIDATION: Loss 0.2286225405660657, Accuracy: 91.74%\n",
            "Epoch: 46, VALIDATION: Loss 0.22717608755361507, Accuracy: 91.90%\n",
            "Epoch: 47, VALIDATION: Loss 0.22813860250938914, Accuracy: 91.90%\n",
            "Epoch: 48, VALIDATION: Loss 0.22895203852695808, Accuracy: 91.80%\n",
            "Epoch: 49, VALIDATION: Loss 0.22799160022091244, Accuracy: 91.79%\n",
            "Epoch: 50, VALIDATION: Loss 0.22616353001866016, Accuracy: 91.87%\n",
            "Epoch: 51, VALIDATION: Loss 0.22817531454249346, Accuracy: 91.85%\n",
            "Epoch: 52, VALIDATION: Loss 0.2266478299471501, Accuracy: 91.88%\n",
            "Epoch: 53, VALIDATION: Loss 0.2281462267817258, Accuracy: 91.87%\n",
            "Epoch: 54, VALIDATION: Loss 0.22933689407884522, Accuracy: 91.74%\n",
            "Epoch: 55, VALIDATION: Loss 0.2269749714538834, Accuracy: 91.81%\n",
            "Epoch: 56, VALIDATION: Loss 0.2272327691047588, Accuracy: 91.92%\n",
            "Epoch: 57, VALIDATION: Loss 0.2268596392415813, Accuracy: 91.89%\n",
            "Epoch: 58, VALIDATION: Loss 0.2275993078399238, Accuracy: 91.82%\n",
            "Epoch: 59, VALIDATION: Loss 0.22814144325115954, Accuracy: 91.76%\n",
            "Epoch: 60, VALIDATION: Loss 0.2272471524509607, Accuracy: 91.86%\n",
            "Epoch: 61, VALIDATION: Loss 0.22763727750147505, Accuracy: 91.86%\n",
            "Epoch: 62, VALIDATION: Loss 0.22934885205741998, Accuracy: 91.81%\n",
            "Epoch: 63, VALIDATION: Loss 0.22628804359201243, Accuracy: 91.90%\n",
            "Epoch: 64, VALIDATION: Loss 0.22789668808627644, Accuracy: 91.80%\n",
            "Epoch: 65, VALIDATION: Loss 0.22658013526239437, Accuracy: 91.87%\n",
            "Epoch: 66, VALIDATION: Loss 0.2267445145565313, Accuracy: 91.87%\n",
            "Epoch: 67, VALIDATION: Loss 0.22634444461202616, Accuracy: 91.85%\n",
            "Epoch: 68, VALIDATION: Loss 0.2265315601738091, Accuracy: 91.88%\n",
            "Epoch: 69, VALIDATION: Loss 0.2266158436950118, Accuracy: 91.89%\n",
            "Epoch: 70, VALIDATION: Loss 0.22744795675923948, Accuracy: 91.83%\n",
            "Epoch: 71, VALIDATION: Loss 0.22665595491099866, Accuracy: 91.86%\n",
            "Epoch: 72, VALIDATION: Loss 0.22841714076615055, Accuracy: 91.82%\n",
            "Epoch: 73, VALIDATION: Loss 0.22747355449693668, Accuracy: 91.86%\n",
            "Epoch: 74, VALIDATION: Loss 0.22742739528843575, Accuracy: 91.80%\n",
            "Epoch: 75, VALIDATION: Loss 0.22616253354858307, Accuracy: 91.92%\n",
            "Epoch: 76, VALIDATION: Loss 0.22653739860031363, Accuracy: 91.88%\n",
            "Epoch: 77, VALIDATION: Loss 0.22660578033070036, Accuracy: 91.82%\n",
            "Epoch: 78, VALIDATION: Loss 0.2288018471104247, Accuracy: 91.71%\n",
            "Epoch: 79, VALIDATION: Loss 0.2265082300815595, Accuracy: 91.81%\n",
            "Epoch: 80, VALIDATION: Loss 0.22617462132537658, Accuracy: 91.81%\n",
            "REMINDER: Epoch: 80, TRAIN: Loss 0.22281194324764814, Accuracy: 91.91%\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "mp = multilayer_perceptron([train_X5.shape[1]] + hidden_layers_topology + [2],\n",
        "                            epochs = epochs, lr = lr, batch_size = batch_size)\n",
        "mp.train(train_X5, np.array(train_y_ohe), val_X5, np.array(val_y_ohe))\n",
        "mp.show(val_show = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "TfqoaooG1OPQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "acc val =  0.9180860947661815\n",
            "roc_auc =  0.8255813860922026\n"
          ]
        }
      ],
      "source": [
        "p_predicted = mp.feedforward(val_X5.T)\n",
        "acc = mp.compute_accuracy(p_predicted.T, np.array(val_y_ohe))\n",
        "print(\"acc val = \", acc)\n",
        "roc_auc = roc_auc_score(np.array(val_y_ohe)[:, 0], p_predicted.T[:, 0])\n",
        "print(\"roc_auc = \", roc_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "CGVAz3B91Vzx"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/9j/mrbb80v92lxdv_3kldctnqq00000gn/T/ipykernel_19384/3202111084.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df_metrics = df_metrics.append(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>hyperparams</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>roc_auc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Naive classifier</td>\n",
              "      <td>baseline</td>\n",
              "      <td>0.919606</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Rand_forest</td>\n",
              "      <td>{'n_est': 1600, 'min_samp_splt': 2, 'min_samp_...</td>\n",
              "      <td>0.920211</td>\n",
              "      <td>0.507280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>MLP</td>\n",
              "      <td>rand_st=21, layers = [20], act_f = relu</td>\n",
              "      <td>0.917691</td>\n",
              "      <td>0.82772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Numpy</td>\n",
              "      <td>layers=[25] epochs = 80 batch_size = 1000 lr =...</td>\n",
              "      <td>0.918086</td>\n",
              "      <td>0.825581</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              model                                        hyperparams  \\\n",
              "0  Naive classifier                                           baseline   \n",
              "0       Rand_forest  {'n_est': 1600, 'min_samp_splt': 2, 'min_samp_...   \n",
              "0               MLP            rand_st=21, layers = [20], act_f = relu   \n",
              "0             Numpy  layers=[25] epochs = 80 batch_size = 1000 lr =...   \n",
              "\n",
              "   accuracy   roc_auc  \n",
              "0  0.919606       0.5  \n",
              "0  0.920211  0.507280  \n",
              "0  0.917691   0.82772  \n",
              "0  0.918086  0.825581  "
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_metrics = df_metrics.append(\n",
        "    pd.DataFrame({'model' : [\"Numpy\"],\n",
        "                                \"hyperparams\" : [\"layers=[25] epochs = 80 batch_size = 1000 lr = 0.0001 accumulation_rate = 0.9\"],\n",
        "                                'accuracy' : \"0.918086\", \n",
        "                                'roc_auc' : \"0.825581\"})\n",
        ")\n",
        "df_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGQTheWcQBhP"
      },
      "source": [
        "## Output prediction for test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "i1wfbbzu4b4C"
      },
      "outputs": [],
      "source": [
        "test_pred = mp.feedforward(test_X5.T).T[:,0]\n",
        "\n",
        "df = pd.concat([pd.DataFrame({\"ID\" : test_indices.values}), pd.DataFrame({\"TARGET\" : test_pred})], axis = 1)\n",
        "df.to_csv(\"data/test_predictions.csv\", index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMGNin8Vw-O_"
      },
      "source": [
        "# TesorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "dDrbgzUo0WHg"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "# Set random seed for reproducible results \n",
        "tf.random.set_seed(21)\n",
        "\n",
        "train_y_ohe = pd.concat([train_y, (1+train_y)%2], axis = 1)\n",
        "val_y_ohe = pd.concat([val_y, (1+val_y)%2], axis = 1)\n",
        "\n",
        "train_y_ohe.columns = ['1', '0']\n",
        "val_y_ohe.columns = ['1', '0']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "4QBAiFlzAgpG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH: 0 train_loss: 0.3487298 val_rocauc: 0.5637772816771943\n",
            "EPOCH: 1 train_loss: 0.27793446 val_rocauc: 0.6301159820960456\n",
            "EPOCH: 2 train_loss: 0.27027005 val_rocauc: 0.6669543519217949\n",
            "EPOCH: 3 train_loss: 0.26648754 val_rocauc: 0.6849732433306835\n",
            "EPOCH: 4 train_loss: 0.26386753 val_rocauc: 0.6971391825974618\n",
            "EPOCH: 5 train_loss: 0.26179105 val_rocauc: 0.7063490692247618\n",
            "EPOCH: 6 train_loss: 0.26005757 val_rocauc: 0.7134601640130012\n",
            "EPOCH: 7 train_loss: 0.25853664 val_rocauc: 0.7201760258402043\n",
            "EPOCH: 8 train_loss: 0.25720885 val_rocauc: 0.7256432469411718\n",
            "EPOCH: 9 train_loss: 0.25597304 val_rocauc: 0.730547076562533\n",
            "EPOCH: 10 train_loss: 0.25488305 val_rocauc: 0.7350199344784594\n",
            "EPOCH: 11 train_loss: 0.25375488 val_rocauc: 0.7398764689301413\n",
            "EPOCH: 12 train_loss: 0.25276867 val_rocauc: 0.7432454449880023\n",
            "EPOCH: 13 train_loss: 0.2518333 val_rocauc: 0.7466267480781872\n",
            "EPOCH: 14 train_loss: 0.25097278 val_rocauc: 0.7507129247609812\n",
            "EPOCH: 15 train_loss: 0.25000444 val_rocauc: 0.7531772456749692\n",
            "EPOCH: 16 train_loss: 0.24908444 val_rocauc: 0.7564946178646381\n",
            "EPOCH: 17 train_loss: 0.2481847 val_rocauc: 0.759385203750258\n",
            "EPOCH: 18 train_loss: 0.2472108 val_rocauc: 0.7620527906995326\n",
            "EPOCH: 19 train_loss: 0.24640185 val_rocauc: 0.7644299633279232\n",
            "EPOCH: 20 train_loss: 0.24563558 val_rocauc: 0.7678551508140035\n",
            "EPOCH: 21 train_loss: 0.24481355 val_rocauc: 0.7686211422563733\n",
            "EPOCH: 22 train_loss: 0.2442027 val_rocauc: 0.7714388407773523\n",
            "EPOCH: 23 train_loss: 0.24344987 val_rocauc: 0.7728434537093816\n",
            "EPOCH: 24 train_loss: 0.24293685 val_rocauc: 0.7740825121045548\n",
            "EPOCH: 25 train_loss: 0.24261145 val_rocauc: 0.7762045199885107\n",
            "EPOCH: 26 train_loss: 0.24197948 val_rocauc: 0.7769760249567537\n",
            "EPOCH: 27 train_loss: 0.24163911 val_rocauc: 0.7786330632927013\n",
            "EPOCH: 28 train_loss: 0.24120827 val_rocauc: 0.7795531319176305\n",
            "EPOCH: 29 train_loss: 0.24083105 val_rocauc: 0.7803845525886464\n",
            "EPOCH: 30 train_loss: 0.24045163 val_rocauc: 0.7813343224193119\n",
            "EPOCH: 31 train_loss: 0.24012052 val_rocauc: 0.7816964675241788\n",
            "EPOCH: 32 train_loss: 0.23982191 val_rocauc: 0.7821484841729254\n",
            "EPOCH: 33 train_loss: 0.23944598 val_rocauc: 0.782507084485392\n",
            "EPOCH: 34 train_loss: 0.23915406 val_rocauc: 0.7838512804850237\n",
            "EPOCH: 35 train_loss: 0.2386995 val_rocauc: 0.7856621842541168\n",
            "EPOCH: 36 train_loss: 0.23842722 val_rocauc: 0.7858281850729654\n",
            "EPOCH: 37 train_loss: 0.23801906 val_rocauc: 0.786726873708042\n",
            "EPOCH: 38 train_loss: 0.23779604 val_rocauc: 0.786673902849964\n",
            "EPOCH: 39 train_loss: 0.23735712 val_rocauc: 0.78739399425645\n",
            "EPOCH: 40 train_loss: 0.23684792 val_rocauc: 0.7889125189917446\n",
            "EPOCH: 41 train_loss: 0.23700164 val_rocauc: 0.7891278051567239\n",
            "EPOCH: 42 train_loss: 0.23606434 val_rocauc: 0.7909427670095175\n",
            "EPOCH: 43 train_loss: 0.23568338 val_rocauc: 0.7919890771504082\n",
            "EPOCH: 44 train_loss: 0.23542675 val_rocauc: 0.7920141453325804\n",
            "EPOCH: 45 train_loss: 0.23510645 val_rocauc: 0.7930835080156549\n",
            "EPOCH: 46 train_loss: 0.23459287 val_rocauc: 0.7949503002281021\n",
            "EPOCH: 47 train_loss: 0.23424682 val_rocauc: 0.7952624791722673\n",
            "EPOCH: 48 train_loss: 0.23400939 val_rocauc: 0.796851533884743\n",
            "EPOCH: 49 train_loss: 0.23362012 val_rocauc: 0.7969077386120315\n",
            "EPOCH: 50 train_loss: 0.23337743 val_rocauc: 0.7976666176864362\n",
            "EPOCH: 51 train_loss: 0.2331541 val_rocauc: 0.7973070591408633\n",
            "EPOCH: 52 train_loss: 0.23276682 val_rocauc: 0.7995722525144352\n",
            "EPOCH: 53 train_loss: 0.23249094 val_rocauc: 0.7996583380324058\n",
            "EPOCH: 54 train_loss: 0.23231654 val_rocauc: 0.8009617548474808\n",
            "EPOCH: 55 train_loss: 0.23225348 val_rocauc: 0.8006503505309176\n",
            "EPOCH: 56 train_loss: 0.2322077 val_rocauc: 0.7999675404231038\n",
            "EPOCH: 57 train_loss: 0.23180683 val_rocauc: 0.8015192883733702\n",
            "EPOCH: 58 train_loss: 0.23153153 val_rocauc: 0.8018608669813851\n",
            "EPOCH: 59 train_loss: 0.2313548 val_rocauc: 0.8018545587251176\n",
            "EPOCH: 60 train_loss: 0.2311095 val_rocauc: 0.8028437648738048\n",
            "EPOCH: 61 train_loss: 0.23108351 val_rocauc: 0.8028162830163539\n",
            "EPOCH: 62 train_loss: 0.2308748 val_rocauc: 0.8041474966326281\n",
            "EPOCH: 63 train_loss: 0.2305867 val_rocauc: 0.804010001573459\n",
            "EPOCH: 64 train_loss: 0.23052989 val_rocauc: 0.8048894504959851\n",
            "EPOCH: 65 train_loss: 0.23050189 val_rocauc: 0.804625677735828\n",
            "EPOCH: 66 train_loss: 0.23012099 val_rocauc: 0.8048708272690692\n",
            "EPOCH: 67 train_loss: 0.23013861 val_rocauc: 0.8055657714223995\n",
            "EPOCH: 68 train_loss: 0.23014529 val_rocauc: 0.8059100706083244\n",
            "EPOCH: 69 train_loss: 0.22972895 val_rocauc: 0.8055273469449306\n",
            "EPOCH: 70 train_loss: 0.22969316 val_rocauc: 0.8061022090779026\n",
            "EPOCH: 71 train_loss: 0.22948499 val_rocauc: 0.8061692063243588\n",
            "EPOCH: 72 train_loss: 0.22946233 val_rocauc: 0.8071025146483889\n",
            "EPOCH: 73 train_loss: 0.22929846 val_rocauc: 0.8059008380658501\n",
            "EPOCH: 74 train_loss: 0.2293247 val_rocauc: 0.8064073332821078\n",
            "EPOCH: 75 train_loss: 0.22891766 val_rocauc: 0.8072613079460547\n",
            "EPOCH: 76 train_loss: 0.2290082 val_rocauc: 0.8077178396819833\n",
            "EPOCH: 77 train_loss: 0.22900109 val_rocauc: 0.8074282777194985\n",
            "EPOCH: 78 train_loss: 0.22863021 val_rocauc: 0.808300575408661\n",
            "EPOCH: 79 train_loss: 0.2285476 val_rocauc: 0.80856606226692\n",
            "EPOCH: 80 train_loss: 0.22847849 val_rocauc: 0.8076507700654743\n",
            "EPOCH: 81 train_loss: 0.22834794 val_rocauc: 0.8081160022899706\n",
            "EPOCH: 82 train_loss: 0.22827584 val_rocauc: 0.8080538484763063\n",
            "EPOCH: 83 train_loss: 0.22828569 val_rocauc: 0.8093044297921057\n",
            "EPOCH: 84 train_loss: 0.22807749 val_rocauc: 0.8091069868656957\n",
            "EPOCH: 85 train_loss: 0.22795138 val_rocauc: 0.8090118363285212\n",
            "EPOCH: 86 train_loss: 0.22787593 val_rocauc: 0.809325568548449\n",
            "EPOCH: 87 train_loss: 0.2277867 val_rocauc: 0.809335366649484\n",
            "EPOCH: 88 train_loss: 0.22772638 val_rocauc: 0.8101341189425995\n",
            "EPOCH: 89 train_loss: 0.22797196 val_rocauc: 0.810502483851447\n",
            "EPOCH: 90 train_loss: 0.22770475 val_rocauc: 0.8096053123071062\n",
            "EPOCH: 91 train_loss: 0.22749633 val_rocauc: 0.810065702438997\n",
            "EPOCH: 92 train_loss: 0.22747064 val_rocauc: 0.810708532113207\n",
            "EPOCH: 93 train_loss: 0.22733645 val_rocauc: 0.8099587354804265\n",
            "EPOCH: 94 train_loss: 0.22730784 val_rocauc: 0.8099819488449496\n",
            "EPOCH: 95 train_loss: 0.22730651 val_rocauc: 0.8108524547052123\n",
            "EPOCH: 96 train_loss: 0.22705404 val_rocauc: 0.8110887951948065\n",
            "EPOCH: 97 train_loss: 0.22786374 val_rocauc: 0.8109409002910153\n",
            "EPOCH: 98 train_loss: 0.22696926 val_rocauc: 0.8106493186679834\n",
            "EPOCH: 99 train_loss: 0.2268636 val_rocauc: 0.8107298611759856\n",
            "EPOCH: 100 train_loss: 0.22680058 val_rocauc: 0.8118636720714346\n",
            "EPOCH: 101 train_loss: 0.22674362 val_rocauc: 0.8120302451497745\n",
            "EPOCH: 102 train_loss: 0.22676244 val_rocauc: 0.8110438466910979\n",
            "EPOCH: 103 train_loss: 0.22679928 val_rocauc: 0.8115563030554072\n",
            "EPOCH: 104 train_loss: 0.2266505 val_rocauc: 0.8119084423303835\n",
            "EPOCH: 105 train_loss: 0.22652769 val_rocauc: 0.8130317381252008\n",
            "EPOCH: 106 train_loss: 0.22639778 val_rocauc: 0.8120334776787987\n",
            "EPOCH: 107 train_loss: 0.22667795 val_rocauc: 0.811840807155314\n",
            "EPOCH: 108 train_loss: 0.22632198 val_rocauc: 0.8130755608725326\n",
            "EPOCH: 109 train_loss: 0.22618878 val_rocauc: 0.8134328973894479\n",
            "EPOCH: 110 train_loss: 0.22603959 val_rocauc: 0.8132213020971606\n",
            "EPOCH: 111 train_loss: 0.2261658 val_rocauc: 0.812776167304359\n",
            "EPOCH: 112 train_loss: 0.2263938 val_rocauc: 0.8137198433085431\n",
            "EPOCH: 113 train_loss: 0.22605287 val_rocauc: 0.8131294993450202\n",
            "EPOCH: 114 train_loss: 0.22631426 val_rocauc: 0.8145619332015996\n",
            "EPOCH: 115 train_loss: 0.22575074 val_rocauc: 0.813906046093607\n",
            "EPOCH: 116 train_loss: 0.2259211 val_rocauc: 0.8145181171551985\n",
            "EPOCH: 117 train_loss: 0.22565672 val_rocauc: 0.8149542123923331\n",
            "EPOCH: 118 train_loss: 0.22563018 val_rocauc: 0.8142836582869767\n",
            "EPOCH: 119 train_loss: 0.2257267 val_rocauc: 0.8145911814244129\n",
            "EPOCH: 120 train_loss: 0.22563176 val_rocauc: 0.8152698034504684\n",
            "EPOCH: 121 train_loss: 0.2254679 val_rocauc: 0.815464433326123\n",
            "EPOCH: 122 train_loss: 0.22582646 val_rocauc: 0.8148639991009817\n",
            "EPOCH: 123 train_loss: 0.22530267 val_rocauc: 0.8142797730872913\n",
            "EPOCH: 124 train_loss: 0.22543749 val_rocauc: 0.8150788135204317\n",
            "EPOCH: 125 train_loss: 0.22538882 val_rocauc: 0.8149436249216484\n",
            "EPOCH: 126 train_loss: 0.22511183 val_rocauc: 0.8149511500669515\n",
            "EPOCH: 127 train_loss: 0.22534738 val_rocauc: 0.8147973248393907\n",
            "EPOCH: 128 train_loss: 0.22516973 val_rocauc: 0.8161450227454653\n",
            "EPOCH: 129 train_loss: 0.22514895 val_rocauc: 0.8148761679913386\n",
            "EPOCH: 130 train_loss: 0.2251948 val_rocauc: 0.8159539108584877\n",
            "EPOCH: 131 train_loss: 0.22503789 val_rocauc: 0.816348463004953\n",
            "EPOCH: 132 train_loss: 0.22507577 val_rocauc: 0.8156458918521713\n",
            "EPOCH: 133 train_loss: 0.225307 val_rocauc: 0.8152709345675897\n",
            "EPOCH: 134 train_loss: 0.22501734 val_rocauc: 0.8161640721515809\n",
            "EPOCH: 135 train_loss: 0.22479108 val_rocauc: 0.8166782613765986\n",
            "EPOCH: 136 train_loss: 0.22480328 val_rocauc: 0.8159701391727311\n",
            "EPOCH: 137 train_loss: 0.22480367 val_rocauc: 0.816028959943409\n",
            "EPOCH: 138 train_loss: 0.22458553 val_rocauc: 0.8165787605951398\n",
            "EPOCH: 139 train_loss: 0.22450542 val_rocauc: 0.8166122330847378\n",
            "EPOCH: 140 train_loss: 0.22441043 val_rocauc: 0.8171080738210026\n",
            "EPOCH: 141 train_loss: 0.2244421 val_rocauc: 0.8169913864921946\n",
            "EPOCH: 142 train_loss: 0.22490083 val_rocauc: 0.8179065835404227\n",
            "EPOCH: 143 train_loss: 0.22424386 val_rocauc: 0.8172654987885963\n",
            "EPOCH: 144 train_loss: 0.22446604 val_rocauc: 0.8175940373852488\n",
            "EPOCH: 145 train_loss: 0.22427608 val_rocauc: 0.8179047515059383\n",
            "EPOCH: 146 train_loss: 0.22423989 val_rocauc: 0.8171439050382469\n",
            "EPOCH: 147 train_loss: 0.22446622 val_rocauc: 0.8156995743491018\n",
            "EPOCH: 148 train_loss: 0.22429612 val_rocauc: 0.8170768541843441\n",
            "EPOCH: 149 train_loss: 0.22416301 val_rocauc: 0.8173828830142091\n",
            "EPOCH: 150 train_loss: 0.22405465 val_rocauc: 0.8175919225714843\n",
            "EPOCH: 151 train_loss: 0.2242487 val_rocauc: 0.8173074921818164\n",
            "EPOCH: 152 train_loss: 0.22382669 val_rocauc: 0.8181748834494372\n",
            "EPOCH: 153 train_loss: 0.22413294 val_rocauc: 0.8170014204659941\n",
            "EPOCH: 154 train_loss: 0.22372727 val_rocauc: 0.8182005171901698\n",
            "EPOCH: 155 train_loss: 0.22406344 val_rocauc: 0.8177236628712712\n",
            "EPOCH: 156 train_loss: 0.22365429 val_rocauc: 0.8180033771458327\n",
            "EPOCH: 157 train_loss: 0.22386639 val_rocauc: 0.8175927829710007\n",
            "EPOCH: 158 train_loss: 0.22400457 val_rocauc: 0.8171413372415591\n",
            "EPOCH: 159 train_loss: 0.22360227 val_rocauc: 0.8185795580219889\n",
            "EPOCH: 160 train_loss: 0.22348079 val_rocauc: 0.8193186881131012\n",
            "EPOCH: 161 train_loss: 0.22344841 val_rocauc: 0.8192340379345109\n",
            "EPOCH: 162 train_loss: 0.22351122 val_rocauc: 0.8181427069198586\n",
            "EPOCH: 163 train_loss: 0.22356988 val_rocauc: 0.8185222597027916\n",
            "EPOCH: 164 train_loss: 0.22351725 val_rocauc: 0.8193632948693383\n",
            "EPOCH: 165 train_loss: 0.22343919 val_rocauc: 0.8193871756465703\n",
            "EPOCH: 166 train_loss: 0.22342928 val_rocauc: 0.8197568713602772\n",
            "EPOCH: 167 train_loss: 0.22312367 val_rocauc: 0.8200902064832057\n",
            "EPOCH: 168 train_loss: 0.22319612 val_rocauc: 0.8189948173848531\n",
            "EPOCH: 169 train_loss: 0.22334264 val_rocauc: 0.8205700172527793\n",
            "EPOCH: 170 train_loss: 0.22318864 val_rocauc: 0.818985653191873\n",
            "EPOCH: 171 train_loss: 0.22309022 val_rocauc: 0.8195709071283501\n",
            "EPOCH: 172 train_loss: 0.22308648 val_rocauc: 0.820196314382446\n",
            "EPOCH: 173 train_loss: 0.22295772 val_rocauc: 0.820026295685482\n",
            "EPOCH: 174 train_loss: 0.22301252 val_rocauc: 0.8192809913567185\n",
            "EPOCH: 175 train_loss: 0.22281082 val_rocauc: 0.8195961267515587\n",
            "EPOCH: 176 train_loss: 0.22284253 val_rocauc: 0.8200393960052218\n",
            "EPOCH: 177 train_loss: 0.2227262 val_rocauc: 0.8202072234978098\n",
            "EPOCH: 178 train_loss: 0.22310284 val_rocauc: 0.8195450871017409\n",
            "EPOCH: 179 train_loss: 0.2227209 val_rocauc: 0.8194817887690937\n",
            "EPOCH: 180 train_loss: 0.2226683 val_rocauc: 0.820838763443786\n",
            "EPOCH: 181 train_loss: 0.22278701 val_rocauc: 0.8201879730637698\n",
            "EPOCH: 182 train_loss: 0.22250585 val_rocauc: 0.8205019746611164\n",
            "EPOCH: 183 train_loss: 0.22262865 val_rocauc: 0.821189504904595\n",
            "EPOCH: 184 train_loss: 0.22243781 val_rocauc: 0.8205851881601403\n",
            "EPOCH: 185 train_loss: 0.22251989 val_rocauc: 0.820930077027977\n",
            "EPOCH: 186 train_loss: 0.2225552 val_rocauc: 0.8201714592898739\n",
            "EPOCH: 187 train_loss: 0.22252299 val_rocauc: 0.820331178656178\n",
            "EPOCH: 188 train_loss: 0.22238365 val_rocauc: 0.8206735412730981\n",
            "EPOCH: 189 train_loss: 0.22268668 val_rocauc: 0.8190751816480955\n",
            "EPOCH: 190 train_loss: 0.22260965 val_rocauc: 0.8210114839560538\n",
            "EPOCH: 191 train_loss: 0.22249933 val_rocauc: 0.8198302264498889\n",
            "EPOCH: 192 train_loss: 0.22236393 val_rocauc: 0.8204378052074632\n",
            "EPOCH: 193 train_loss: 0.22257644 val_rocauc: 0.8206754698009862\n",
            "EPOCH: 194 train_loss: 0.22223608 val_rocauc: 0.8208736324073653\n",
            "EPOCH: 195 train_loss: 0.22224048 val_rocauc: 0.8204007343180186\n",
            "EPOCH: 196 train_loss: 0.22210734 val_rocauc: 0.8211616544959491\n",
            "EPOCH: 197 train_loss: 0.22206372 val_rocauc: 0.8210568385361696\n",
            "EPOCH: 198 train_loss: 0.22212848 val_rocauc: 0.819677183891047\n",
            "EPOCH: 199 train_loss: 0.22196636 val_rocauc: 0.8207533949254124\n"
          ]
        }
      ],
      "source": [
        "class Dense(tf.Module):\n",
        "    def __init__ (self, output_size, activate = \"relu\"):\n",
        "        super().__init__()\n",
        "        self.output_size = output_size\n",
        "        self.activate = activate\n",
        "        self.init = False\n",
        "    \n",
        "    def __call__(self, x):\n",
        "        if not self.init:\n",
        "            self.w = tf.Variable(\n",
        "                        tf.random.truncated_normal((x.shape[-1], self.output_size), stddev = 0.1),\n",
        "                        name = \"w\", \n",
        "                        dtype = tf.float32\n",
        "                    )\n",
        "            self.b = tf.Variable(\n",
        "                        tf.zeros([self.output_size]),\n",
        "                        name = 'b', \n",
        "                        dtype = tf.float32\n",
        "                    )\n",
        "            self.init = True\n",
        "        y = x @ self.w + self.b\n",
        "        if self.activate == 'relu':\n",
        "            return tf.nn.relu(y)\n",
        "        elif self.activate == 'softmax':\n",
        "            return tf.nn.softmax(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "\n",
        "loss_function = lambda y_true, y_pred: tf.reduce_mean(tf.losses.categorical_crossentropy(y_true, y_pred))\n",
        "opt = tf.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "class mlp(tf.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer_1 = Dense(25)\n",
        "        self.layer_2 = Dense(2, activate=\"softmax\")\n",
        " \n",
        "    def __call__(self, x):\n",
        "        return self.layer_2(self.layer_1(x))\n",
        "\n",
        "model = mlp()\n",
        "\n",
        "BATCH_SIZE = 5000\n",
        "EPOCHS = 200\n",
        "train_x_tf = tf.constant(np.array(train_X5), dtype = tf.float32)\n",
        "train_y_tf = tf.constant(train_y_ohe, dtype = tf.float32)\n",
        "\n",
        "val_x_tf = tf.constant(np.array(val_X5), dtype = tf.float32)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_x_tf, train_y_tf))\n",
        "train_dataset = train_dataset.shuffle(buffer_size = train_X5.shape[0], seed = 21).batch(BATCH_SIZE)\n",
        "\n",
        "for e in range(EPOCHS):\n",
        "    for x, y, in train_dataset:\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = loss_function(y, model(x))\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        opt.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    e_loss = loss_function(train_y_tf,\n",
        "                           model(train_x_tf)).numpy()\n",
        "    val_rocauc = roc_auc_score(val_y,  model(val_x_tf)[:, 0])\n",
        "    print(\"EPOCH:\", e, \"train_loss:\", e_loss, \"val_rocauc:\",  val_rocauc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "acc = 0.9190433289225485\n",
            "val_rocauc =  0.8207533949254124\n"
          ]
        }
      ],
      "source": [
        "pred = np.where(model(val_x_tf).numpy()[:, 0] > 0.5 , 1, 0)\n",
        "acc = accuracy_score(val_y, pred)\n",
        "print(\"acc =\", acc)\n",
        "val_rocauc = roc_auc_score(val_y,  model(val_x_tf)[:, 0])\n",
        "print(\"val_rocauc = \", val_rocauc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/9j/mrbb80v92lxdv_3kldctnqq00000gn/T/ipykernel_19384/168561520.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df_metrics = df_metrics.append(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>hyperparams</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>roc_auc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Naive classifier</td>\n",
              "      <td>baseline</td>\n",
              "      <td>0.919606</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Rand_forest</td>\n",
              "      <td>{'n_est': 1600, 'min_samp_splt': 2, 'min_samp_...</td>\n",
              "      <td>0.920211</td>\n",
              "      <td>0.507280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>MLP</td>\n",
              "      <td>rand_st=21, layers = [20], act_f = relu</td>\n",
              "      <td>0.917691</td>\n",
              "      <td>0.82772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Numpy</td>\n",
              "      <td>layers=[25] epochs = 80 batch_size = 1000 lr =...</td>\n",
              "      <td>0.918086</td>\n",
              "      <td>0.825581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TensorFlow</td>\n",
              "      <td>layers=[25] epochs = 200 batch_size = 5000</td>\n",
              "      <td>0.9190433</td>\n",
              "      <td>0.8207533</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              model                                        hyperparams  \\\n",
              "0  Naive classifier                                           baseline   \n",
              "0       Rand_forest  {'n_est': 1600, 'min_samp_splt': 2, 'min_samp_...   \n",
              "0               MLP            rand_st=21, layers = [20], act_f = relu   \n",
              "0             Numpy  layers=[25] epochs = 80 batch_size = 1000 lr =...   \n",
              "0        TensorFlow         layers=[25] epochs = 200 batch_size = 5000   \n",
              "\n",
              "    accuracy    roc_auc  \n",
              "0   0.919606        0.5  \n",
              "0   0.920211   0.507280  \n",
              "0   0.917691    0.82772  \n",
              "0   0.918086   0.825581  \n",
              "0  0.9190433  0.8207533  "
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_metrics = df_metrics.append(\n",
        "    pd.DataFrame({'model' : [\"TensorFlow\"],\n",
        "                                \"hyperparams\" : [\"layers=[25] epochs = 200 batch_size = 5000\"],\n",
        "                                'accuracy' : \"0.9190433\", \n",
        "                                'roc_auc' : \"0.8207533\"})\n",
        ")\n",
        "df_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xN0azxm__E4"
      },
      "source": [
        "#  Keras from the TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "-2aQjG2bAnyO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/350\n",
            "29/29 [==============================] - 1s 14ms/step - loss: 0.1638 - auroc: 0.5158 - val_loss: 0.3653 - val_auroc: 0.5752\n",
            "Epoch 2/350\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 0.1473 - auroc: 0.6102 - val_loss: 0.3315 - val_auroc: 0.6554\n",
            "Epoch 3/350\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 0.1416 - auroc: 0.6639 - val_loss: 0.3195 - val_auroc: 0.6871\n",
            "Epoch 4/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1389 - auroc: 0.6873 - val_loss: 0.3126 - val_auroc: 0.7049\n",
            "Epoch 5/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1371 - auroc: 0.7019 - val_loss: 0.3081 - val_auroc: 0.7173\n",
            "Epoch 6/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1356 - auroc: 0.7134 - val_loss: 0.3028 - val_auroc: 0.7273\n",
            "Epoch 7/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1344 - auroc: 0.7226 - val_loss: 0.2999 - val_auroc: 0.7351\n",
            "Epoch 8/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1333 - auroc: 0.7295 - val_loss: 0.2953 - val_auroc: 0.7422\n",
            "Epoch 9/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1323 - auroc: 0.7360 - val_loss: 0.2918 - val_auroc: 0.7479\n",
            "Epoch 10/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1315 - auroc: 0.7417 - val_loss: 0.2882 - val_auroc: 0.7524\n",
            "Epoch 11/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1309 - auroc: 0.7454 - val_loss: 0.2860 - val_auroc: 0.7568\n",
            "Epoch 12/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1301 - auroc: 0.7508 - val_loss: 0.2843 - val_auroc: 0.7601\n",
            "Epoch 13/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1296 - auroc: 0.7543 - val_loss: 0.2820 - val_auroc: 0.7626\n",
            "Epoch 14/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1293 - auroc: 0.7563 - val_loss: 0.2809 - val_auroc: 0.7651\n",
            "Epoch 15/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1289 - auroc: 0.7589 - val_loss: 0.2794 - val_auroc: 0.7673\n",
            "Epoch 16/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1283 - auroc: 0.7622 - val_loss: 0.2782 - val_auroc: 0.7691\n",
            "Epoch 17/350\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 0.1279 - auroc: 0.7636 - val_loss: 0.2770 - val_auroc: 0.7714\n",
            "Epoch 18/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1279 - auroc: 0.7646 - val_loss: 0.2764 - val_auroc: 0.7727\n",
            "Epoch 19/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1274 - auroc: 0.7672 - val_loss: 0.2754 - val_auroc: 0.7741\n",
            "Epoch 20/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1271 - auroc: 0.7688 - val_loss: 0.2742 - val_auroc: 0.7752\n",
            "Epoch 21/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1268 - auroc: 0.7700 - val_loss: 0.2734 - val_auroc: 0.7766\n",
            "Epoch 22/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1266 - auroc: 0.7711 - val_loss: 0.2733 - val_auroc: 0.7781\n",
            "Epoch 23/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1264 - auroc: 0.7727 - val_loss: 0.2721 - val_auroc: 0.7792\n",
            "Epoch 24/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1262 - auroc: 0.7733 - val_loss: 0.2717 - val_auroc: 0.7804\n",
            "Epoch 25/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1259 - auroc: 0.7749 - val_loss: 0.2713 - val_auroc: 0.7814\n",
            "Epoch 26/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1256 - auroc: 0.7763 - val_loss: 0.2703 - val_auroc: 0.7824\n",
            "Epoch 27/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1254 - auroc: 0.7772 - val_loss: 0.2698 - val_auroc: 0.7833\n",
            "Epoch 28/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1251 - auroc: 0.7782 - val_loss: 0.2690 - val_auroc: 0.7844\n",
            "Epoch 29/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1250 - auroc: 0.7792 - val_loss: 0.2684 - val_auroc: 0.7853\n",
            "Epoch 30/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1247 - auroc: 0.7804 - val_loss: 0.2676 - val_auroc: 0.7864\n",
            "Epoch 31/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1244 - auroc: 0.7815 - val_loss: 0.2679 - val_auroc: 0.7872\n",
            "Epoch 32/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1243 - auroc: 0.7825 - val_loss: 0.2669 - val_auroc: 0.7881\n",
            "Epoch 33/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1240 - auroc: 0.7834 - val_loss: 0.2660 - val_auroc: 0.7887\n",
            "Epoch 34/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1237 - auroc: 0.7849 - val_loss: 0.2656 - val_auroc: 0.7900\n",
            "Epoch 35/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1235 - auroc: 0.7852 - val_loss: 0.2653 - val_auroc: 0.7909\n",
            "Epoch 36/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1233 - auroc: 0.7862 - val_loss: 0.2642 - val_auroc: 0.7919\n",
            "Epoch 37/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1231 - auroc: 0.7872 - val_loss: 0.2640 - val_auroc: 0.7929\n",
            "Epoch 38/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1229 - auroc: 0.7877 - val_loss: 0.2631 - val_auroc: 0.7941\n",
            "Epoch 39/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1228 - auroc: 0.7882 - val_loss: 0.2631 - val_auroc: 0.7952\n",
            "Epoch 40/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1226 - auroc: 0.7893 - val_loss: 0.2622 - val_auroc: 0.7957\n",
            "Epoch 41/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1223 - auroc: 0.7908 - val_loss: 0.2621 - val_auroc: 0.7966\n",
            "Epoch 42/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1221 - auroc: 0.7918 - val_loss: 0.2612 - val_auroc: 0.7971\n",
            "Epoch 43/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1219 - auroc: 0.7929 - val_loss: 0.2612 - val_auroc: 0.7985\n",
            "Epoch 44/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1217 - auroc: 0.7934 - val_loss: 0.2610 - val_auroc: 0.7989\n",
            "Epoch 45/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1213 - auroc: 0.7955 - val_loss: 0.2610 - val_auroc: 0.7999\n",
            "Epoch 46/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1213 - auroc: 0.7953 - val_loss: 0.2597 - val_auroc: 0.8003\n",
            "Epoch 47/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1211 - auroc: 0.7962 - val_loss: 0.2602 - val_auroc: 0.8007\n",
            "Epoch 48/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1209 - auroc: 0.7972 - val_loss: 0.2598 - val_auroc: 0.8017\n",
            "Epoch 49/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1207 - auroc: 0.7977 - val_loss: 0.2597 - val_auroc: 0.8022\n",
            "Epoch 50/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1207 - auroc: 0.7981 - val_loss: 0.2594 - val_auroc: 0.8028\n",
            "Epoch 51/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1205 - auroc: 0.7987 - val_loss: 0.2604 - val_auroc: 0.8035\n",
            "Epoch 52/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1203 - auroc: 0.7995 - val_loss: 0.2592 - val_auroc: 0.8039\n",
            "Epoch 53/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1203 - auroc: 0.8000 - val_loss: 0.2592 - val_auroc: 0.8047\n",
            "Epoch 54/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1199 - auroc: 0.8014 - val_loss: 0.2588 - val_auroc: 0.8049\n",
            "Epoch 55/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1200 - auroc: 0.8011 - val_loss: 0.2593 - val_auroc: 0.8061\n",
            "Epoch 56/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1199 - auroc: 0.8014 - val_loss: 0.2588 - val_auroc: 0.8065\n",
            "Epoch 57/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1199 - auroc: 0.8016 - val_loss: 0.2583 - val_auroc: 0.8070\n",
            "Epoch 58/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1196 - auroc: 0.8030 - val_loss: 0.2588 - val_auroc: 0.8073\n",
            "Epoch 59/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1196 - auroc: 0.8027 - val_loss: 0.2583 - val_auroc: 0.8080\n",
            "Epoch 60/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1195 - auroc: 0.8033 - val_loss: 0.2587 - val_auroc: 0.8084\n",
            "Epoch 61/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1191 - auroc: 0.8048 - val_loss: 0.2588 - val_auroc: 0.8092\n",
            "Epoch 62/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1192 - auroc: 0.8048 - val_loss: 0.2582 - val_auroc: 0.8095\n",
            "Epoch 63/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1190 - auroc: 0.8055 - val_loss: 0.2573 - val_auroc: 0.8100\n",
            "Epoch 64/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1189 - auroc: 0.8058 - val_loss: 0.2568 - val_auroc: 0.8104\n",
            "Epoch 65/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1187 - auroc: 0.8072 - val_loss: 0.2556 - val_auroc: 0.8112\n",
            "Epoch 66/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1186 - auroc: 0.8072 - val_loss: 0.2560 - val_auroc: 0.8113\n",
            "Epoch 67/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1185 - auroc: 0.8075 - val_loss: 0.2568 - val_auroc: 0.8116\n",
            "Epoch 68/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1184 - auroc: 0.8083 - val_loss: 0.2561 - val_auroc: 0.8123\n",
            "Epoch 69/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1182 - auroc: 0.8090 - val_loss: 0.2560 - val_auroc: 0.8125\n",
            "Epoch 70/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1184 - auroc: 0.8085 - val_loss: 0.2564 - val_auroc: 0.8131\n",
            "Epoch 71/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1182 - auroc: 0.8092 - val_loss: 0.2563 - val_auroc: 0.8136\n",
            "Epoch 72/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1178 - auroc: 0.8107 - val_loss: 0.2564 - val_auroc: 0.8142\n",
            "Epoch 73/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1179 - auroc: 0.8107 - val_loss: 0.2563 - val_auroc: 0.8146\n",
            "Epoch 74/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1177 - auroc: 0.8115 - val_loss: 0.2558 - val_auroc: 0.8149\n",
            "Epoch 75/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1175 - auroc: 0.8120 - val_loss: 0.2557 - val_auroc: 0.8157\n",
            "Epoch 76/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1176 - auroc: 0.8115 - val_loss: 0.2556 - val_auroc: 0.8163\n",
            "Epoch 77/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1173 - auroc: 0.8126 - val_loss: 0.2553 - val_auroc: 0.8166\n",
            "Epoch 78/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1175 - auroc: 0.8123 - val_loss: 0.2563 - val_auroc: 0.8171\n",
            "Epoch 79/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1171 - auroc: 0.8140 - val_loss: 0.2564 - val_auroc: 0.8175\n",
            "Epoch 80/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1171 - auroc: 0.8134 - val_loss: 0.2564 - val_auroc: 0.8184\n",
            "Epoch 81/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1171 - auroc: 0.8143 - val_loss: 0.2560 - val_auroc: 0.8183\n",
            "Epoch 82/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1170 - auroc: 0.8142 - val_loss: 0.2565 - val_auroc: 0.8190\n",
            "Epoch 83/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1169 - auroc: 0.8144 - val_loss: 0.2551 - val_auroc: 0.8195\n",
            "Epoch 84/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1166 - auroc: 0.8159 - val_loss: 0.2550 - val_auroc: 0.8200\n",
            "Epoch 85/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1165 - auroc: 0.8166 - val_loss: 0.2557 - val_auroc: 0.8201\n",
            "Epoch 86/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1166 - auroc: 0.8158 - val_loss: 0.2557 - val_auroc: 0.8203\n",
            "Epoch 87/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1165 - auroc: 0.8164 - val_loss: 0.2565 - val_auroc: 0.8205\n",
            "Epoch 88/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1164 - auroc: 0.8170 - val_loss: 0.2562 - val_auroc: 0.8213\n",
            "Epoch 89/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1165 - auroc: 0.8164 - val_loss: 0.2569 - val_auroc: 0.8215\n",
            "Epoch 90/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1161 - auroc: 0.8179 - val_loss: 0.2551 - val_auroc: 0.8221\n",
            "Epoch 91/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1160 - auroc: 0.8183 - val_loss: 0.2550 - val_auroc: 0.8223\n",
            "Epoch 92/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1161 - auroc: 0.8181 - val_loss: 0.2553 - val_auroc: 0.8232\n",
            "Epoch 93/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1159 - auroc: 0.8187 - val_loss: 0.2550 - val_auroc: 0.8240\n",
            "Epoch 94/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1158 - auroc: 0.8189 - val_loss: 0.2544 - val_auroc: 0.8240\n",
            "Epoch 95/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1159 - auroc: 0.8191 - val_loss: 0.2539 - val_auroc: 0.8243\n",
            "Epoch 96/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1158 - auroc: 0.8192 - val_loss: 0.2529 - val_auroc: 0.8245\n",
            "Epoch 97/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1156 - auroc: 0.8198 - val_loss: 0.2535 - val_auroc: 0.8242\n",
            "Epoch 98/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1154 - auroc: 0.8207 - val_loss: 0.2541 - val_auroc: 0.8243\n",
            "Epoch 99/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1154 - auroc: 0.8207 - val_loss: 0.2541 - val_auroc: 0.8252\n",
            "Epoch 100/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1156 - auroc: 0.8203 - val_loss: 0.2537 - val_auroc: 0.8254\n",
            "Epoch 101/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1154 - auroc: 0.8211 - val_loss: 0.2542 - val_auroc: 0.8255\n",
            "Epoch 102/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1152 - auroc: 0.8216 - val_loss: 0.2537 - val_auroc: 0.8257\n",
            "Epoch 103/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1152 - auroc: 0.8214 - val_loss: 0.2525 - val_auroc: 0.8258\n",
            "Epoch 104/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1149 - auroc: 0.8232 - val_loss: 0.2536 - val_auroc: 0.8264\n",
            "Epoch 105/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1153 - auroc: 0.8214 - val_loss: 0.2532 - val_auroc: 0.8266\n",
            "Epoch 106/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1148 - auroc: 0.8235 - val_loss: 0.2527 - val_auroc: 0.8263\n",
            "Epoch 107/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1150 - auroc: 0.8222 - val_loss: 0.2524 - val_auroc: 0.8263\n",
            "Epoch 108/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1148 - auroc: 0.8239 - val_loss: 0.2518 - val_auroc: 0.8273\n",
            "Epoch 109/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1148 - auroc: 0.8234 - val_loss: 0.2533 - val_auroc: 0.8271\n",
            "Epoch 110/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1149 - auroc: 0.8230 - val_loss: 0.2531 - val_auroc: 0.8275\n",
            "Epoch 111/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1149 - auroc: 0.8232 - val_loss: 0.2524 - val_auroc: 0.8274\n",
            "Epoch 112/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1146 - auroc: 0.8239 - val_loss: 0.2523 - val_auroc: 0.8284\n",
            "Epoch 113/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1148 - auroc: 0.8234 - val_loss: 0.2530 - val_auroc: 0.8282\n",
            "Epoch 114/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1145 - auroc: 0.8245 - val_loss: 0.2526 - val_auroc: 0.8279\n",
            "Epoch 115/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1144 - auroc: 0.8251 - val_loss: 0.2526 - val_auroc: 0.8283\n",
            "Epoch 116/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1145 - auroc: 0.8245 - val_loss: 0.2536 - val_auroc: 0.8285\n",
            "Epoch 117/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1144 - auroc: 0.8247 - val_loss: 0.2528 - val_auroc: 0.8293\n",
            "Epoch 118/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1143 - auroc: 0.8253 - val_loss: 0.2545 - val_auroc: 0.8292\n",
            "Epoch 119/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1145 - auroc: 0.8246 - val_loss: 0.2519 - val_auroc: 0.8295\n",
            "Epoch 120/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1142 - auroc: 0.8258 - val_loss: 0.2515 - val_auroc: 0.8293\n",
            "Epoch 121/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1142 - auroc: 0.8258 - val_loss: 0.2521 - val_auroc: 0.8301\n",
            "Epoch 122/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1144 - auroc: 0.8251 - val_loss: 0.2530 - val_auroc: 0.8295\n",
            "Epoch 123/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1143 - auroc: 0.8255 - val_loss: 0.2520 - val_auroc: 0.8299\n",
            "Epoch 124/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1143 - auroc: 0.8254 - val_loss: 0.2519 - val_auroc: 0.8296\n",
            "Epoch 125/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1141 - auroc: 0.8264 - val_loss: 0.2539 - val_auroc: 0.8300\n",
            "Epoch 126/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1140 - auroc: 0.8266 - val_loss: 0.2526 - val_auroc: 0.8304\n",
            "Epoch 127/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1143 - auroc: 0.8256 - val_loss: 0.2555 - val_auroc: 0.8304\n",
            "Epoch 128/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1141 - auroc: 0.8261 - val_loss: 0.2547 - val_auroc: 0.8307\n",
            "Epoch 129/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1140 - auroc: 0.8264 - val_loss: 0.2536 - val_auroc: 0.8304\n",
            "Epoch 130/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1140 - auroc: 0.8267 - val_loss: 0.2526 - val_auroc: 0.8303\n",
            "Epoch 131/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1140 - auroc: 0.8266 - val_loss: 0.2527 - val_auroc: 0.8309\n",
            "Epoch 132/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1138 - auroc: 0.8273 - val_loss: 0.2523 - val_auroc: 0.8307\n",
            "Epoch 133/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1138 - auroc: 0.8276 - val_loss: 0.2538 - val_auroc: 0.8311\n",
            "Epoch 134/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1138 - auroc: 0.8275 - val_loss: 0.2531 - val_auroc: 0.8311\n",
            "Epoch 135/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1139 - auroc: 0.8270 - val_loss: 0.2530 - val_auroc: 0.8315\n",
            "Epoch 136/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1137 - auroc: 0.8279 - val_loss: 0.2522 - val_auroc: 0.8317\n",
            "Epoch 137/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1139 - auroc: 0.8274 - val_loss: 0.2525 - val_auroc: 0.8315\n",
            "Epoch 138/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1135 - auroc: 0.8285 - val_loss: 0.2527 - val_auroc: 0.8312\n",
            "Epoch 139/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1136 - auroc: 0.8286 - val_loss: 0.2539 - val_auroc: 0.8315\n",
            "Epoch 140/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1139 - auroc: 0.8275 - val_loss: 0.2531 - val_auroc: 0.8313\n",
            "Epoch 141/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1137 - auroc: 0.8281 - val_loss: 0.2542 - val_auroc: 0.8316\n",
            "Epoch 142/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1136 - auroc: 0.8285 - val_loss: 0.2540 - val_auroc: 0.8312\n",
            "Epoch 143/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1134 - auroc: 0.8294 - val_loss: 0.2540 - val_auroc: 0.8318\n",
            "Epoch 144/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1134 - auroc: 0.8295 - val_loss: 0.2535 - val_auroc: 0.8323\n",
            "Epoch 145/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1136 - auroc: 0.8282 - val_loss: 0.2554 - val_auroc: 0.8319\n",
            "Epoch 146/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1135 - auroc: 0.8289 - val_loss: 0.2537 - val_auroc: 0.8319\n",
            "Epoch 147/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1134 - auroc: 0.8291 - val_loss: 0.2539 - val_auroc: 0.8315\n",
            "Epoch 148/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1135 - auroc: 0.8288 - val_loss: 0.2527 - val_auroc: 0.8322\n",
            "Epoch 149/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1133 - auroc: 0.8298 - val_loss: 0.2549 - val_auroc: 0.8320\n",
            "Epoch 150/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1134 - auroc: 0.8292 - val_loss: 0.2545 - val_auroc: 0.8324\n",
            "Epoch 151/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1135 - auroc: 0.8289 - val_loss: 0.2545 - val_auroc: 0.8315\n",
            "Epoch 152/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1134 - auroc: 0.8289 - val_loss: 0.2531 - val_auroc: 0.8323\n",
            "Epoch 153/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1135 - auroc: 0.8288 - val_loss: 0.2551 - val_auroc: 0.8323\n",
            "Epoch 154/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1133 - auroc: 0.8293 - val_loss: 0.2543 - val_auroc: 0.8323\n",
            "Epoch 155/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1134 - auroc: 0.8293 - val_loss: 0.2538 - val_auroc: 0.8325\n",
            "Epoch 156/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1134 - auroc: 0.8293 - val_loss: 0.2534 - val_auroc: 0.8327\n",
            "Epoch 157/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1132 - auroc: 0.8301 - val_loss: 0.2551 - val_auroc: 0.8325\n",
            "Epoch 158/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1131 - auroc: 0.8301 - val_loss: 0.2550 - val_auroc: 0.8326\n",
            "Epoch 159/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1133 - auroc: 0.8295 - val_loss: 0.2545 - val_auroc: 0.8326\n",
            "Epoch 160/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1134 - auroc: 0.8289 - val_loss: 0.2539 - val_auroc: 0.8329\n",
            "Epoch 161/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1131 - auroc: 0.8307 - val_loss: 0.2544 - val_auroc: 0.8331\n",
            "Epoch 162/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1132 - auroc: 0.8296 - val_loss: 0.2540 - val_auroc: 0.8331\n",
            "Epoch 163/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1131 - auroc: 0.8305 - val_loss: 0.2549 - val_auroc: 0.8330\n",
            "Epoch 164/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1133 - auroc: 0.8294 - val_loss: 0.2550 - val_auroc: 0.8334\n",
            "Epoch 165/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1131 - auroc: 0.8303 - val_loss: 0.2542 - val_auroc: 0.8328\n",
            "Epoch 166/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1131 - auroc: 0.8304 - val_loss: 0.2539 - val_auroc: 0.8335\n",
            "Epoch 167/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1131 - auroc: 0.8306 - val_loss: 0.2544 - val_auroc: 0.8337\n",
            "Epoch 168/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1129 - auroc: 0.8311 - val_loss: 0.2543 - val_auroc: 0.8334\n",
            "Epoch 169/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1131 - auroc: 0.8302 - val_loss: 0.2553 - val_auroc: 0.8339\n",
            "Epoch 170/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1130 - auroc: 0.8308 - val_loss: 0.2547 - val_auroc: 0.8341\n",
            "Epoch 171/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1128 - auroc: 0.8314 - val_loss: 0.2556 - val_auroc: 0.8337\n",
            "Epoch 172/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1131 - auroc: 0.8301 - val_loss: 0.2538 - val_auroc: 0.8333\n",
            "Epoch 173/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1130 - auroc: 0.8309 - val_loss: 0.2553 - val_auroc: 0.8339\n",
            "Epoch 174/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1128 - auroc: 0.8315 - val_loss: 0.2544 - val_auroc: 0.8341\n",
            "Epoch 175/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1130 - auroc: 0.8309 - val_loss: 0.2564 - val_auroc: 0.8337\n",
            "Epoch 176/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1127 - auroc: 0.8319 - val_loss: 0.2537 - val_auroc: 0.8342\n",
            "Epoch 177/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1129 - auroc: 0.8316 - val_loss: 0.2559 - val_auroc: 0.8341\n",
            "Epoch 178/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1127 - auroc: 0.8321 - val_loss: 0.2559 - val_auroc: 0.8336\n",
            "Epoch 179/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1129 - auroc: 0.8313 - val_loss: 0.2565 - val_auroc: 0.8337\n",
            "Epoch 180/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1130 - auroc: 0.8309 - val_loss: 0.2544 - val_auroc: 0.8342\n",
            "Epoch 181/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1130 - auroc: 0.8307 - val_loss: 0.2559 - val_auroc: 0.8341\n",
            "Epoch 182/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1127 - auroc: 0.8321 - val_loss: 0.2540 - val_auroc: 0.8344\n",
            "Epoch 183/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1128 - auroc: 0.8319 - val_loss: 0.2525 - val_auroc: 0.8338\n",
            "Epoch 184/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1128 - auroc: 0.8317 - val_loss: 0.2552 - val_auroc: 0.8338\n",
            "Epoch 185/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1127 - auroc: 0.8320 - val_loss: 0.2555 - val_auroc: 0.8342\n",
            "Epoch 186/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1128 - auroc: 0.8316 - val_loss: 0.2562 - val_auroc: 0.8336\n",
            "Epoch 187/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1128 - auroc: 0.8318 - val_loss: 0.2536 - val_auroc: 0.8343\n",
            "Epoch 188/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1128 - auroc: 0.8317 - val_loss: 0.2578 - val_auroc: 0.8347\n",
            "Epoch 189/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1127 - auroc: 0.8321 - val_loss: 0.2538 - val_auroc: 0.8340\n",
            "Epoch 190/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1127 - auroc: 0.8319 - val_loss: 0.2568 - val_auroc: 0.8347\n",
            "Epoch 191/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1125 - auroc: 0.8329 - val_loss: 0.2549 - val_auroc: 0.8342\n",
            "Epoch 192/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1126 - auroc: 0.8324 - val_loss: 0.2547 - val_auroc: 0.8338\n",
            "Epoch 193/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1127 - auroc: 0.8324 - val_loss: 0.2564 - val_auroc: 0.8345\n",
            "Epoch 194/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1128 - auroc: 0.8317 - val_loss: 0.2576 - val_auroc: 0.8341\n",
            "Epoch 195/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1129 - auroc: 0.8312 - val_loss: 0.2568 - val_auroc: 0.8344\n",
            "Epoch 196/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1127 - auroc: 0.8318 - val_loss: 0.2563 - val_auroc: 0.8342\n",
            "Epoch 197/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1127 - auroc: 0.8320 - val_loss: 0.2551 - val_auroc: 0.8340\n",
            "Epoch 198/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1125 - auroc: 0.8332 - val_loss: 0.2555 - val_auroc: 0.8345\n",
            "Epoch 199/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1127 - auroc: 0.8322 - val_loss: 0.2555 - val_auroc: 0.8346\n",
            "Epoch 200/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1124 - auroc: 0.8332 - val_loss: 0.2556 - val_auroc: 0.8348\n",
            "Epoch 201/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1127 - auroc: 0.8318 - val_loss: 0.2565 - val_auroc: 0.8343\n",
            "Epoch 202/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1127 - auroc: 0.8324 - val_loss: 0.2550 - val_auroc: 0.8344\n",
            "Epoch 203/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1127 - auroc: 0.8322 - val_loss: 0.2570 - val_auroc: 0.8340\n",
            "Epoch 204/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1125 - auroc: 0.8329 - val_loss: 0.2559 - val_auroc: 0.8347\n",
            "Epoch 205/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1124 - auroc: 0.8332 - val_loss: 0.2549 - val_auroc: 0.8346\n",
            "Epoch 206/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1124 - auroc: 0.8333 - val_loss: 0.2556 - val_auroc: 0.8350\n",
            "Epoch 207/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1124 - auroc: 0.8335 - val_loss: 0.2536 - val_auroc: 0.8347\n",
            "Epoch 208/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1124 - auroc: 0.8332 - val_loss: 0.2569 - val_auroc: 0.8347\n",
            "Epoch 209/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1125 - auroc: 0.8330 - val_loss: 0.2559 - val_auroc: 0.8353\n",
            "Epoch 210/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1124 - auroc: 0.8336 - val_loss: 0.2560 - val_auroc: 0.8348\n",
            "Epoch 211/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1125 - auroc: 0.8329 - val_loss: 0.2539 - val_auroc: 0.8352\n",
            "Epoch 212/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1125 - auroc: 0.8331 - val_loss: 0.2559 - val_auroc: 0.8347\n",
            "Epoch 213/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1124 - auroc: 0.8332 - val_loss: 0.2539 - val_auroc: 0.8347\n",
            "Epoch 214/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1124 - auroc: 0.8330 - val_loss: 0.2550 - val_auroc: 0.8349\n",
            "Epoch 215/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1123 - auroc: 0.8337 - val_loss: 0.2549 - val_auroc: 0.8348\n",
            "Epoch 216/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1125 - auroc: 0.8331 - val_loss: 0.2552 - val_auroc: 0.8347\n",
            "Epoch 217/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1123 - auroc: 0.8335 - val_loss: 0.2542 - val_auroc: 0.8349\n",
            "Epoch 218/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1121 - auroc: 0.8344 - val_loss: 0.2539 - val_auroc: 0.8356\n",
            "Epoch 219/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1123 - auroc: 0.8336 - val_loss: 0.2557 - val_auroc: 0.8355\n",
            "Epoch 220/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1124 - auroc: 0.8336 - val_loss: 0.2528 - val_auroc: 0.8354\n",
            "Epoch 221/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1124 - auroc: 0.8336 - val_loss: 0.2562 - val_auroc: 0.8354\n",
            "Epoch 222/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1122 - auroc: 0.8341 - val_loss: 0.2549 - val_auroc: 0.8349\n",
            "Epoch 223/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1123 - auroc: 0.8338 - val_loss: 0.2532 - val_auroc: 0.8352\n",
            "Epoch 224/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1123 - auroc: 0.8335 - val_loss: 0.2549 - val_auroc: 0.8353\n",
            "Epoch 225/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1122 - auroc: 0.8340 - val_loss: 0.2544 - val_auroc: 0.8352\n",
            "Epoch 226/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1123 - auroc: 0.8338 - val_loss: 0.2534 - val_auroc: 0.8356\n",
            "Epoch 227/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1123 - auroc: 0.8337 - val_loss: 0.2558 - val_auroc: 0.8349\n",
            "Epoch 228/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1125 - auroc: 0.8331 - val_loss: 0.2550 - val_auroc: 0.8353\n",
            "Epoch 229/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1123 - auroc: 0.8337 - val_loss: 0.2534 - val_auroc: 0.8353\n",
            "Epoch 230/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1123 - auroc: 0.8338 - val_loss: 0.2545 - val_auroc: 0.8346\n",
            "Epoch 231/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1124 - auroc: 0.8336 - val_loss: 0.2564 - val_auroc: 0.8353\n",
            "Epoch 232/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1124 - auroc: 0.8335 - val_loss: 0.2549 - val_auroc: 0.8357\n",
            "Epoch 233/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1123 - auroc: 0.8338 - val_loss: 0.2548 - val_auroc: 0.8354\n",
            "Epoch 234/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1124 - auroc: 0.8331 - val_loss: 0.2544 - val_auroc: 0.8356\n",
            "Epoch 235/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1124 - auroc: 0.8335 - val_loss: 0.2540 - val_auroc: 0.8355\n",
            "Epoch 236/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1122 - auroc: 0.8343 - val_loss: 0.2558 - val_auroc: 0.8353\n",
            "Epoch 237/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1122 - auroc: 0.8338 - val_loss: 0.2551 - val_auroc: 0.8352\n",
            "Epoch 238/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1125 - auroc: 0.8330 - val_loss: 0.2536 - val_auroc: 0.8354\n",
            "Epoch 239/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1123 - auroc: 0.8337 - val_loss: 0.2550 - val_auroc: 0.8356\n",
            "Epoch 240/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1122 - auroc: 0.8341 - val_loss: 0.2551 - val_auroc: 0.8357\n",
            "Epoch 241/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1123 - auroc: 0.8336 - val_loss: 0.2552 - val_auroc: 0.8361\n",
            "Epoch 242/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1122 - auroc: 0.8342 - val_loss: 0.2548 - val_auroc: 0.8354\n",
            "Epoch 243/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1122 - auroc: 0.8340 - val_loss: 0.2542 - val_auroc: 0.8358\n",
            "Epoch 244/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1123 - auroc: 0.8336 - val_loss: 0.2544 - val_auroc: 0.8353\n",
            "Epoch 245/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1124 - auroc: 0.8334 - val_loss: 0.2555 - val_auroc: 0.8350\n",
            "Epoch 246/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1121 - auroc: 0.8346 - val_loss: 0.2566 - val_auroc: 0.8353\n",
            "Epoch 247/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1121 - auroc: 0.8346 - val_loss: 0.2550 - val_auroc: 0.8353\n",
            "Epoch 248/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1123 - auroc: 0.8337 - val_loss: 0.2534 - val_auroc: 0.8357\n",
            "Epoch 249/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1121 - auroc: 0.8348 - val_loss: 0.2548 - val_auroc: 0.8358\n",
            "Epoch 250/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1122 - auroc: 0.8341 - val_loss: 0.2535 - val_auroc: 0.8356\n",
            "Epoch 251/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1121 - auroc: 0.8345 - val_loss: 0.2544 - val_auroc: 0.8354\n",
            "Epoch 252/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1120 - auroc: 0.8348 - val_loss: 0.2571 - val_auroc: 0.8350\n",
            "Epoch 253/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1120 - auroc: 0.8348 - val_loss: 0.2536 - val_auroc: 0.8361\n",
            "Epoch 254/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1122 - auroc: 0.8343 - val_loss: 0.2557 - val_auroc: 0.8359\n",
            "Epoch 255/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1121 - auroc: 0.8344 - val_loss: 0.2536 - val_auroc: 0.8360\n",
            "Epoch 256/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1121 - auroc: 0.8344 - val_loss: 0.2551 - val_auroc: 0.8353\n",
            "Epoch 257/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1121 - auroc: 0.8343 - val_loss: 0.2554 - val_auroc: 0.8361\n",
            "Epoch 258/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1120 - auroc: 0.8349 - val_loss: 0.2539 - val_auroc: 0.8354\n",
            "Epoch 259/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1120 - auroc: 0.8350 - val_loss: 0.2558 - val_auroc: 0.8353\n",
            "Epoch 260/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1119 - auroc: 0.8352 - val_loss: 0.2533 - val_auroc: 0.8357\n",
            "Epoch 261/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1120 - auroc: 0.8351 - val_loss: 0.2543 - val_auroc: 0.8354\n",
            "Epoch 262/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1122 - auroc: 0.8340 - val_loss: 0.2541 - val_auroc: 0.8357\n",
            "Epoch 263/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1120 - auroc: 0.8346 - val_loss: 0.2551 - val_auroc: 0.8359\n",
            "Epoch 264/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1120 - auroc: 0.8349 - val_loss: 0.2536 - val_auroc: 0.8361\n",
            "Epoch 265/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1119 - auroc: 0.8355 - val_loss: 0.2538 - val_auroc: 0.8363\n",
            "Epoch 266/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1121 - auroc: 0.8345 - val_loss: 0.2547 - val_auroc: 0.8360\n",
            "Epoch 267/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1119 - auroc: 0.8352 - val_loss: 0.2546 - val_auroc: 0.8362\n",
            "Epoch 268/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1121 - auroc: 0.8346 - val_loss: 0.2553 - val_auroc: 0.8358\n",
            "Epoch 269/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1120 - auroc: 0.8346 - val_loss: 0.2551 - val_auroc: 0.8361\n",
            "Epoch 270/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1120 - auroc: 0.8347 - val_loss: 0.2539 - val_auroc: 0.8360\n",
            "Epoch 271/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1122 - auroc: 0.8342 - val_loss: 0.2544 - val_auroc: 0.8354\n",
            "Epoch 272/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1120 - auroc: 0.8346 - val_loss: 0.2544 - val_auroc: 0.8354\n",
            "Epoch 273/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1119 - auroc: 0.8353 - val_loss: 0.2543 - val_auroc: 0.8364\n",
            "Epoch 274/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1117 - auroc: 0.8364 - val_loss: 0.2545 - val_auroc: 0.8357\n",
            "Epoch 275/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1119 - auroc: 0.8354 - val_loss: 0.2522 - val_auroc: 0.8360\n",
            "Epoch 276/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1118 - auroc: 0.8355 - val_loss: 0.2544 - val_auroc: 0.8358\n",
            "Epoch 277/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1118 - auroc: 0.8355 - val_loss: 0.2550 - val_auroc: 0.8357\n",
            "Epoch 278/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1117 - auroc: 0.8361 - val_loss: 0.2532 - val_auroc: 0.8361\n",
            "Epoch 279/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1120 - auroc: 0.8350 - val_loss: 0.2532 - val_auroc: 0.8356\n",
            "Epoch 280/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1119 - auroc: 0.8354 - val_loss: 0.2532 - val_auroc: 0.8361\n",
            "Epoch 281/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1119 - auroc: 0.8355 - val_loss: 0.2540 - val_auroc: 0.8360\n",
            "Epoch 282/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1118 - auroc: 0.8353 - val_loss: 0.2534 - val_auroc: 0.8359\n",
            "Epoch 283/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1121 - auroc: 0.8343 - val_loss: 0.2542 - val_auroc: 0.8360\n",
            "Epoch 284/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1118 - auroc: 0.8356 - val_loss: 0.2536 - val_auroc: 0.8359\n",
            "Epoch 285/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1119 - auroc: 0.8352 - val_loss: 0.2542 - val_auroc: 0.8359\n",
            "Epoch 286/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1119 - auroc: 0.8352 - val_loss: 0.2537 - val_auroc: 0.8357\n",
            "Epoch 287/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1117 - auroc: 0.8359 - val_loss: 0.2547 - val_auroc: 0.8356\n",
            "Epoch 288/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1118 - auroc: 0.8356 - val_loss: 0.2559 - val_auroc: 0.8356\n",
            "Epoch 289/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1118 - auroc: 0.8354 - val_loss: 0.2523 - val_auroc: 0.8359\n",
            "Epoch 290/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1120 - auroc: 0.8351 - val_loss: 0.2537 - val_auroc: 0.8364\n",
            "Epoch 291/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1120 - auroc: 0.8349 - val_loss: 0.2543 - val_auroc: 0.8357\n",
            "Epoch 292/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1117 - auroc: 0.8359 - val_loss: 0.2535 - val_auroc: 0.8358\n",
            "Epoch 293/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1117 - auroc: 0.8362 - val_loss: 0.2544 - val_auroc: 0.8363\n",
            "Epoch 294/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1118 - auroc: 0.8356 - val_loss: 0.2540 - val_auroc: 0.8362\n",
            "Epoch 295/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1116 - auroc: 0.8363 - val_loss: 0.2528 - val_auroc: 0.8361\n",
            "Epoch 296/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1118 - auroc: 0.8357 - val_loss: 0.2558 - val_auroc: 0.8358\n",
            "Epoch 297/350\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 0.1120 - auroc: 0.8349 - val_loss: 0.2542 - val_auroc: 0.8360\n",
            "Epoch 298/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1118 - auroc: 0.8356 - val_loss: 0.2556 - val_auroc: 0.8359\n",
            "Epoch 299/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1120 - auroc: 0.8353 - val_loss: 0.2527 - val_auroc: 0.8363\n",
            "Epoch 300/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1118 - auroc: 0.8360 - val_loss: 0.2536 - val_auroc: 0.8358\n",
            "Epoch 301/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1118 - auroc: 0.8359 - val_loss: 0.2552 - val_auroc: 0.8353\n",
            "Epoch 302/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1120 - auroc: 0.8348 - val_loss: 0.2541 - val_auroc: 0.8358\n",
            "Epoch 303/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1116 - auroc: 0.8365 - val_loss: 0.2536 - val_auroc: 0.8354\n",
            "Epoch 304/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1120 - auroc: 0.8351 - val_loss: 0.2524 - val_auroc: 0.8355\n",
            "Epoch 305/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1119 - auroc: 0.8354 - val_loss: 0.2548 - val_auroc: 0.8361\n",
            "Epoch 306/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1117 - auroc: 0.8360 - val_loss: 0.2523 - val_auroc: 0.8362\n",
            "Epoch 307/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1118 - auroc: 0.8359 - val_loss: 0.2554 - val_auroc: 0.8356\n",
            "Epoch 308/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1117 - auroc: 0.8361 - val_loss: 0.2554 - val_auroc: 0.8354\n",
            "Epoch 309/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1119 - auroc: 0.8351 - val_loss: 0.2552 - val_auroc: 0.8358\n",
            "Epoch 310/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1118 - auroc: 0.8358 - val_loss: 0.2527 - val_auroc: 0.8357\n",
            "Epoch 311/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1116 - auroc: 0.8365 - val_loss: 0.2541 - val_auroc: 0.8357\n",
            "Epoch 312/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1115 - auroc: 0.8369 - val_loss: 0.2541 - val_auroc: 0.8354\n",
            "Epoch 313/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1118 - auroc: 0.8360 - val_loss: 0.2556 - val_auroc: 0.8356\n",
            "Epoch 314/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1116 - auroc: 0.8366 - val_loss: 0.2533 - val_auroc: 0.8364\n",
            "Epoch 315/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1117 - auroc: 0.8361 - val_loss: 0.2551 - val_auroc: 0.8356\n",
            "Epoch 316/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1117 - auroc: 0.8361 - val_loss: 0.2531 - val_auroc: 0.8357\n",
            "Epoch 317/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1117 - auroc: 0.8361 - val_loss: 0.2540 - val_auroc: 0.8357\n",
            "Epoch 318/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1115 - auroc: 0.8370 - val_loss: 0.2555 - val_auroc: 0.8354\n",
            "Epoch 319/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1119 - auroc: 0.8355 - val_loss: 0.2547 - val_auroc: 0.8352\n",
            "Epoch 320/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1116 - auroc: 0.8363 - val_loss: 0.2540 - val_auroc: 0.8358\n",
            "Epoch 321/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1117 - auroc: 0.8360 - val_loss: 0.2538 - val_auroc: 0.8359\n",
            "Epoch 322/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1117 - auroc: 0.8361 - val_loss: 0.2528 - val_auroc: 0.8361\n",
            "Epoch 323/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1116 - auroc: 0.8367 - val_loss: 0.2532 - val_auroc: 0.8364\n",
            "Epoch 324/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1115 - auroc: 0.8369 - val_loss: 0.2541 - val_auroc: 0.8364\n",
            "Epoch 325/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1116 - auroc: 0.8366 - val_loss: 0.2530 - val_auroc: 0.8368\n",
            "Epoch 326/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1116 - auroc: 0.8364 - val_loss: 0.2552 - val_auroc: 0.8361\n",
            "Epoch 327/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1115 - auroc: 0.8368 - val_loss: 0.2521 - val_auroc: 0.8359\n",
            "Epoch 328/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1117 - auroc: 0.8364 - val_loss: 0.2523 - val_auroc: 0.8357\n",
            "Epoch 329/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1113 - auroc: 0.8373 - val_loss: 0.2545 - val_auroc: 0.8362\n",
            "Epoch 330/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1115 - auroc: 0.8366 - val_loss: 0.2524 - val_auroc: 0.8358\n",
            "Epoch 331/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1116 - auroc: 0.8363 - val_loss: 0.2551 - val_auroc: 0.8361\n",
            "Epoch 332/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1114 - auroc: 0.8371 - val_loss: 0.2524 - val_auroc: 0.8360\n",
            "Epoch 333/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1117 - auroc: 0.8362 - val_loss: 0.2523 - val_auroc: 0.8360\n",
            "Epoch 334/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1114 - auroc: 0.8375 - val_loss: 0.2532 - val_auroc: 0.8362\n",
            "Epoch 335/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1115 - auroc: 0.8369 - val_loss: 0.2531 - val_auroc: 0.8361\n",
            "Epoch 336/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1117 - auroc: 0.8360 - val_loss: 0.2529 - val_auroc: 0.8357\n",
            "Epoch 337/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1113 - auroc: 0.8373 - val_loss: 0.2542 - val_auroc: 0.8354\n",
            "Epoch 338/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1116 - auroc: 0.8366 - val_loss: 0.2534 - val_auroc: 0.8358\n",
            "Epoch 339/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1117 - auroc: 0.8362 - val_loss: 0.2534 - val_auroc: 0.8355\n",
            "Epoch 340/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1117 - auroc: 0.8360 - val_loss: 0.2539 - val_auroc: 0.8356\n",
            "Epoch 341/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1116 - auroc: 0.8367 - val_loss: 0.2525 - val_auroc: 0.8361\n",
            "Epoch 342/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1118 - auroc: 0.8357 - val_loss: 0.2523 - val_auroc: 0.8359\n",
            "Epoch 343/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1116 - auroc: 0.8365 - val_loss: 0.2535 - val_auroc: 0.8354\n",
            "Epoch 344/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1116 - auroc: 0.8365 - val_loss: 0.2528 - val_auroc: 0.8364\n",
            "Epoch 345/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1116 - auroc: 0.8366 - val_loss: 0.2539 - val_auroc: 0.8365\n",
            "Epoch 346/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1117 - auroc: 0.8363 - val_loss: 0.2544 - val_auroc: 0.8361\n",
            "Epoch 347/350\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.1115 - auroc: 0.8368 - val_loss: 0.2528 - val_auroc: 0.8359\n",
            "Epoch 348/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1114 - auroc: 0.8373 - val_loss: 0.2535 - val_auroc: 0.8357\n",
            "Epoch 349/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1115 - auroc: 0.8369 - val_loss: 0.2520 - val_auroc: 0.8359\n",
            "Epoch 350/350\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.1115 - auroc: 0.8368 - val_loss: 0.2526 - val_auroc: 0.8356\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x3f9ad6b90>"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras.constraints import MaxNorm\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import random as rn\n",
        "\n",
        "#tf.random.set_seed(0)\n",
        "\n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "\n",
        "# Setting the seed for numpy-generated random numbers\n",
        "np.random.seed(21)\n",
        "\n",
        "# Setting the seed for python random numbers\n",
        "rn.seed(21)\n",
        "\n",
        "# Setting the graph-level random seed.\n",
        "tf.random.set_seed(21)\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "session_conf = tf.compat.v1.ConfigProto(\n",
        "      intra_op_parallelism_threads=1,\n",
        "      inter_op_parallelism_threads=1)\n",
        "\n",
        "#Force Tensorflow to use a single thread\n",
        "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
        "\n",
        "K.set_session(sess)\n",
        "\n",
        "col_counts = train_X5.shape[1]\n",
        "\n",
        "def auroc(y_true, y_pred):\n",
        "    return tf.py_function(roc_auc_score, (y_true, y_pred), tf.double)\n",
        "\n",
        "class_weight = {0: 0.75,\n",
        "                1: 0.25}\n",
        "\n",
        "network = models.Sequential()\n",
        "\n",
        "network.add(layers.Dense(units = 22, activation = \"relu\", input_shape = (col_counts,)))\n",
        "network.add(layers.Dropout(0.1))\n",
        "network.add(layers.Dense(units = 22, activation = \"relu\"))\n",
        "network.add(layers.Dense(units = 2, activation = \"sigmoid\"))\n",
        "\n",
        "sgd = SGD(learning_rate=0.1, momentum=0.9)\n",
        "\n",
        "network.compile(\n",
        "    loss = \"binary_crossentropy\",\n",
        "    optimizer = tf.keras.optimizers.Adam(),\n",
        "    #optimizer = sgd,\n",
        "    metrics = [auroc]\n",
        ")\n",
        "network.fit(train_X5, train_y_ohe, validation_data=(val_X5, val_y_ohe), epochs=350, batch_size=10000, class_weight=class_weight, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "qv3EhHPLQAB-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2220/2220 [==============================] - 1s 319us/step\n",
            "acc_val =  0.90407950674287\n",
            "roc_auc =  0.8336328640472244\n"
          ]
        }
      ],
      "source": [
        "y_pred = network.predict(val_X5)\n",
        "acc = accuracy_score(val_y, np.where(y_pred[:, 0] > 0.5, 1, 0))\n",
        "print(\"acc_val = \", acc)\n",
        "roc_auc = roc_auc_score(np.array(val_y_ohe), y_pred)\n",
        "print(\"roc_auc = \", roc_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "3L_pt7A36HBH"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/9j/mrbb80v92lxdv_3kldctnqq00000gn/T/ipykernel_19384/3590628818.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df_metrics = df_metrics.append(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>hyperparams</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>roc_auc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Naive classifier</td>\n",
              "      <td>baseline</td>\n",
              "      <td>0.919606</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Rand_forest</td>\n",
              "      <td>{'n_est': 1600, 'min_samp_splt': 2, 'min_samp_...</td>\n",
              "      <td>0.920211</td>\n",
              "      <td>0.507280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>MLP</td>\n",
              "      <td>rand_st=21, layers = [20], act_f = relu</td>\n",
              "      <td>0.917691</td>\n",
              "      <td>0.82772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Numpy</td>\n",
              "      <td>layers=[25] epochs = 80 batch_size = 1000 lr =...</td>\n",
              "      <td>0.918086</td>\n",
              "      <td>0.825581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TensorFlow</td>\n",
              "      <td>layers=[25] epochs = 200 batch_size = 5000</td>\n",
              "      <td>0.9190433</td>\n",
              "      <td>0.8207533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Keras</td>\n",
              "      <td>layers=[22, dropout(0,1), 22] epochs = 350 bat...</td>\n",
              "      <td>0.90407</td>\n",
              "      <td>0.83363</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              model                                        hyperparams  \\\n",
              "0  Naive classifier                                           baseline   \n",
              "0       Rand_forest  {'n_est': 1600, 'min_samp_splt': 2, 'min_samp_...   \n",
              "0               MLP            rand_st=21, layers = [20], act_f = relu   \n",
              "0             Numpy  layers=[25] epochs = 80 batch_size = 1000 lr =...   \n",
              "0        TensorFlow         layers=[25] epochs = 200 batch_size = 5000   \n",
              "0             Keras  layers=[22, dropout(0,1), 22] epochs = 350 bat...   \n",
              "\n",
              "    accuracy    roc_auc  \n",
              "0   0.919606        0.5  \n",
              "0   0.920211   0.507280  \n",
              "0   0.917691    0.82772  \n",
              "0   0.918086   0.825581  \n",
              "0  0.9190433  0.8207533  \n",
              "0    0.90407    0.83363  "
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_metrics = df_metrics.append(\n",
        "    pd.DataFrame({'model' : [\"Keras\"],\n",
        "                                \"hyperparams\" : [\"layers=[22, dropout(0,1), 22] epochs = 350 batch_size = 10000, class_weight\"],\n",
        "                                'accuracy' : \"0.90407\", \n",
        "                                'roc_auc' : \"0.83363\"})\n",
        ")\n",
        "df_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "A3mKGAfAtB1b",
        "dNy0sWjO4thV",
        "V-PhERRcyFe0",
        "bGj2Ej0-9GbB",
        "yBHasCcwJyI3",
        "_Bf1bZNEg5Jq",
        "uwjZh-_RBKWG",
        "nQkTg2El0aFv",
        "xGQTheWcQBhP",
        "dMGNin8Vw-O_",
        "-xN0azxm__E4"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.8 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
